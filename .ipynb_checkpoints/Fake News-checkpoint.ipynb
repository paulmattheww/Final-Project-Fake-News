{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from TextCleaner import TextCleaner\n",
    "from helpers import binary_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas_profiling as pr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@inproceedings{wang2018eann,\n",
    "  title={EANN: Event Adversarial Neural Networks for Multi-Modal Fake News Detection},\n",
    "  author={Wang, Yaqing and Ma, Fenglong and Jin, Zhiwei and Yuan, Ye and Xun, Guangxu and Jha, Kishlay and Su, Lu and Gao, Jing},\n",
    "  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining},\n",
    "  pages={849--857},\n",
    "  year={2018},\n",
    "  organization={ACM}\n",
    "}\n",
    "\n",
    "https://arxiv.org/abs/1803.11175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20800.000000</td>\n",
       "      <td>20800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10399.500000</td>\n",
       "      <td>0.500625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6004.587135</td>\n",
       "      <td>0.500012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5199.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10399.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15599.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20799.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         label\n",
       "count  20800.000000  20800.000000\n",
       "mean   10399.500000      0.500625\n",
       "std     6004.587135      0.500012\n",
       "min        0.000000      0.000000\n",
       "25%     5199.750000      0.000000\n",
       "50%    10399.500000      1.000000\n",
       "75%    15599.250000      1.000000\n",
       "max    20799.000000      1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word2vec(post, word_id_map, W):\n",
    "#     \"\"\"Cited from https://github.com/yaqingwang/EANN-KDD18\n",
    "#     Generates word2vec embeddings.\n",
    "    \n",
    "#     INPUTS:\n",
    "    \n",
    "#     RETURN:\n",
    "#     \"\"\"\n",
    "#     word_embedding = []\n",
    "#     mask = []\n",
    "#     for sentence in post:\n",
    "#         sen_embedding = []\n",
    "#         seq_len = len(sentence) - 1\n",
    "#         mask_seq = np.zeros(args.sequence_len, dtype=np.float32)\n",
    "#         mask_seq[:len(sentence)] = 1.0\n",
    "#         for i, word in enumerate(sentence):\n",
    "#             sen_embedding.append(word_id_map[word])\n",
    "\n",
    "#         while len(sen_embedding) < args.sequence_len:\n",
    "#             sen_embedding.append(0)\n",
    "#         word_embedding.append(copy.deepcopy(sen_embedding))\n",
    "#         mask.append(copy.deepcopy(mask_seq))\n",
    "#     return word_embedding, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "# %matplotlib inline\n",
    "\n",
    "# # read in data and preprocess\n",
    "# train_df = pd.read_csv('c:/users/washburp/documents/kaggle/arthur/data/train_df.csv')\n",
    "\n",
    "# # drop missing descriptions\n",
    "# train_df.dropna(inplace=True, subset=['desc_of_operations'])\n",
    "\n",
    "# # derive length of description & filter long/short ones\n",
    "# train_df['len_description'] = train_df.desc_of_operations.apply(len)\n",
    "# reasonable_sized_texts = (train_df.len_description.astype(int) >= 1) | (train_df.len_description.astype(int) <= 200)\n",
    "# train_df = train_df.loc[reasonable_sized_texts]\n",
    "\n",
    "# # clean up text\n",
    "# from text_cleanup import TextCleaner\n",
    "\n",
    "# train_df['clean_desc'] = TextCleaner().transform(train_df.desc_of_operations.values)\n",
    "# train_df.head()\n",
    "\n",
    "# sentences = []\n",
    "# for description in train_df.clean_desc.tolist():\n",
    "#     sentences.append(description.split())\n",
    "    \n",
    "# sentences[:1]\n",
    "\n",
    "# import gensim\n",
    "\n",
    "# model = gensim.models.Word2Vec(iter=1, min_count=10, size=150, workers=4)\n",
    "# model.build_vocab(sentences)\n",
    "# model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# topn = 20\n",
    "\n",
    "# dat = model.most_similar(['salon'], topn=topn)\n",
    "# df = pd.DataFrame(dat, columns=['word', 'prob']).set_index('word')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "#help(DefaultTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(UnigramTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine columnt texts\n",
    "df['combined_text'] = df[['title', 'author', 'text']].apply(lambda x: ''.join(str(x)), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine columnt texts\n",
    "df['combined_text'] = TextCleaner(custom_stopwords=['title', 'author', 'text']).transform(df['combined_text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['House', 'Dem', 'Aide:', 'We', 'Didn’t', 'Even', 'See', 'Comey’s', 'Letter', 'Until', 'Jason', 'Chaffetz', 'Tweeted', 'It']\",\n",
       " \"['FLYNN:', 'Hillary', 'Clinton,', 'Big', 'Woman', 'on', 'Campus', '-', 'Breitbart']\",\n",
       " \"['Why', 'the', 'Truth', 'Might', 'Get', 'You', 'Fired']\",\n",
       " \"['15', 'Civilians', 'Killed', 'In', 'Single', 'US', 'Airstrike', 'Have', 'Been', 'Identified']\",\n",
       " \"['Iranian', 'woman', 'jailed', 'for', 'fictional', 'unpublished', 'story', 'about', 'woman', 'stoned', 'to', 'death', 'for', 'adultery']\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['title'] = TextCleaner().transform(df['title'].astype(str))\n",
    "txt = df['title'].str.split().astype(str).tolist() #combined_text got 81% accuracy\n",
    "txt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tag import UnigramTagger, DefaultTagger\n",
    "# from nltk.corpus import treebank\n",
    "# tagger = DefaultTagger('NN')\n",
    "# #tagger.tag(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "def fetch_universal_sentence_embeddings(messages, verbose=0):\n",
    "    \"\"\"Fetches universal sentence embeddings from Google's\n",
    "    research paper https://arxiv.org/pdf/1803.11175.pdf.\n",
    "    \n",
    "    INPUTS:\n",
    "    RETURNS:\n",
    "    \"\"\"\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "\n",
    "    # Import the Universal Sentence Encoder's TF Hub module\n",
    "    embed = hub.Module(module_url)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(embed(messages))\n",
    "        embeddings = list()\n",
    "        for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "            if verbose:\n",
    "                print(\"Message: {}\".format(messages[i]))\n",
    "                print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "                message_embedding_snippet = \", \".join(\n",
    "                    (str(x) for x in message_embedding[:3]))\n",
    "                print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n",
    "            embeddings.append(message_embedding)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = fetch_universal_sentence_embeddings(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05387789383530617,\n",
       " 0.006952833849936724,\n",
       " -0.057719238102436066,\n",
       " 0.016467709094285965,\n",
       " -0.08529816567897797,\n",
       " 0.007254021242260933,\n",
       " 0.08881591260433197,\n",
       " 0.007081518415361643,\n",
       " -0.03469545766711235,\n",
       " -0.0006816966342739761]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500625\n",
       "0    0.499375\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = np.array(embeddings), df.label.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7, random_state=7)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, train_size=.5, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05387789,  0.00695283, -0.05771924,  0.01646771, -0.08529817])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   38.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('clf', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'clf__C': [1, 2, 3, 4, 5], 'clf__random_state': [777]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test LinearSVC model as baseline\n",
    "\n",
    "params = {'clf__C':[1, 2, 3, 4, 5], # 1 best here  [.001, .01, .1, 1, 10, 100, 1000]\n",
    "          'clf__random_state': [777]}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(pipeline, params, cv=5, verbose=1)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('clf',\n",
       "   LinearSVC(C=2, class_weight=None, dual=True, fit_intercept=True,\n",
       "        intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "        multi_class='ovr', penalty='l2', random_state=777, tol=0.0001,\n",
       "        verbose=0))],\n",
       " 'clf': LinearSVC(C=2, class_weight=None, dual=True, fit_intercept=True,\n",
       "      intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "      multi_class='ovr', penalty='l2', random_state=777, tol=0.0001,\n",
       "      verbose=0),\n",
       " 'clf__C': 2,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__dual': True,\n",
       " 'clf__fit_intercept': True,\n",
       " 'clf__intercept_scaling': 1,\n",
       " 'clf__loss': 'squared_hinge',\n",
       " 'clf__max_iter': 1000,\n",
       " 'clf__multi_class': 'ovr',\n",
       " 'clf__penalty': 'l2',\n",
       " 'clf__random_state': 777,\n",
       " 'clf__tol': 0.0001,\n",
       " 'clf__verbose': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy: 0.8994\n",
      "Validation Accuracy: 0.8875\n",
      "\n",
      "\n",
      "        Condition Positive:                        1511\n",
      "        Condition Negative:                        1609\n",
      "        Total Observations:                        3120\n",
      "\n",
      "        True Positive:                             1340\n",
      "        True Negative:                             1429\n",
      "        False Positive:                            180\n",
      "        False Negative                             171\n",
      "\n",
      "        True Positive Rate (recall):               88.68%\n",
      "        True Negative Rate (specificity):          88.81%\n",
      "        False Positive Rate (fall-out):            11.19%\n",
      "        False Negative Rate (miss rate):           11.32%\n",
      "\n",
      "        Positive Predictive Value (precision):     88.16%\n",
      "        Negative Predictive Value:                 89.31%\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(+) actual</th>\n",
       "      <th>(-) actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(+) predicted</th>\n",
       "      <td>1340</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-) predicted</th>\n",
       "      <td>171</td>\n",
       "      <td>1429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               (+) actual  (-) actual\n",
       "(+) predicted        1340         180\n",
       "(-) predicted         171        1429"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_confusion_matrix(y, y_hat, as_pct=False, verbose=True):\n",
    "    cm = pd.DataFrame(confusion_matrix(y, y_hat),\n",
    "                      columns=['(+) actual', '(-) actual'],\n",
    "                      index=['(+) predicted', '(-) predicted'])\n",
    "    if as_pct:\n",
    "        cm = cm / cm.sum().sum()\n",
    "\n",
    "    P = cm['(+) actual'].sum()\n",
    "    N = cm['(-) actual'].sum()\n",
    "    total = P + N\n",
    "    TP = cm.loc['(+) predicted', '(+) actual']\n",
    "    FP = cm.loc['(+) predicted', '(-) actual']\n",
    "    TN = cm.loc['(-) predicted', '(-) actual']\n",
    "    FN = cm.loc['(-) predicted', '(+) actual']\n",
    "    TPR = TP / (TP + FN)          # recall/sensitivity\n",
    "    TNR = TN / (TN + FP)   # specificity\n",
    "    FPR = FP / (FP + TN)   # fall-out\n",
    "    FNR = FN / (FN + TP)   # miss rate\n",
    "    PPV = TP / (TP + FP)   # precision\n",
    "    NPV = TN / (TN + FN)   # neg predictive value\n",
    "\n",
    "    if verbose:\n",
    "        print('''\n",
    "        Condition Positive:                        %i\n",
    "        Condition Negative:                        %i\n",
    "        Total Observations:                        %i\n",
    "\n",
    "        True Positive:                             %i\n",
    "        True Negative:                             %i\n",
    "        False Positive:                            %i\n",
    "        False Negative                             %i\n",
    "\n",
    "        True Positive Rate (recall):               %.2f%%\n",
    "        True Negative Rate (specificity):          %.2f%%\n",
    "        False Positive Rate (fall-out):            %.2f%%\n",
    "        False Negative Rate (miss rate):           %.2f%%\n",
    "\n",
    "        Positive Predictive Value (precision):     %.2f%%\n",
    "        Negative Predictive Value:                 %.2f%%\n",
    "        ''' %(P, N, total,\n",
    "             TP, TN, FP, FN,\n",
    "             TPR*100, TNR*100, FPR*100, FNR*100,\n",
    "             PPV*100, NPV*100))\n",
    "\n",
    "    metrics = {'P': P, 'N': N, 'total': total,\n",
    "              'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN,\n",
    "              'TPR': TPR, 'TNR': TNR, 'FPR': FPR, 'FNR': FNR, 'PPV': PPV, 'NPV': NPV}\n",
    "\n",
    "    return cm, metrics\n",
    "\n",
    "\n",
    "yhat_val = grid.predict(X_val)\n",
    "val_acc = accuracy_score(y_val, yhat_val)\n",
    "yhat_train = grid.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, yhat_train) \n",
    "\n",
    "print(f'''\n",
    "Training Accuracy: {round(train_acc, 4)}\n",
    "Validation Accuracy: {round(val_acc, 4)}\n",
    "''')\n",
    "\n",
    "_, __ = binary_confusion_matrix(y_val, yhat_val)\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearSVC in module sklearn.svm.classes:\n",
      "\n",
      "class LinearSVC(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      " |  Linear Support Vector Classification.\n",
      " |  \n",
      " |  Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
      " |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      " |  penalties and loss functions and should scale better to large numbers of\n",
      " |  samples.\n",
      " |  \n",
      " |  This class supports both dense and sparse input and the multiclass support\n",
      " |  is handled according to a one-vs-the-rest scheme.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : string, 'l1' or 'l2' (default='l2')\n",
      " |      Specifies the norm used in the penalization. The 'l2'\n",
      " |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
      " |      vectors that are sparse.\n",
      " |  \n",
      " |  loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')\n",
      " |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
      " |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
      " |      square of the hinge loss.\n",
      " |  \n",
      " |  dual : bool, (default=True)\n",
      " |      Select the algorithm to either solve the dual or primal\n",
      " |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, optional (default=1e-4)\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, optional (default=1.0)\n",
      " |      Penalty parameter C of the error term.\n",
      " |  \n",
      " |  multi_class : string, 'ovr' or 'crammer_singer' (default='ovr')\n",
      " |      Determines the multi-class strategy if `y` contains more than\n",
      " |      two classes.\n",
      " |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
      " |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
      " |      While `crammer_singer` is interesting from a theoretical perspective\n",
      " |      as it is consistent, it is seldom used in practice as it rarely leads\n",
      " |      to better accuracy and is more expensive to compute.\n",
      " |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
      " |      will be ignored.\n",
      " |  \n",
      " |  fit_intercept : boolean, optional (default=True)\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be already centered).\n",
      " |  \n",
      " |  intercept_scaling : float, optional (default=1)\n",
      " |      When self.fit_intercept is True, instance vector x becomes\n",
      " |      ``[x, self.intercept_scaling]``,\n",
      " |      i.e. a \"synthetic\" feature with constant value equals to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : {dict, 'balanced'}, optional\n",
      " |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
      " |      SVC. If not given, all classes are supposed to have\n",
      " |      weight one.\n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |  verbose : int, (default=0)\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      The seed of the pseudo random number generator to use when shuffling\n",
      " |      the data.  If int, random_state is the seed used by the random number\n",
      " |      generator; If RandomState instance, random_state is the random number\n",
      " |      generator; If None, the random number generator is the RandomState\n",
      " |      instance used by `np.random`.\n",
      " |  \n",
      " |  max_iter : int, (default=1000)\n",
      " |      The maximum number of iterations to be run.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem). This is only available in the case of a linear kernel.\n",
      " |  \n",
      " |      ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
      " |      follows the internal memory layout of liblinear.\n",
      " |  \n",
      " |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.svm import LinearSVC\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      " |  >>> clf = LinearSVC(random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      " |       intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      " |       multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      " |       verbose=0)\n",
      " |  >>> print(clf.coef_)\n",
      " |  [[ 0.08551385  0.39414796  0.49847831  0.37513797]]\n",
      " |  >>> print(clf.intercept_)\n",
      " |  [ 0.28418066]\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller ``tol`` parameter.\n",
      " |  \n",
      " |  The underlying implementation, liblinear, uses a sparse internal\n",
      " |  representation for the data that will incur a memory copy.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  `LIBLINEAR: A Library for Large Linear Classification\n",
      " |  <http://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  SVC\n",
      " |      Implementation of Support Vector Machine classifier using libsvm:\n",
      " |      the kernel can be non-linear but its SMO algorithm does not\n",
      " |      scale to large number of samples as LinearSVC does.\n",
      " |  \n",
      " |      Furthermore SVC multi-class mode is implemented using one\n",
      " |      vs one scheme while LinearSVC uses one vs the rest. It is\n",
      " |      possible to implement one vs the rest with SVC by using the\n",
      " |      :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
      " |  \n",
      " |      Finally SVC can fit dense data without memory copy if the input\n",
      " |      is C-contiguous. Sparse data will still incur memory copy though.\n",
      " |  \n",
      " |  sklearn.linear_model.SGDClassifier\n",
      " |      SGDClassifier can optimize the same cost function as LinearSVC\n",
      " |      by adjusting the penalty and loss parameters. In addition it requires\n",
      " |      less memory, allows incremental (online) learning, and implements\n",
      " |      various loss functions and regularization regimes.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearSVC\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.linear_model.base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model.base.SparseCoefMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', loss='squared_hinge', dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training vector, where n_samples in the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          Target vector relative to X\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Array of weights that are assigned to individual\n",
      " |          samples. If not provided,\n",
      " |          then each sample is given unit weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is the signed distance of that\n",
      " |      sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LinearSVCarSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "skplt.metrics.plot_roc_curve(y_val, grid.predict_proba(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Autoencoder to Enhance Feature Set of Text\n",
    "\n",
    "Titles of news articles worked well -- now let's combine the power of sentence embeddings on short sequences (titles) with the power of autoencoders on the entire cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Embedding in module keras.layers.embeddings:\n",
      "\n",
      "class Embedding(keras.engine.topology.Layer)\n",
      " |  Turns positive integers (indexes) into dense vectors of fixed size.\n",
      " |  eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
      " |  \n",
      " |  This layer can only be used as the first layer in a model.\n",
      " |  \n",
      " |  # Example\n",
      " |  \n",
      " |  ```python\n",
      " |    model = Sequential()\n",
      " |    model.add(Embedding(1000, 64, input_length=10))\n",
      " |    # the model will take as input an integer matrix of size (batch, input_length).\n",
      " |    # the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
      " |    # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
      " |  \n",
      " |    input_array = np.random.randint(1000, size=(32, 10))\n",
      " |  \n",
      " |    model.compile('rmsprop', 'mse')\n",
      " |    output_array = model.predict(input_array)\n",
      " |    assert output_array.shape == (32, 10, 64)\n",
      " |  ```\n",
      " |  \n",
      " |  # Arguments\n",
      " |    input_dim: int > 0. Size of the vocabulary,\n",
      " |        i.e. maximum integer index + 1.\n",
      " |    output_dim: int >= 0. Dimension of the dense embedding.\n",
      " |    embeddings_initializer: Initializer for the `embeddings` matrix\n",
      " |        (see [initializers](../initializers.md)).\n",
      " |    embeddings_regularizer: Regularizer function applied to\n",
      " |        the `embeddings` matrix\n",
      " |        (see [regularizer](../regularizers.md)).\n",
      " |    embeddings_constraint: Constraint function applied to\n",
      " |        the `embeddings` matrix\n",
      " |        (see [constraints](../constraints.md)).\n",
      " |    mask_zero: Whether or not the input value 0 is a special \"padding\"\n",
      " |        value that should be masked out.\n",
      " |        This is useful when using [recurrent layers](recurrent.md)\n",
      " |        which may take variable length input.\n",
      " |        If this is `True` then all subsequent layers\n",
      " |        in the model need to support masking or an exception will be raised.\n",
      " |        If mask_zero is set to True, as a consequence, index 0 cannot be\n",
      " |        used in the vocabulary (input_dim should equal size of\n",
      " |        vocabulary + 1).\n",
      " |    input_length: Length of input sequences, when it is constant.\n",
      " |        This argument is required if you are going to connect\n",
      " |        `Flatten` then `Dense` layers upstream\n",
      " |        (without it, the shape of the dense outputs cannot be computed).\n",
      " |  \n",
      " |  # Input shape\n",
      " |      2D tensor with shape: `(batch_size, sequence_length)`.\n",
      " |  \n",
      " |  # Output shape\n",
      " |      3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n",
      " |  \n",
      " |  # References\n",
      " |      - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Embedding\n",
      " |      keras.engine.topology.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the layer weights.\n",
      " |      \n",
      " |      Must be implemented on all layers that have weights.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Keras tensor (future input to layer)\n",
      " |              or list/tuple of Keras tensors to reference\n",
      " |              for weight shape computations.\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      # Returns\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Container` (one layer of abstraction above).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.topology.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the _keras_shape of the input(s).\n",
      " |          - We update the _keras_shape of every input tensor with\n",
      " |              its new shape (obtained via self.compute_output_shape).\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Adds losses to the layer.\n",
      " |      \n",
      " |      The loss may potentially be conditional on some inputs tensors,\n",
      " |      for instance activity losses are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          losses: loss tensor or list of loss tensors\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the losses as conditional on these inputs.\n",
      " |              If None is passed, the loss is assumed unconditional\n",
      " |              (e.g. L2 weight regularization, which only depends\n",
      " |              on the layer's weights variables, not on any inputs tensors).\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Adds updates to the layer.\n",
      " |      \n",
      " |      The updates may potentially be conditional on some inputs tensors,\n",
      " |      for instance batch norm updates are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          updates: update op or list of update ops\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the updates as conditional on these inputs.\n",
      " |              If None is passed, the updates are assumed unconditional.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  assert_input_compatibility(self, inputs)\n",
      " |      Checks compatibility between the layer and provided inputs.\n",
      " |      \n",
      " |      This checks that the tensor(s) `input`\n",
      " |      verify the input assumptions of the layer\n",
      " |      (if any). If not, exceptions are raised.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of mismatch between\n",
      " |              the provided inputs and the expectations of the layer.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Counts the total number of scalars composing the weights.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An integer count.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the layer isn't yet built\n",
      " |              (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.topology.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Container), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.topology.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  built\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input shape tuple\n",
      " |          (or list of input shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  losses\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output tensor or list of output tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one inbound node,\n",
      " |      or if all inbound nodes have the same output shape.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output shape tuple\n",
      " |          (or list of input shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14559, 512)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'author', 'text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- column selector, feature union\n",
    "    - autoencoder of text in article\n",
    "        - Transform text via embeddings\n",
    "    - model on title using sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 40], [36, 29], [36, 16], [23, 29], [38], [5], [26, 16], [10, 36], [26, 29], [30, 7, 40, 4]]\n",
      "[[ 3 40  0  0]\n",
      " [36 29  0  0]\n",
      " [36 16  0  0]\n",
      " [23 29  0  0]\n",
      " [38  0  0  0]\n",
      " [ 5  0  0  0]\n",
      " [26 16  0  0]\n",
      " [10 36  0  0]\n",
      " [26 29  0  0]\n",
      " [30  7 40  4]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 69.999999\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-169bd8928ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mearly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    120\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "early = EarlyStopping(patience=2)\n",
    "\n",
    "txt = CountVectorizer().fit_transform(df['text'].values)\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64))\n",
    "model.compile('rmsprop', 'mse')\n",
    "X_embedded = model.predict(txt)\n",
    "\n",
    "activation = 'relu'\n",
    "\n",
    "input_dim = 512\n",
    "row_dim = 14559\n",
    "\n",
    "input_i = Input(shape=(input_dim,))\n",
    "encoded_h1 = Dense(256, activation=activation)(input_i)\n",
    "encoded_h2 = Dense(128, activation=activation)(encoded_h1)\n",
    "latent = Dense(64, activation='tanh')(encoded_h2)\n",
    "decoder_h1 = Dense(128, activation=activation)(latent)\n",
    "decoder_h2 = Dense(256, activation=activation)(decoder_h1)\n",
    "\n",
    "output = Dense(input_dim, activation='tanh')(decoder_h2)\n",
    "\n",
    "autoencoder = Model(input_i, output)\n",
    "\n",
    "autoencoder.compile('adadelta','mse')\n",
    "\n",
    "#X_embedded = model.predict(np.array(X_train))\n",
    "autoencoder.fit(X_embedded, X_embedded, epochs=50,\n",
    "            batch_size=256, validation_split=.1, callbacks=[early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['House', 'Dem', 'Aide:', 'We', 'Didn’t', 'Even', 'See', 'Comey’s', 'Letter', 'Until', 'Jason', 'Chaffetz', 'Tweeted', 'It']\",\n",
       " \"['FLYNN:', 'Hillary', 'Clinton,', 'Big', 'Woman', 'on', 'Campus', '-', 'Breitbart']\",\n",
       " \"['Why', 'the', 'Truth', 'Might', 'Get', 'You', 'Fired']\",\n",
       " \"['15', 'Civilians', 'Killed', 'In', 'Single', 'US', 'Airstrike', 'Have', 'Been', 'Identified']\",\n",
       " \"['Iranian', 'woman', 'jailed', 'for', 'fictional', 'unpublished', 'story', 'about', 'woman', 'stoned', 'to', 'death', 'for', 'adultery']\",\n",
       " \"['Jackie', 'Mason:', 'Hollywood', 'Would', 'Love', 'Trump', 'if', 'He', 'Bombed', 'North', 'Korea', 'over', 'Lack', 'of', 'Trans', 'Bathrooms', '(Exclusive', 'Video)', '-', 'Breitbart']\",\n",
       " \"['Life:', 'Life', 'Of', 'Luxury:', 'Elton', 'John’s', '6', 'Favorite', 'Shark', 'Pictures', 'To', 'Stare', 'At', 'During', 'Long,', 'Transcontinental', 'Flights']\",\n",
       " \"['Benoît', 'Hamon', 'Wins', 'French', 'Socialist', 'Party’s', 'Presidential', 'Nomination', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Excerpts', 'From', 'a', 'Draft', 'Script', 'for', 'Donald', 'Trump’s', 'Q&ampA', 'With', 'a', 'Black', 'Church’s', 'Pastor', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['A', 'Back-Channel', 'Plan', 'for', 'Ukraine', 'and', 'Russia,', 'Courtesy', 'of', 'Trump', 'Associates', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Obama’s', 'Organizing', 'for', 'Action', 'Partners', 'with', 'Soros-Linked', '‘Indivisible’', 'to', 'Disrupt', 'Trump’s', 'Agenda']\",\n",
       " '[\\'BBC\\', \\'Comedy\\', \\'Sketch\\', \\'\"Real\\', \\'Housewives\\', \\'of\\', \\'ISIS\"\\', \\'Causes\\', \\'Outrage\\']',\n",
       " \"['Russian', 'Researchers', 'Discover', 'Secret', 'Nazi', 'Military', 'Base', '‘Treasure', 'Hunter’', 'in', 'the', 'Arctic', '[Photos]']\",\n",
       " \"['US', 'Officials', 'See', 'No', 'Link', 'Between', 'Trump', 'and', 'Russia']\",\n",
       " \"['Re:', 'Yes,', 'There', 'Are', 'Paid', 'Government', 'Trolls', 'On', 'Social', 'Media,', 'Blogs,', 'Forums', 'And', 'Websites']\",\n",
       " \"['In', 'Major', 'League', 'Soccer,', 'Argentines', 'Find', 'a', 'Home', 'and', 'Success', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Wells', 'Fargo', 'Chief', 'Abruptly', 'Steps', 'Down', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Anonymous', 'Donor', 'Pays', '$2.5', 'Million', 'To', 'Release', 'Everyone', 'Arrested', 'At', 'The', 'Dakota', 'Access', 'Pipeline']\",\n",
       " \"['FBI', 'Closes', 'In', 'On', 'Hillary!']\",\n",
       " \"['Chuck', 'Todd:', '’BuzzFeed', 'Did', 'Donald', 'Trump', 'a', 'Political', 'Favor’', '-', 'Breitbart']\",\n",
       " \"['News:', 'Hope', 'For', 'The', 'GOP:', 'A', 'Nude', 'Paul', 'Ryan', 'Has', 'Just', 'Emerged', 'From', 'An', 'Ayahuasca', 'Tent', 'With', 'Visions', 'Of', 'A', 'New', 'Republican', 'Party']\",\n",
       " \"['Monica', 'Lewinsky,', 'Clinton', 'Sex', 'Scandal', 'Set', 'for', '’American', 'Crime', 'Story’']\",\n",
       " \"['Rob', 'Reiner:', 'Trump', 'Is', '’Mentally', 'Unstable’', '-', 'Breitbart']\",\n",
       " \"['Massachusetts', 'Cop’s', 'Wife', 'Busted', 'for', 'Pinning', 'Fake', 'Home-Invasion', 'Robbery', 'on', 'Black', 'Lives', 'Matter']\",\n",
       " \"['Abortion', 'Pill', 'Orders', 'Rise', 'in', '7', 'Latin', 'American', 'Nations', 'on', 'Zika', 'Alert', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Nukes', 'and', 'the', 'UN:', 'a', 'Historic', 'Treaty', 'to', 'Ban', 'Nuclear', 'Weapons']\",\n",
       " \"['EXCLUSIVE:', 'Islamic', 'State', 'Supporters', 'Vow', 'to', '‘Shake’', 'the', 'West', 'Following', 'Manchester', 'Terrorist', 'Massacre', '-', 'Breitbart']\",\n",
       " \"['Humiliated', 'Hillary', 'Tries', 'To', 'Hide', 'What', 'Camera', 'Caught', '15', 'Mins', 'Before', 'Rally']\",\n",
       " \"['Andrea', 'Tantaros', 'of', 'Fox', 'News', 'Claims', 'Retaliation', 'for', 'Sex', 'Harassment', 'Complaints', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['How', 'Hillary', 'Clinton', 'Became', 'a', 'Hawk', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Chuck', 'Todd', 'to', 'BuzzFeed', 'EIC:', '’You', 'Just', 'Published', 'Fake', 'News’', '-', 'Breitbart']\",\n",
       " \"['Israel', 'is', 'Becoming', 'Pivotal', 'to', 'China’s', 'Mid-Eastern', 'Calculus', 'http://journal-neo.org/2016/11/07/israel-is-becoming-pivotal-to-china-s-mid-eastern-calculus/']\",\n",
       " \"['Having', 'Won,', 'Boris', 'Johnson', 'and', '‘Brexit’', 'Leaders', 'Fumble', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Texas', 'Oil', 'Fields', 'Rebound', 'From', 'Price', 'Lull,', 'but', 'Jobs', 'Are', 'Left', 'Behind', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Bayer', 'Deal', 'for', 'Monsanto', 'Follows', 'Agribusiness', 'Trend,', 'Raising', 'Worries', 'for', 'Farmers', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Russia', 'Moves', 'to', 'Ban', 'Jehovah’s', 'Witnesses', 'as', '‘Extremist’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Re:', 'Why', 'We', 'Are', 'Still', 'In', '‘The', 'Danger', 'Zone’', 'Until', 'January', '20th,', '2017']\",\n",
       " \"['Open', 'Thread', '(NOT', 'U.S.', 'Election)', '2016-39']\",\n",
       " \"['Democrat', 'Gutierrez', 'Blames', 'Chicago’s', 'Gun', 'Violence', 'on', 'NRA', '-', 'Breitbart']\",\n",
       " \"['Avoiding', 'Peanuts', 'to', 'Avoid', 'an', 'Allergy', 'Is', 'a', 'Bad', 'Strategy', 'for', 'Most', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['MRI', 'Shows', 'Detailed', 'Images', 'of', '20-Week', 'Unborn', 'Babies', '-', 'Breitbart']\",\n",
       " \"['4', 'of', 'the', 'Best', 'Kinds', 'of', 'Milk', 'That', 'Aren’t', 'Dairy']\",\n",
       " \"['Ryan', 'Lochte', 'Dropped', 'by', 'Speedo', 'USA', 'and', 'Other', 'Retailers', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Can', 'I', 'have', 'one', 'girlfriend', 'without', 'you', 'bastards', 'f**king', 'it', 'up?', 'asks', 'Harry']\",\n",
       " \"['Conservatives', 'Urge', 'Sessions', 'to', 'Clean', 'Out', 'Obama’s', 'Civil', 'Rights', 'Division', '-', 'Breitbart']\",\n",
       " \"['Internal', 'Inquiry', 'Sealed', 'the', 'Fate', 'of', 'Roger', 'Ailes', 'at', 'Fox', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Press', 'TV', 'Debate:', 'Duff', 'on', 'Lebanon,', 'Hezbollah', 'and', 'Aoun’s', 'Presidency', '(3', 'videos)']\",\n",
       " '[\\'СМИ\\', \\'Сербии\\', \\'приписали\\', \\'россиянам\\', \\'\"подготовку\\', \\'терактов\"\\', \\'в\\', \\'Черногории\\']',\n",
       " \"['Samsung,', 'After', 'Combustible', 'Galaxy', 'Note', '7,', 'Unveils', 'New', 'Smartphone', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Poland', 'Vows', 'Referendum', 'on', 'Migrant', 'Quota', 'Amidst', 'EU', 'Pressure:', '’The', 'Public’s', 'Voice', 'Will', 'Be', 'Heard’', '-', 'Breitbart']\",\n",
       " \"['Sparking', 'An', 'Inner', 'Revolution']\",\n",
       " \"['Dueling', 'Drills', 'Further', 'Increase', 'NATO', 'Russia', 'Tensions']\",\n",
       " \"['Study:', 'More', 'Than', 'Half', 'of', 'Car', 'Crashes', 'Involve', 'Drivers', 'Distracted', 'by', 'Cell', 'Phones', '-', 'Breitbart']\",\n",
       " 'nan',\n",
       " \"['Samantha', 'Bee', 'Explores', 'the', 'Dangerous', 'Rise', 'of', 'the', 'Alt', 'Right', 'in', 'American', 'Politics', '(Video)']\",\n",
       " \"['The', 'Trump', 'Election', 'Will', 'Spark', 'More', 'Individual', '&', 'Collective', 'Healing']\",\n",
       " \"['Ep.', '544', 'FADE', 'to', 'BLACK', 'Jimmy', 'Church', 'w/', 'Laura', 'Eisenhower', ':', 'Restoring', 'the', 'Balance', '[VIDEO]']\",\n",
       " \"['Cognition', 'and', 'True', 'Islam', '-', 'A', 'Book', 'Review']\",\n",
       " \"['If', 'Donald', 'Trump', 'Wins', 'The', 'Election,', 'It', 'Will', 'Be', 'The', 'Biggest', 'Miracle', 'In', 'US', 'Political', 'History']\",\n",
       " \"['Mindful', 'Eating', 'as', 'Way', 'to', 'Fight', 'Bingeing', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'Major', 'Potential', 'Impact', 'of', 'a', 'Corporate', 'Tax', 'Overhaul', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['I', 'wonder', 'what', 'GLP', 'will', 'be', 'like', 'the', 'day', 'after', 'the', 'election?']\",\n",
       " \"['3', 'Makers', 'of', 'World’s', 'Smallest', 'Machines', 'Awarded', 'Nobel', 'Prize', 'in', 'Chemistry', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Massive', 'Anti-Trump', 'Protests,', 'Union', 'Square', 'NYC', 'Live', 'Stream']\",\n",
       " \"['Review:', '‘Lion’', 'Brings', 'Tears', 'for', 'a', 'Lost', 'Boy,', 'Wiped', 'Dry', 'by', 'Google', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['U.S.', 'General:', 'Islamic', 'State', 'Chemical', 'Attack', 'Had', '’No', 'Impact’', 'on', 'U.S.', 'Forces']\",\n",
       " \"['Jury', 'finds', 'all', 'Oregon', 'standoff', 'defendants', 'not', 'guilty', 'of', 'federal', 'conspiracy,', 'gun', 'charges']\",\n",
       " \"['Clinton', 'Campaign', 'STUNNED', 'As', 'FBI', 'Reportedly', 'Reopens', 'Probe', 'Into', 'Hillary', 'Clinton', 'Emails']\",\n",
       " \"['Pence', 'Will', 'Speak', 'at', 'Anti-Abortion', 'Rally', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Bernie', 'Sanders', 'Says', 'What', 'The', 'Media', 'Won’t:', 'Trump', 'Is', 'A', 'Gutless', 'Political', 'Coward']\",\n",
       " \"['How', 'To', 'Make', 'Briquettes', 'From', 'Daily', 'Waste']\",\n",
       " '[\\'Treason!\\', \\'NYT\\', \\'vows\\', \"\\'rededication\\'\", \\'to\\', \\'reporting!\\']',\n",
       " \"['Dress', 'Like', 'a', 'Woman?', 'What', 'Does', 'That', 'Mean?', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['At', '91,', 'Ella', 'Brennan', 'Still', 'Feeds', '(and', 'Leads)', 'New', 'Orleans', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Pressing', 'Asia', 'Agenda,', 'Obama', 'Treads', 'Lightly', 'on', 'Human', 'Rights', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Democrats', 'Have', 'a', '60', 'Percent', 'Chance', 'to', 'Retake', 'the', 'Senate', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['News:', 'PR', 'Disaster:', 'The', 'President', 'Of', 'Panasonic', 'Has', 'Been', 'Forced', 'To', 'Resign', 'After', '60,000', 'Panasonic', 'TVs', 'Ascended', 'To', 'Heaven', 'Without', 'Warning']\",\n",
       " \"['Judge', 'spanks', 'transgender-obsessed', 'Obama:', 'You', 'lie!', '|', 'RedFlag', 'News']\",\n",
       " \"['51', 'U.S.', 'Diplomats', 'Urge', 'Strikes', 'Against', 'Assad', 'in', 'Syria', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Franken', 'Calls', 'for', '’Independent', 'Investigation’', 'Into', 'Trump’s', '’Putin', 'Crush’', '-', 'Breitbart']\",\n",
       " \"['Louisiana,', 'Simone', 'Biles,', 'U.S.', 'Presidential', 'Race:', 'Your', 'Tuesday', 'Evening', 'Briefing', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Turkey', 'Threatens', 'to', 'Open', 'Migrant', '’Land', 'Passage’', 'to', 'Europe', 'After', 'Row', 'With', 'Dutch']\",\n",
       " \"['Huma’s', 'Weiner', 'Dogs', 'Hillary']\",\n",
       " \"['Colin', 'Kaepernick', 'Starts', 'Black', 'Panther-Inspired', 'Youth', 'Camp?', 'Wow!']\",\n",
       " \"['Trump’s', 'Immigration', 'Policies', 'Explained', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Mary', 'Tyler', 'Moore', 'Is', 'Mourned', 'by', 'Dick', 'Van', 'Dyke', 'and', 'Other', 'Stars', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Poison']\",\n",
       " \"['Trump', 'Fans', 'Rally', 'Across', 'the', 'Nation', 'to', 'Support', 'the', 'President', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\\'Fox\\', \\'Biz\\', \\'Reporter\\', \"Can\\'t\", \\'Help\\', \\'But\\', \\'Bash\\', \\'Clinton’s\\', \\'Rally\\', \\'After\\', \\'Covering\\', \\'Trump’s\\', \\'Packed\\', \\'Event\\', \\'Day\\', \\'Before\\']',\n",
       " \"['11', 'Fiction', 'Podcasts', 'Worth', 'a', 'Listen', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Mike', 'Birbiglia’s', '6', 'Tips', 'for', 'Making', 'It', 'Small', 'in', 'Hollywood.', 'Or', 'Anywhere.', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Investment', 'Strategist', 'Forecasts', 'Collapse', 'Timeline:', '‘The', 'Last', 'Gasp', 'Of', 'This', 'Economic', 'Cycle', 'Will', 'Come', 'In', '2018’']\",\n",
       " \"['Venezuela', 'Muzzles', 'Legislature,', 'Moving', 'Closer', 'to', 'One-Man', 'Rule', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\\'Whether\\', \"it\\'s\", \\'John\\', \\'McCain,\\', \\'Mitt\\', \\'Romney\\', \\'or\\', \\'Donald\\', \\'Trump,\\', \\'Democrats\\', \\'always\\', \\'run\\', \"\\'War\", \\'on\\', \"Women\\'\", \\'tactic\\', \\'to\\', \\'destroy\\', \\'the\\', \\'Republican\\', \\'candidate\\']',\n",
       " \"['Breitbart', 'News', 'Daily:', 'Trump', 'Boom', '-', 'Breitbart']\",\n",
       " \"['White', 'House', 'Confirms', 'More', 'Gitmo', 'Transfers', 'Before', 'Obama', 'Leaves', 'Office']\",\n",
       " \"['The', 'Geometry', 'of', 'Energy', 'and', 'Meditation', 'of', 'Buddha']\",\n",
       " \"['Poll:', 'Most', 'Voters', 'Have', 'Not', 'Heard', 'of', 'Democratic', '2020', 'Election', 'Candidates', '-', 'Breitbart']\",\n",
       " \"['Migrants', 'Confront', 'Judgment', 'Day', 'Over', 'Old', 'Deportation', 'Orders', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['M.I.T.,', 'N.Y.U.', 'and', 'Yale', 'Are', 'Sued', 'Over', 'Retirement', 'Plan', 'Fees', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Technocracy:', 'The', 'Real', 'Reason', 'Why', 'The', 'UN', 'Wants', 'Control', 'Over', 'The', 'Internet']\",\n",
       " \"['American', 'Drivers', 'Regain', 'Appetite', 'for', 'Gas', 'Guzzlers', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Hillary', 'Clinton', 'Builds', '$150', 'Million', 'War', 'Chest,', 'Doubling', 'Donald', 'Trump', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump', 'Catches', 'What', 'Sick', 'Reporter', 'Snuck', 'In', 'Interview,', 'Has', 'Priceless', 'Response']\",\n",
       " \"['All', '100', 'Senators', 'Contacted', 'Russian', 'Government', 'This', 'Week']\",\n",
       " \"['10', 'Images', 'That', 'Perfectly', 'Illustrate', 'the', 'Struggle', 'Against', 'the', 'Dakota', 'Access', 'Pipeline']\",\n",
       " \"['Washington', 'State', 'Takes', '10', 'Refugees,', '0', 'Muslim', 'Rest', 'of', 'Country', 'Takes', '189', 'Refugees,', '151', 'Muslim', '-', 'Breitbart']\",\n",
       " \"['Houthi', 'forces', 'capture', 'Saudi', 'military', 'base', 'in', 'Asir', '-', 'Russia', 'News', 'Now']\",\n",
       " \"['NCAA,', 'Big', '12', 'Keeps', 'Watchful', 'Eye', 'on', 'Texas', 'Bathroom', 'Bill', '-', 'Breitbart']\",\n",
       " \"['Massive', 'ESPN', 'Financial,', 'Subscriber', 'Losses', 'Drag', 'Down', 'Disney’s', 'First-Quarter', 'Sales', '-', 'Breitbart']\",\n",
       " \"['Megyn', 'Kelly,', 'Contract', 'Set', 'to', 'Expire', 'Next', 'Year,', 'Is', 'Primed', 'for', 'the', 'Big', 'Show', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Teacher', 'Suspended', 'for', 'Allowing', 'Students', 'to', 'Hit', 'Trump', 'Pinata', 'for', 'Cinco', 'de', 'Mayo']\",\n",
       " \"['BREAKING', ':', 'Trump', 'Expressed', 'Concern', 'Over', 'Anthony', 'Weiner’s', '“Illegal', 'Access”', 'to', 'Classified', 'Info', '2', 'Months', 'ago', '–', 'TruthFeed']\",\n",
       " \"['Snap', 'Shares', 'Leap', '44%', 'in', 'Debut', 'as', 'Investors', 'Doubt', 'Value', 'Will', 'Vanish', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Clinton', 'Campaign', 'Chair', 'Had', 'Dinner', 'With', 'Top', 'DOJ', 'Official', 'One', 'Day', 'After', 'Hillary’s', 'Benghazi', 'Hearing']\",\n",
       " \"['TV', 'Series', 'About', '’First', 'Female', 'MLB', 'Pitcher’', 'Canned', 'After', 'One,', 'Low-Rated', 'Season', '-', 'Breitbart']\",\n",
       " \"['Seeking', 'Best', 'Fit,', 'Women’s', 'Final', 'Four', 'Returns', 'to', 'Friday-Sunday', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['A', 'Proposal', 'for', 'a', 'Canadian', 'National', 'Bird', 'Ruffles', 'Feathers', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Review:', 'Beyoncé', 'Makes', '‘Lemonade’', 'Out', 'of', 'Marital', 'Strife', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump', 'to', 'Ask', 'for', 'Sharp', 'Increases', 'in', 'Military', 'Spending,', 'Officials', 'Say', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['Why', 'Is', 'This', 'Not', 'Watergate?', 'Smoking', 'Gun', 'Emails', 'Discuss', '“Cleaning', 'Up”', 'Obama/Hillary', 'Emails']\",\n",
       " \"['“Chapo', 'Trap', 'House”:', 'New', 'Left-Wing', 'Podcast', 'is', 'a', 'Flagrant', 'Rip-Off', 'of', 'The', 'Right', 'Stuff']\",\n",
       " \"['Taiwan', 'Responds', 'After', 'China', 'Sends', 'Carrier', 'to', 'Taiwan', 'Strait', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['The', 'Mother', 'Of', 'All', 'October', 'Surprises—Why', 'The', 'House', 'Of', 'Cards', 'Will', 'Now', 'Come', 'Tumbling', 'Down']\",\n",
       " \"['Explosive', 'Assange/Pilger', 'Interview', 'on', 'US', 'Election:', 'Expect', 'Riots', 'if', 'Hillary', 'Wins']\",\n",
       " \"['Telescope', 'That', '‘Ate', 'Astronomy’', 'Is', 'on', 'Track', 'to', 'Surpass', 'Hubble', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Closed', 'Afghan-Pakistani', 'Border', 'Is', 'Becoming', '‘Humanitarian', 'Crisis’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['TV', 'Anchors', 'Arrive', 'at', 'the', 'White', 'House', 'for', 'Lunch', 'with', 'Donald', 'Trump', '-', 'Breitbart']\",\n",
       " \"['Pelosi:', 'Republicans', 'Should', 'Tell', 'Trump', 'He’s', '’Bringing', 'Dishonor’', 'to', 'the', 'Presidency', '-', 'Breitbart']\",\n",
       " \"['The', 'Beautiful', 'Prehistoric', 'World:', 'Is', 'Earth', 'Now', 'a', 'Wasteland?']\",\n",
       " \"['I', 'Ignored', 'Trump', 'News', 'for', 'a', 'Week.', 'Here’s', 'What', 'I', 'Learned.', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Donald', 'Trump', 'Unveils', 'Plan', 'for', 'Families', 'in', 'Bid', 'for', 'Women’s', 'Votes', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['EU,', 'Finland', 'can', 'help', 'settlement', 'of', 'Syria', 'conflict:', 'Iran', 'parliament', 'speaker']\",\n",
       " \"['FBI', 'Director', 'Comey’s', '‘Leaked’', 'Memo', 'Explains', 'Why', 'He’s', 'Reopening', 'the', 'Clinton', 'Email', 'Case']\",\n",
       " \"['Montana', 'Democrats', 'Vote', 'Against', 'Bill', 'Banning', 'Sharia', 'Law,', 'Call', 'It', '’Repugnant’', '-', 'Breitbart']\",\n",
       " \"['The', 'Monsanto', 'Tribunal', 'Is', 'Over.', 'How', 'Did', 'It', 'Go?', 'And', 'What', 'Happens', 'Now?']\",\n",
       " \"['The', 'Shame,', 'The', 'Heartbreak-', 'Another', 'Day', 'In', 'America']\",\n",
       " \"['It’s', 'Official:', 'Simone', 'Biles', 'Is', 'the', 'World’s', 'Best', 'Gymnast', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['It', 'Literally', 'Hurts', 'My', 'Brain', 'to', 'Read', 'the', 'Economic', 'Idiocy', 'Emitted', 'by', 'Trumpkins', '|', 'Libertarian']\",\n",
       " \"['Gorafi', 'Magazine', ':', 'Entretien', 'exclusif', 'avec', 'Barack', 'Obama', '«', 'Plus', 'rien', 'à', 'secouer.', 'Démerdez-vous', '»', '>>', 'Le', 'Gorafi']\",\n",
       " \"['U.N.', 'Secretary', 'General', 'Complains', 'That', 'The', '‘Masses', 'Have', 'Rejected', 'Globalism’', 'In', 'Favor', 'Of', 'Nationalism']\",\n",
       " \"['Trump', 'Bollywood', 'Ad', 'Meant', 'To', 'Sway', 'Indian', 'American', 'Voters', 'Is', 'An', 'Hilarious', 'Fail', '(VIDEO)']\",\n",
       " \"['FBI', 'Finds', 'Previously', 'Unseen', 'Hillary', 'Clinton', 'Emails', 'On', 'Weiner’s', 'Laptop']\",\n",
       " \"['2', 'Years', 'After', 'This', 'American', 'Journalist', 'Was', 'Killed,', 'Her', '‘Conspiracy', 'Theories’', 'on', 'Syria', 'are', 'Proven', 'as', 'Facts']\",\n",
       " \"['Report:', 'Illegal', 'Aliens', 'Forego', 'Food', 'Stamps', 'to', 'Stay', 'off', 'Trump’s', 'Radar']\",\n",
       " '[\\'Make\\', \\'Netherlands\\', \\'Great\\', \\'Again!\\', \\'Hahaha\\', \"It\\'s\", \\'Spreading\\', \\'Worldwide!\\']',\n",
       " \"['Four', 'killed,', '10', 'Injured', 'in', 'Jerusalem', 'Truck-Ramming', 'Terror', 'Attack']\",\n",
       " \"['The', 'Leader', 'Salutes', 'Comrade', 'Newt', 'on', 'Brutal', 'Megyn', '[sic]', 'Kelly', 'Beatdown:', '“We', 'Don’t', 'Play', 'Games”']\",\n",
       " \"['Students', 'At', 'Black', 'College', 'Just', 'Got', 'Beaten', 'And', 'Maced', 'For', 'Protesting', 'KKK’s', 'David', 'Duke']\",\n",
       " \"['Despite', 'Strict', 'Gun', 'Control,', 'One', '’Child', 'or', 'Youth’', 'Shot', 'Every', 'Day', 'in', 'Ontario']\",\n",
       " \"['The', 'Rise', 'of', 'the', 'Internet', 'Fan', 'Bully', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['A', 'Newly', 'Vibrant', 'Washington', 'Fears', 'That', 'Trump', 'Will', 'Drain', 'Its', 'Culture', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Fed', 'Holds', 'Interest', 'Rates', 'Steady', 'and', 'Plans', 'Slower', 'Increases', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'Battle', 'at', 'UNESCO']\",\n",
       " \"['The', 'Latest', 'Test', 'for', 'the', 'White', 'House?', 'Pulling', 'Off', 'Its', 'Easter', 'Egg', 'Roll', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Burlesque', 'Dancer', 'Fired,', 'Investigated', 'by', 'Secret', 'Service', 'for', 'Trump', 'Assassination', 'Tweet', '-', 'Breitbart']\",\n",
       " \"['What', 'the', 'Clintons', 'Did', 'to', 'Haiti']\",\n",
       " \"['For', 'Cuomo', 'and', 'Christie,', 'Parallel', 'Paths', 'to', 'the', 'Top,', 'and', 'Trouble', 'When', 'They', 'Got', 'There', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\\'The\\', \\'Top\\', \\'10\\', \\'Places\\', \\'In\\', \\'The\\', \\'World\\', \"You\\'re\", \\'NOT\\', \\'Allowed\\', \\'To\\', \\'Visit\\']',\n",
       " \"['New', 'Study', 'Links', 'Fluoride', 'Consumption', 'To', 'Hypothyroidism,', 'Weight', 'Gain,', 'And', 'Worse']\",\n",
       " \"['James', 'Mattis', 'Is', 'a', 'Secretary', 'of', 'Offense']\",\n",
       " \"['A', 'Black', 'Church', 'Was', 'Just', 'Burned', 'And', 'Spray-Painted', '“Vote', 'Trump”']\",\n",
       " \"['Sears', 'Agrees', 'to', 'Sell', 'Craftsman', 'to', 'Stanley', 'Black', '&amp', 'Decker', 'to', 'Raise', 'Cash', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Takata', 'Chief', 'Executive', 'to', 'Resign', 'as', 'Financial', 'Pressure', 'Mounts', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Goodbye,', 'for', 'Good,', 'to', 'Black', 'Sabbath', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Teen', '’Geisha', 'Dolls’', 'Gang', 'Busted', 'for', 'Armed', 'Robberies', '-', 'Breitbart']\",\n",
       " \"['Mohamad', 'Khweis:', 'Another', '“Virginia', 'Man”', '(Palestinian-American', 'Muslim)', 'Charged', 'With', 'Terrorism']\",\n",
       " \"['Remember', 'This', 'When', 'You', 'Talk', 'About', 'Standing', 'Rock']\",\n",
       " \"['Price', 'on', 'Obamacare', 'Replacement:', '‘Nobody', 'Will', 'Be', 'Worse', 'Off', 'Financially’', '-', 'Breitbart']\",\n",
       " \"['VA', 'fails', 'to', 'properly', 'examine', 'thousands', 'of', 'veterans']\",\n",
       " \"['Trump', 'family', 'already', '‘sworn', 'to', 'secrecy’', 'about', 'faked', 'Moon', 'landings.', 'More', 'soon.']\",\n",
       " \"['Sports', 'Writer:', 'NFL', 'Great', 'Jim', 'Brown’s', 'Decades', 'of', 'Civil', 'Rights', 'Work', 'is', 'Erased', 'for', 'Saying', 'Nice', 'Things', 'About', 'Donald', 'Trump', '-', 'Breitbart']\",\n",
       " \"['Spider-pig', 'Found', 'in', 'Amazon', 'Rain', 'Forest']\",\n",
       " \"['‘I', 'Can', 'Watch', 'It', 'on', 'TV’:', 'Excuses', 'for', 'Republicans', 'Skipping', 'a', 'Donald', 'Trump', 'Convention', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['This', 'Open', 'Letter', 'To', 'Trump', 'Voters', 'Just', 'Told', 'It', '“Like', 'It', 'Is”']\",\n",
       " \"['Comment', 'on', 'This', 'Powerful', 'Corporate', 'Lobby', 'Is', 'Quietly', 'Backing', 'Hillary', '—', 'and', 'Nobody’s', 'Talking', 'About', 'It', 'by', 'runsinquicksand']\",\n",
       " \"['Teacher', 'unsure', 'about', 'getting', 'smashed', 'eighth', 'night', 'running']\",\n",
       " \"['Hijacking', 'Ends', 'Peacefully', 'After', 'Libyan', 'Airliner', 'Lands', 'in', 'Malta', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['You', 'Don’t', 'Like', 'the', 'Girls', 'in', '‘Girls’?', 'That’s', 'Its', 'Genius.', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Scientists', 'Say', 'Canadian', 'Bacteria', 'Fossils', 'May', 'Be', 'Earth’s', 'Oldest', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Pro-Government', 'Forces', 'Advance', 'in', 'Syria', 'Amid', 'Talk', 'of', 'U.S.-Russia', 'Cooperation', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['WHO', 'cancer', 'agency', 'under', 'fire', 'for', 'withholding', '‘carcinogenic', 'glyphosate’', 'documents']\",\n",
       " \"['Work.', 'Walk', '5', 'Minutes.', 'Work.', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Militants', 'Used', 'Toxic', 'Gas', 'Near', 'Aleppo', 'Airport', '–', 'Report']\",\n",
       " \"['Steve', 'Harvey', 'Talks', 'Housing', 'With', 'President-Elect', 'Trump', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Coalition:', 'U.S.', 'Troops', 'Fighting', 'in', 'Mosul', 'Offensive', '’Come', 'Under', 'Fire’']\",\n",
       " \"['UK', 'citizens', 'and', 'war', 'heroes', 'get', 'cheap', 'pre-fab', 'houses', 'while', 'Muslim', 'colonizers', 'get', 'taxpayer-funded', 'luxurious', 'council', 'homes']\",\n",
       " \"['After', 'Vets', 'Fight', 'War,', 'Feds', 'Demand', 'Money', 'Back…', 'But', 'Illegals', 'And', 'Refugees', 'Can', 'Keep', 'Their', 'Money']\",\n",
       " \"['Mr.', 'Trump’s', 'Wild', 'Ride', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Here', 'Is', 'How', 'FBI', 'Director', 'Comey', 'BAMBOOZLED', 'The', 'DOJ,', 'CONGRESS,', 'And', 'The', 'CLINTONS', 'All', 'At', 'Once']\",\n",
       " \"['20', 'Foods', 'That', 'Naturally', 'Unclog', 'Arteries', 'and', 'Prevent', 'Heart', 'Attacks']\",\n",
       " \"['Death', 'of', 'the', '‘Two-State', 'Solution’']\",\n",
       " \"['Watch:', 'Police', 'viciously', 'attack,', 'arrest', 'peaceful', 'protesters', 'at', 'DAPL', 'including', 'children', 'and', 'the', 'elderly']\",\n",
       " 'nan',\n",
       " \"['Comment', 'on', 'Parents', 'on', 'a', 'Date', 'Were', 'Asleep', 'in', 'Car', 'When', 'Cops', 'Arrived', 'and', 'Killed', 'Them', 'Both', 'by', 'FaceSpace']\",\n",
       " \"['Could', 'Teaching', 'Men', 'to', 'Nurture', 'Counteract', 'Our', 'Culture', 'of', 'Toxic', 'Masculinity?']\",\n",
       " \"['Donald', 'Trump’s', 'Team', 'Shows', 'Few', 'Signs', 'of', 'Post-Election', 'Moderation', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Miami', 'Beach', 'Tries', 'to', 'Tame', 'Its', 'Most', 'Raucous', 'Street', '(but', 'the', 'Fishbowl', 'Drinks', 'Can', 'Stay)', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Doctors', 'Mysteriously', 'Found', 'Dead', 'After', 'Summit', 'For', 'Breakthrough', 'Cure', 'For', 'Cancer']\",\n",
       " \"['Donald', 'Trump,', 'the', 'Unsinkable', 'Candidate', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Shocking', 'New', 'Mock', 'Hillary', 'Ad', 'Campaign', 'Warns', 'She’ll', 'Take', 'Us', 'to', 'War:', '#EnlistforHer,', '#FightforHer,', '#DieforHer']\",\n",
       " \"['The', 'Failure', 'of', 'US', 'Democracy']\",\n",
       " \"['EXCLUSIVE', '--', 'Family', 'of', 'Slain', 'Border', 'Patrol', 'Agent', 'Brian', 'Terry', 'Says', 'Eric', 'Holder', 'Among', '’The', 'Real', 'Criminals’', 'Responsible']\",\n",
       " \"['Trump', 'Tells', 'Reporters:', '‘Walls', 'Work', '-', 'Just', 'Ask', 'Israel’', '-', 'Breitbart']\",\n",
       " \"['Will', 'America', 'Survive', 'the', 'Next', '4', 'Years?']\",\n",
       " \"['Commissioner', 'Starts', 'to', 'Press', 'Cleveland', 'Indians', 'About', 'Logo', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Re:', 'Babylon', 'Mystery', 'Religion', 'Series']\",\n",
       " \"['The', 'UN', 'Plans', 'To', 'Implant', 'Everyone', 'With', 'A', 'Biometric', 'ID,', 'This', 'Is', 'Not', 'A', 'Drill']\",\n",
       " \"['Trump', 'Attacks', 'Senator’s', 'Credibility', 'Over', 'Gorsuch’s', 'Comments', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Clinton', 'Advisor', 'LOSES', 'IT', 'In', 'Leaked', 'Email', 'Over', 'Hillary’s', 'Illegal', 'Activity']\",\n",
       " \"['Art', 'Laffer:', '’Paul', 'Ryan’s', 'Just', 'About', 'Perfect', 'Right', 'Now’', '-', 'Breitbart']\",\n",
       " \"['How', 'Donald', 'Trump', 'Will', 'Be', 'Blamed', 'For', 'Economic', 'Crash']\",\n",
       " \"['New', 'World', 'Order', 'Pushes', 'Back', 'on', 'Brexit', 'Revolution']\",\n",
       " \"['Pokemon', 'Go', 'players', 'are', 'inadvertently', 'stopping', 'people', 'committing', 'suicide', 'in', 'Japan']\",\n",
       " \"['California', 'Senate', 'Race', 'Is', 'a', 'Tale', 'of', 'Diversity', 'and', 'a', 'Flailing', 'G.O.P.', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Exclusive:', 'Source', 'Says', 'Megyn', 'Kelly', '‘Would', 'Not', 'Be', 'Welcomed', 'Back’', 'at', 'Fox', 'News']\",\n",
       " \"['In', 'Break', 'With', 'Precedent,', 'Obama', 'Envoys', 'Are', 'Denied', 'Extensions', 'Past', 'Inauguration', 'Day', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['How', 'Will', '‘Brexit’', 'Vote', 'Go?', 'Monty', 'Python', 'May', 'Offer', 'Clue', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Blind', 'Mystic', 'Who', 'Predicted', '9/11', 'Has', 'Bad', 'News', 'About', 'Trump']\",\n",
       " \"['Total', 'Vetting', 'Fail:', 'Left-Wing', 'Snowden', 'Fan', 'Girl', 'Reality', 'Winner', 'Gets', 'Access', 'to', 'Our', 'NSA', 'Secrets']\",\n",
       " 'nan',\n",
       " \"['In', 'Somalia,', 'U.S.', 'Escalates', 'a', 'Shadow', 'War', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Free', 'Care', 'a', '‘Blessing’', 'for', 'Victims', 'of', 'Orlando', 'Nightclub', 'Attack', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'most', 'durably', 'Democratic', 'county', 'in', 'the', 'country', 'could', 'go', 'for', 'Trump']\",\n",
       " \"['Fed’s', 'Challenge,', 'After', 'Raising', 'Rates,', 'May', 'Be', 'Existential', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\"\\'Russia\", \\'has\\', \\'no\\', \\'intention\\', \\'of\\', \\'attacking\\', \\'anyone\\', \\'-this\\', \\'is\\', \"absurd,\\'\", \\'says\\', \\'Vladimir\\', \\'Putin\\']',\n",
       " \"['США—КНР:', 'на', 'кого', 'возлагать', 'ответственность', 'за', 'развитие', 'ракетно-ядерной', 'программы', 'КНДР?', '|', 'Новое', 'восточное', 'обозрение']\",\n",
       " \"['F.A.A.', 'Investigates', 'Errant', 'Flight', 'Involving', 'Harrison', 'Ford', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Ask', 'Holly:', 'What’s', 'everyone’s', 'problem?']\",\n",
       " \"['Fed', 'Raises', 'Key', 'Interest', 'Rate,', 'Citing', 'Strengthening', 'Economy', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['La', 'expresión', '“no,', 'lo', 'siguiente”', 'ya', 'es', 'la', 'más', 'utilizada', 'no,', 'lo', 'siguiente,', 'en', 'el', 'castellano']\",\n",
       " \"['As', 'Trump', 'Berates', 'News', 'Media,', 'a', 'New', 'Strategy', 'Is', 'Needed', 'to', 'Cover', 'Him', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['‘Sunk', 'Costs’:', '$58', 'Billion', 'Wasted', 'On', 'Imaginary', 'Weapons', 'Since', '1997']\",\n",
       " \"['Changing', 'the', 'Montenegrin', 'leader', 'does', 'not', 'change', 'the', 'ideology']\",\n",
       " \"['U.S.', 'Drone', 'Strike', 'Targets', 'Taliban', 'Leader', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['U.S.', 'Intelligence:', '‘Expect', 'al-Qaeda', 'attacks', 'Monday', 'in', 'New', 'York,', 'Virginia', 'and', 'Texas’']\",\n",
       " \"['What', 'If', 'I', 'Told', 'You', 'Cannabis', 'Is', 'Great', 'for', 'Reversing', 'Alzheimer’s?']\",\n",
       " \"['Report:', 'Megyn', 'Kelly', 'to', 'Kick', 'Off', 'NBC', 'Show', 'with', 'Kardashian', 'Family', 'Interview']\",\n",
       " \"['Your', 'Local', '1-Percenters', 'May', 'Not', 'Be', 'as', 'Rich', 'as', 'You', 'Think', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Dr.', 'David', 'Duke', 'with', 'Mark', 'Collett', 'of', 'the', 'UK.', 'Collett', 'explains', 'why', 'Duke', '&', 'Trump', 'victories', 'would', 'change', 'politics', 'forever!']\",\n",
       " \"['In', 'Statement', 'to', 'Senate,', 'Wells', 'Fargo', 'Chief', 'Is', '‘Deeply', 'Sorry’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['CNN', 'Statement', 'Distances', 'Network', 'from', 'Buzzfeed', 'Fake', 'News', 'Dossier', '-', 'Breitbart']\",\n",
       " \"['C.E.O.s', 'Ponder', 'a', 'New', 'Game,', 'With', 'Trump’s', 'Rules', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Spicer', 'on', 'Brady’s', 'Stolen', 'Jersey:', '’Another', 'Bad', 'on', 'the', 'Press’', '-', 'Breitbart']\",\n",
       " \"['A', 'Scaredy-Cat’s', 'Investigation', 'Into', 'Why', 'People', 'Enjoy', 'Fear', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'Left’s', 'Vision']\",\n",
       " \"['Showdown', 'Looms', 'as', 'U.S.', 'Questions', 'Chinese', 'Deal', 'for', 'German', 'Chip', 'Designer', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\\'FBI\\', \\'Obtains\\', \\'Warrant\\', \\'To\\', \\'Search\\', \\'Huma\\', \"Abedin\\'s\", \\'Emails\\']',\n",
       " \"['Trump', 'Administration', 'to', 'Take', 'Harder', 'Tack', 'on', 'Trade', 'With', 'China', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Pew:', 'American', 'Trust', 'Level', 'in', 'Federal', 'Government', 'Plummets', 'to', 'Historic', 'Lows', '-', 'Breitbart']\",\n",
       " \"['Islamic', 'State', 'Supporting', 'Former', 'National', 'Guardsman', 'Pleads', 'Guilty', 'to', 'Terrorism', 'Charge', 'in', 'Virginia']\",\n",
       " \"['Western', 'QLD', 'Drovers', 'Show', 'Solidarity', 'With', 'C.U.B', 'Workers', 'By', 'Only', 'Drinking', 'Sauv', 'Blanc', '–', 'The', 'Betoota', 'Advocate']\",\n",
       " \"['Spicer', 'to', 'Reporter:', '’We’re', 'Going', 'to', 'Raise', 'Our', 'Hands', 'Like', 'Big', 'Boys', 'and', 'Girls’', '-', 'Breitbart']\",\n",
       " \"['Leaders', 'Applaud', 'Gorsuch', 'Confirmation', 'as', '’Win', 'for', 'Pro-Life', 'Movement’']\",\n",
       " \"['Newsticker', '(974)']\",\n",
       " \"['French', 'Vogue’s', 'March', 'Cover', 'Features', 'a', 'Transgender', 'Model', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump', 'Veers', 'From', 'Party', 'Line', 'on', 'Gun', 'Control', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['How', 'the', 'Oligarchy', 'Has', 'Prepared', 'the', 'Groundwork', 'for', 'Stealing', 'the', 'Election']\",\n",
       " \"['Ya', 'hay', 'más', 'reencuentros', 'de', '“Operación', 'Triunfo”', 'que', 'ediciones', 'de', '“Operación', 'Triunfo”']\",\n",
       " \"['At', 'Chatsworth', 'House,', 'a', 'Tale', 'of', 'Five', 'Centuries', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'Uncomfortable', 'Love', 'Affair', 'Between', 'Donald', 'Trump', 'and', 'the', 'New', 'England', 'Patriots', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['John', 'McCain', 'Withdraws', 'Support', 'for', 'Donald', 'Trump', 'After', 'Disclosure', 'of', 'Recording', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'Strange,', 'Unending', 'Limbo', 'of', 'Egypt’s', 'Hosni', 'Mubarak', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Poverty', 'Rose', 'in', '96%', 'of', 'U.S.', 'House', 'Districts,', 'During', 'Obama’s', 'Presidency']\",\n",
       " \"['Huma', 'Abedin', 'Seeks', 'FBI', 'Immunity', 'Deal']\",\n",
       " \"['A', 'Single', 'Mom', 'Escapes', 'the', 'Friend', 'Zone,', 'One', 'Non-Date', 'at', 'a', 'Time', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Boeing', 'Suits', 'Up', 'for', 'Future', 'of', 'Spaceflight', 'with', 'New', 'Spacesuit', 'Design', '-', 'Breitbart']\",\n",
       " \"['Trump', 'Floats', 'an', 'Olive', 'Branch:', 'Might', 'Keep', 'Parts', 'of', 'the', 'Health', 'Law', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['WaPo', 'Tries', 'to', 'Compare', 'Elizabeth', 'Warren', 'Breaking', 'Senate', 'Rules', 'to', 'MILO', 'Being', 'Shut', 'Down', 'by', 'Violent', 'Riot', '-', 'Breitbart']\",\n",
       " \"['A', 'Crumpled', 'School', 'Bus', 'Leaves', 'Chattanooga', 'Dazed', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['She', 'Died', 'And', 'Came', 'Back', 'To', 'Life', 'With', 'This', 'Incredible', 'Message', 'For', 'Humanity']\",\n",
       " \"['How', 'the', 'Fight', 'for', 'a', 'National', 'African-American', 'Museum', 'Was', 'Won', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Father', 'of', 'Manchester', 'Suicide', 'Bomber', 'Arrested', 'in', 'Libya', '-', 'Breitbart']\",\n",
       " \"['4', 'Secrets', 'About', 'True', 'Leaders']\",\n",
       " \"['Muslims', 'DEMAND', 'Locals', 'Don’t', 'Walk', 'Dogs', 'In', 'Public', '–', 'Violation', 'Of', 'Sharia', 'And', '“DISRESPECTS”', 'Them']\",\n",
       " '[\\'Hillary\\', \\'Campaign\\', \\'Now\\', \\'in\\', \\'Bed\\', \\'with\\', \\'PAC\\', \\'Staff\\', \\'That\\', \\'Donated\\', \\'$500K\\', \\'to\\', \\'FBI\\', \"Agent\\'s\", \\'Wife\\', \\'After\\', \\'Investigation\\']',\n",
       " \"['Still', 'Not', 'Over:', 'They', 'Are', 'Trying', 'to', '“Flip', 'the', 'Electoral', 'College”', 'To', 'Block', 'Trump’s', 'Win']\",\n",
       " \"['Al', 'Sharpton', 'to', 'Dems:', 'No', 'Point', 'Appealing', 'to', '‘Archie', 'Bunker’', 'Trump', 'Voters', '-', 'Breitbart']\",\n",
       " \"['Do', 'you', 'think', 'there', 'will', 'be', 'as', 'many', 'doom', 'sayers', 'if', 'trump', 'should', 'get', 'in', 'office', '?']\",\n",
       " \"['Democrats', 'Jump', 'on', 'Sessions', 'Resignation', 'Band', 'Wagon', '-', 'Breitbart']\",\n",
       " \"['Russia', 'and', 'Turkey', 'now', 'sharing', 'intelligence', 'data']\",\n",
       " \"['Alt-Right', 'Architect', 'Glenn', 'Beck', 'Opens', 'Fire', 'On', 'Alt-Right:', '‘Grave', 'Threat', 'To', 'The', 'Republic’', '(AUDIO,', 'TWEET)']\",\n",
       " \"['More', 'Politicized', 'Justice', 'to', 'Protect', 'Hillary']\",\n",
       " \"['North', 'Carolina,', 'Saturated', 'and', 'Surprised,', 'Reels', 'from', 'Hurricane', 'Matthew', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'Lives', 'They', 'Loved:', 'Submit', 'Your', 'Memories', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['Achieving', 'Mindfulness', 'at', 'Work,', 'No', 'Meditation', 'Cushion', 'Required', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['World’s', 'First', 'Zero-Emissions', 'Hydrogen-Powered', 'Passenger', 'Train', 'Unveiled', 'In', 'Germany']\",\n",
       " \"['Confused', 'by', 'Chip', 'Credit', 'Cards?', 'Get', 'in', 'Line', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Britain', 'sends', 'hundreds', 'of', 'soldiers', 'and', 'tanks', 'to', 'Estonia', 'in', 'biggest', 'military', 'deployment', 'since', 'Cold', 'War']\",\n",
       " \"['Brooks:', 'Trump', '’Siding', 'With', 'a', 'Foreign', 'Leader', 'Against', 'the', 'US', 'President’', 'on', 'Israel', 'and', 'Russia', '-', 'Breitbart']\",\n",
       " \"['There’s', 'Toxic', 'Air', 'In', 'Your', 'Home', 'and', 'This', 'Is', 'You', 'Can', 'Get', 'Rid', 'of', 'It', 'Naturally']\",\n",
       " \"['A', 'Connecticut', 'Reader', 'Reports', 'Record', 'Voter', 'Registration–Inspired', 'By', 'Trump']\",\n",
       " \"['Germany', 'Reacts', 'to', 'Merkel-Trump', 'Visit:', '‘Could', 'Have', 'Been', 'a', 'Lot', 'Worse’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Justin', 'Rose', 'Outduels', 'Henrik', 'Stenson', 'for', 'Golf', 'Gold', 'Medal', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Iceland’s', 'Water', 'Cure', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['US', 'to', 'Hold', 'Off', 'on', 'Cyberwar', 'With', 'Russia', 'Until', 'After', 'Election', '-', 'Jason', 'Ditz']\",\n",
       " \"['M.T.A.', 'Shortens', 'L', 'Train', 'Shutdown', 'to', '15', 'Months', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['What', 'Time', 'Will', 'the', 'Presidency', 'Be', 'Decided?', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['U.N.', 'Relief', 'Official', 'Calls', 'Crisis', 'in', 'Aleppo', 'the', '‘Apex', 'of', 'Horror’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['After', 'Berkeley,', 'Treat', 'the', 'Violent,', 'Anti-Speech', 'Left', 'Like', 'the', 'KKK']\",\n",
       " \"['Statistical', 'Propaganda:', 'How', 'many', 'Syrians', 'has', 'US', 'regime-change', 'killed?']\",\n",
       " \"['Why', 'Self-Help', 'Guru', 'James', 'Altucher', 'Only', 'Owns', '15', 'Things', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump’s', 'Religious', 'Liberty', 'Order', 'Gives', 'Sessions', 'Major', 'Leeway', '-', 'Breitbart']\",\n",
       " \"['¡Demoledor', 'amparo', 'contra', 'Salgado,', 'Keiko', 'y', '72', 'congresistas', 'más!']\",\n",
       " \"['In', 'Era', 'of', 'Trump,', 'China’s', 'President', 'Champions', 'Economic', 'Globalization', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'Sad', 'Saga', 'Of', 'John', 'Walker', 'Lindh,', 'Rebel', 'Without', 'A', 'Clue']\",\n",
       " \"['Courts', 'Disagree', 'Over', 'Michigan', 'Vote', 'Recount', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Bernie', 'Sanders’s', 'Feud', 'With', 'the', 'Democratic', 'Leadership', 'Heats', 'Up', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\\'The\\', \\'shortest\\', \\'and\\', \\'most\\', \\'powerful\\', \\'explanation\\', \\'of\\', \"Trump\\'s\", \\'victory\\', \\'I\\', \\'have\\', \\'ever\\', \\'seen\\']',\n",
       " \"['Russia', 'Looks', 'to', 'Populate', 'Its', 'Far', 'East.', 'Wimps', 'Need', 'Not', 'Apply.', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'Johnson', 'Amendment,', 'Which', 'Trump', 'Vows', 'to', '‘Destroy,’', 'Explained', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Donald', 'Trump', 'to', 'March', 'for', 'Life:', '‘You', 'Have', 'My', 'Full', 'Support!’']\",\n",
       " \"['Police', 'Official', 'Found', 'Dead', 'on', 'Long', 'Island,', 'and', 'Suicide', 'Is', 'Suspected', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Life:', 'If', 'You', 'Love', 'Enamel', 'Pins,', 'You’ll', 'Love', 'This', 'Etsy', 'Shop,', 'And', 'If', 'You', 'Don’t', 'Care', 'About', 'Enamel', 'Pins,', 'You’ll', 'Explode', 'Over', 'This', 'Picture', 'Of', 'A', 'Bulldog', 'Puppy,', 'And', 'If', 'That', 'Doesn’t', 'Do', 'It', 'For', 'You,', 'You’ll', 'Lose', 'Your', 'Shit', 'Over', 'This', 'Amazing-Looking', 'Pizza,', 'And', 'If', 'You', 'Don’t,', 'We’ll', 'Find', 'Something', 'For', 'You']\",\n",
       " \"['Nevada:', 'Rep.', 'Election', 'Workers', 'Intimidated']\",\n",
       " \"['Radical', 'Changes', 'Are', 'In', 'Store', 'For', 'The', 'World', 'And', 'Global', 'Markets,', 'Are', 'You', 'Ready?']\",\n",
       " \"['What', 'It’s', 'Like', 'to', 'Make', 'It', 'in', 'Showbiz', 'With', 'Your', 'Best', 'Friend', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump', 'Nominates', 'Neil', 'Gorsuch', 'to', 'the', 'Supreme', 'Court', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Sparks', 'Fly', 'as', 'Bikers', 'for', 'Trump', 'Meet', 'Protesters']\",\n",
       " \"['Bidders', 'Cast', 'Doubt', 'on', 'Seriousness', 'of', 'Mexican', 'Border', 'Wall', 'Projects']\",\n",
       " \"['What’s', 'at', 'stake', 'in', 'the', 'US', 'election?']\",\n",
       " \"['Israel', 'Approves', 'Additional', 'Funding', 'for', 'Settlements', 'in', 'West', 'Bank', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['TX', 'Gov', 'Abbott:', 'I', 'Will', 'Sign', 'Legislation', 'That', 'Could', 'Put', 'Sheriffs', 'of', 'Sanctuary', 'Cities', 'in', 'Jail', '-', 'Breitbart']\",\n",
       " \"['Donald', 'Trump', 'Holds', '‘Thank', 'You’', 'Rally', 'in', 'Cincinnati,', 'and', 'Announces', 'His', 'Pick', 'for', 'Defense', 'Secretary', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Some', 'Lawmakers', 'Now', 'Look', 'to', 'Bipartisanship', 'on', 'Health', 'Care', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Inside', 'the', 'Conservative', 'Push', 'for', 'States', 'to', 'Amend', 'the', 'Constitution', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Donald', 'Trump', 'Tells', 'N.R.A.', 'Hillary', 'Clinton', 'Wants', 'to', 'Let', 'Violent', 'Criminals', 'Go', 'Free', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Pope', 'Francis,', 'Trump,', 'Japan:', 'Your', 'Tuesday', 'Briefing', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['#MayorsStand4All', 'Day', 'Touts', 'Support', 'for', 'Illegal', 'Immigrants']\",\n",
       " \"['Trump’s', 'campaign', 'for', 'celebrity']\",\n",
       " \"['Democrats', 'Drag', 'Out', 'Jeff', 'Sessions’', 'Confirmation', 'Fight', '-', 'Breitbart']\",\n",
       " \"['BREAKING', 'NEWS:', 'Podesta', 'Brothers', 'Pedo', 'Ring:', 'MR.', 'TRUMP,', 'DRAIN', 'THE', 'SWAMP!!', '—', 'V,', 'The', 'Guerrilla', 'Economist']\",\n",
       " \"['Las', 'frases', 'más', 'destacadas', 'del', 'debate', 'de', 'investidura']\",\n",
       " '[\\'Confusing\\', \\'Jihad\\', \\'with\\', \\'Hirabah\\', \"Won\\'t\", \\'Build\\', \\'a\\', \\'More\\', \\'Peaceful\\', \\'World\\']',\n",
       " \"['Lazy', 'Liberal', 'Journalists', 'Smear', 'Bannon']\",\n",
       " \"['Australia', 'Will', 'Close', 'Detention', 'Center', 'on', 'Manus', 'Island,', 'but', 'Still', 'Won’t', 'Accept', 'Asylum', 'Seekers', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Politico:', 'Hillary', 'Clinton', 'Is', 'Running', 'Again', '-', 'Breitbart']\",\n",
       " \"['Giant', 'Lynx', 'Makes', 'The', 'Most', 'Adorable', 'Sound,', 'Whenever', 'His', 'Human', 'Rubs', 'His', 'Face']\",\n",
       " \"['Minnesota', 'Cop', 'Found', 'Not', 'Guilty', 'in', 'Philando', 'Castile', 'Shooting', 'Trial']\",\n",
       " \"['115', 'Million', 'Americans', 'Killed', 'In', '30', 'Minutes']\",\n",
       " \"['Wayne', 'Madsen:', 'The', 'CIA', 'Has', 'Always', 'Served', 'the', 'Interests', 'of', 'Wall', 'Street']\",\n",
       " \"['November', '10:', 'Daily', 'Contrarian', 'Reads']\",\n",
       " \"['FLASHBACK', '-', 'Reports:', 'Obama’s', '2008', 'Campaign', 'Reps', 'Talked', 'with', 'Iran,', 'Hamas']\",\n",
       " \"['LesserOfTwoEvilism']\",\n",
       " \"['Another', 'powerful', 'earthquake', 'strikes', 'central', 'Italy']\",\n",
       " \"['Aya', 'Cash:', 'The', 'First', 'Time', 'I', 'Ate', 'a', 'Vegetable', '(I', 'Was', '22)', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump', 'Adviser', 'Says', 'Israeli', 'Settlements', '‘Not', 'Illegal’']\",\n",
       " \"['Serena', 'Williams', 'Prevails', 'in', 'Opener,', 'No', 'Problem', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['’Soul', 'Man’', 'Sam', 'Moore', '’Honored’', 'to', 'Perform', 'at', 'Trump', 'inauguration']\",\n",
       " \"['Will', 'Ferrell', 'Just', 'Came', 'Back', 'As', 'Bush', 'To', 'Destroy', 'Trump', '(VIDEO)']\",\n",
       " \"['Chinese', 'Government', 'Not', 'Concerned', 'with', 'Tough', 'Talk', 'from', 'Trump', 'Cabinet', '-', 'Breitbart']\",\n",
       " \"['Billionaire', 'Is', 'Reported', 'Seized', 'From', 'Hong', 'Kong', 'Hotel', 'and', 'Taken', 'Into', 'China', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Easy', 'to', 'know', 'what', 'is', 'up', '-', 'link']\",\n",
       " \"['A', 'Homebody', 'Finds', 'the', 'Ultimate', 'Home', 'Office', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Ann', 'Coulter', 'Unloads', 'on', 'Paul', 'Ryan’s', '’Deeply', 'Unpopular’', 'Obamacare', '2.0', 'Bill', '-', 'Breitbart']\",\n",
       " \"['Transgender', 'Bathroom', 'Debate', 'Turns', 'Personal', 'at', 'a', 'Vermont', 'High', 'School', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Why', 'Obama', 'And', 'Hillary', 'Wanted', 'Libya’s', 'Gaddafi', 'Toppled', 'And', 'Killed']\",\n",
       " \"['How', 'NATO', 'Is', 'Trying', 'to', 'Sabotage', 'the', 'Turkey-Russia', 'Reset', '-', 'M.K.', 'Bhadrakumar']\",\n",
       " \"['ALERT:', 'Former', 'Soros', 'Associate', 'Just', 'Warned', 'The', 'Pros', 'Have', 'It', 'Wrong,', 'Gold', '&', 'Silver', 'Will', 'Skyrocket', 'Like', 'The', '1970s']\",\n",
       " '[\\'Beauty\\', \\'Queen\\', \\'Told\\', \\'To\\', \\'Lose\\', \\'Weight\\', \\'Quits\\', \\'Pageant\\', \\'-\\', \\'The\\', \\'Onion\\', \\'-\\', \"America\\'s\", \\'Finest\\', \\'News\\', \\'Source\\']',\n",
       " \"['Is', 'Hillary', 'Panicking', 'Over', 'the', 'FBI', 'and', 'Weiner', 'Emails?']\",\n",
       " \"['In', '77', 'Chaotic', 'Minutes,', 'Trump', 'Defends', '‘Fine-Tuned', 'Machine’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['One', 'Police', 'Shift:', 'Patrolling', 'an', 'Anxious', 'America', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Fighting', 'Ghost', 'Fascists', 'While', 'Aiding', 'Real', 'Ones']\",\n",
       " \"['Your', 'Monday', 'Evening', 'Briefing:', 'Brexit,', 'Abortion,', '‘Game', 'of', 'Thrones’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['EU', 'Increases', '‘Brexit', 'Bill’', 'Demand', 'to', '€100', 'Billion,', 'Up', 'From', '€60', 'Billion']\",\n",
       " \"['Arianna', 'Huffington’s', 'Sleep', 'Revolution', 'Starts', 'at', 'Home', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Путин', 'рассказал,', 'когда', 'в', 'Крыму', 'решат', 'проблему', 'с', 'пресной', 'водой']\",\n",
       " \"['IRANIAN', 'MISSILE', 'ACCIDENTALLY', 'DESTROYS', 'IRANIAN', 'SHIP', 'AIMED', 'FOR', 'SYRIA!']\",\n",
       " \"['Sonoma', 'County', 'California', 'Just', 'Voted', 'to', 'Create', 'the', 'Largest', 'GMO', 'Free', 'Zone', 'in', 'America']\",\n",
       " \"['FBI’s', 'Comey:', 'WikiLeaks', 'Is', '‘Intelligence', 'Porn,’', 'Not', 'Journalism', '-', 'Breitbart']\",\n",
       " \"['Clare', 'Waight', 'Keller', 'Is', 'Named', 'First', 'Female', 'Designer', 'of', 'Givenchy', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Realities', 'Faced', 'by', 'Black', 'Canadians', 'are', 'a', 'National', 'Shame']\",\n",
       " \"['Top', 'NFL', 'Draft', 'Prospect', 'Caleb', 'Brantley', 'Charged', 'With', 'Punching', 'a', 'Woman', 'in', 'the', 'Face', '-', 'Breitbart']\",\n",
       " \"['Videos', 'on', 'the', 'Pacific', 'Crest', 'Trail', 'Association', 'and', 'the', 'LaVoy', 'Finicim', 'MURDER']\",\n",
       " \"['12', 'Life', 'Lessons', 'from', 'a', 'Man', 'Who’s', 'Seen', '12000', 'Deaths']\",\n",
       " \"['Trump', 'and', 'G.O.P.', 'Work', 'to', 'Win', 'Repeal', 'of', 'Obama’s', 'Health', 'Act', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Atlantic’s', 'Goldberg:', 'I’m', '‘Not', 'Confident’', 'Trump', 'Can', 'Handle', '‘Matters', 'of', 'Life', 'and', 'Death’', '-', 'Breitbart']\",\n",
       " \"['Why', 'this', 'Orthodox', 'rabbi', 'supports', 'Trump']\",\n",
       " \"['Nuclear', 'tensions', 'between', 'US,', 'Russia', 'reach', '‘dangerous’', 'point']\",\n",
       " \"['Geo-Engineering', 'Unlikely', 'to', 'Work,', 'Conservation', 'Group', 'Says']\",\n",
       " \"['Gambia', 'Joins', 'South', 'Africa', 'and', 'Burundi', 'in', 'Exodus', 'from', 'International', 'Criminal', 'Court']\",\n",
       " \"['Peyton', 'Manning', 'Golfed', 'with', 'President', 'Trump', 'on', 'Sunday']\",\n",
       " \"['Obama', 'Urges', 'Donald', 'Trump', 'to', 'Send', '‘Signals', 'of', 'Unity’', 'to', 'Minority', 'Groups', 'and', 'Women', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Russian', 'companies', 'to', 'be', 'unable', 'to', 'use', 'LinkedIn', 'services', '—', 'company', '-', 'Russia', 'News', 'Now']\",\n",
       " \"['‘It’s', 'Like', 'a', 'Miracle’:', 'Woman', 'Gives', 'Birth', 'Using', 'Ovary', 'Frozen', 'Since', 'Childhood', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['UConn’s', 'Recipe', 'for', 'Success', 'Is', 'to', 'Run,', 'Run', 'and', 'Run', 'Some', 'More', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['IOM', 'says', '5,238', 'refugees', 'have', 'died', 'worldwide', 'in', '2016']\",\n",
       " \"['Assange', '‘finally', 'afforded', 'opportunity’', 'to', 'give', 'statement', 'over', 'rape', 'accusation']\",\n",
       " \"['Hillary’s', 'Puppet', 'Shows', 'Just', 'How', 'Much', 'Hillary', 'Cares', 'About', 'God', 'With', 'Omission', 'of', '2', 'Words']\",\n",
       " \"['The', 'Stock', 'Market', 'Has', 'Gone', 'So', 'High,', 'It’s', 'a', 'Problem', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['John', 'Kerry', 'Rejects', 'Suggestions', 'of', 'U.S.', 'Involvement', 'in', 'Turkey', 'Coup', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump', 'Aide', 'Stephen', 'Miller:', 'The', 'U.S.', '’Has', 'an', 'Absolute', 'Sovereign', 'Right', 'to', 'Determine', 'Who', 'Can', 'and', 'Cannot', 'Enter', 'the', 'Country’', '-', 'Breitbart']\",\n",
       " \"['4-year', 'old', 'Russian', 'girl', 'speaks', '7', 'languages.', 'How', 'did', 'she', 'do', 'this?']\",\n",
       " \"['This', 'Powerful', 'Corporate', 'Lobby', 'Is', 'Quietly', 'Backing', 'Hillary', '—', 'and', 'Nobody’s', 'Talking', 'About', 'It']\",\n",
       " '[\\'Controversial\\', \\'DNA\\', \\'search\\', \\'helps\\', \\'nab\\', \\'the\\', \"\\'Grim\", \"Sleeper\\'\", \\'serial\\', \\'killer\\']',\n",
       " \"['Maintaining', 'a', 'Sunny', 'Spirit', 'in', 'the', 'Face', 'of', 'Hardship', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Rush', 'Limbaugh:', 'Comey', 'Firing', 'an', '’Epic', 'Troll’', 'by', 'Trump', 'on', 'Dems', '-', 'Breitbart']\",\n",
       " \"['Twitter', 'Sues', 'the', 'Government', 'to', 'Block', 'the', 'Unmasking', 'of', 'an', 'Account', 'Critical', 'of', 'Trump', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Warriors,', 'Resilient', 'at', 'Home,', 'Cruise', 'Against', 'the', 'Cavaliers', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Comey', 'Letter', 'on', 'Clinton', 'Email', 'Is', 'Subject', 'of', 'Justice', 'Dept.', 'Inquiry', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump', 'and', 'Clinton', 'Choice', 'Impacting', 'Global', 'Markets']\",\n",
       " \"['Chinese', 'J-20', 'stealth', 'fighter', 'debuts', 'in', 'skies', 'of', 'Zhuhai']\",\n",
       " \"['Spain,', 'Malta,', 'under', 'U.S./UK', 'pressure,', 'refuse', 'to', 'allow', 'Russian', 'carrier', 'group', 'to', 'refuel', 'in', 'their', 'ports']\",\n",
       " \"['Review:', 'In', 'Radiohead’s', '‘A', 'Moon', 'Shaped', 'Pool,’', 'Patient', 'Perfectionism', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['This', 'is', 'the', 'Man', 'Militarized', 'Police', 'at', 'Standing', 'Rock', 'are', 'Working', 'For']\",\n",
       " \"['Woman', 'Arrested', 'On', 'Own', 'Property', 'After', 'Her', 'Land', 'Was', 'Stolen', 'By', 'DAPL']\",\n",
       " \"['Pulitzer', 'Prizes:', 'New', 'York', 'Times', 'Wins', '3', 'Daily', 'News', 'and', 'ProPublica', 'Share', 'Public', 'Service', 'Award', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'Vanquished', 'to', 'Witness', 'the', 'Takeover:', 'Bushes,', 'Clintons', 'Will', 'Attend', 'Donald', 'Trump’s', 'Inauguration', '-', 'Breitbart']\",\n",
       " \"['Speculation', 'Over', 'Possible', 'Obama', 'Pardon', 'of', 'Edward', 'Snowden,', 'Bowe', 'Bergdahl', 'and', 'Chelsea', 'Manning', '-', 'Breitbart']\",\n",
       " \"['Cheesy', 'Mashed', 'Potatoes', 'for', 'the', 'Soul', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Unprincipled', 'WaPo', 'Editors', 'Damned', 'Comey', 'Critics', '-', 'Now', 'Join', 'Them']\",\n",
       " \"['A', 'nation', '‘WRECKED’', 'by', 'immigration:', 'A', 'civil', 'war', 'is', 'brewing', 'as', 'do-gooding', 'Swedes', 'turn', 'against', 'Muslim', 'migrants’', 'violence,', 'rapes', 'and', 'murders']\",\n",
       " \"['Man,', '73,', 'Shot', 'Dead', 'by', 'Officer', 'Had', 'a', 'Crucifix,', 'Not', 'a', 'Gun,', 'Police', 'Say', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Exclusive—Rep.', 'Jim', 'Jordan', 'on', 'Trump’s', 'First', '100', 'Days:', '‘I', 'Think', 'We’re', 'Off', 'to', 'a', 'Great', 'Start’', '-', 'Breitbart']\",\n",
       " \"['Haley:', 'Attack', 'on', 'Syria', '’One', 'of', 'the', 'President’s', 'Finest', 'Hours’', '-', 'Breitbart']\",\n",
       " \"['Justin', 'Bieber', 'Defecates', 'On', 'Adoring', 'Irish', 'Fans', 'From', 'Hotel', 'Window']\",\n",
       " \"['WHO', 'Cancer', 'Agency', 'Under', 'Fire', 'for', 'withholding', '‘carcinogenic', 'glyphosate’', 'Documents']\",\n",
       " \"['Gianno', 'Caldwell', 'Claims', 'Hillary', 'Only', 'Cares', 'About', 'Black', 'Vote,', 'Not', 'Black', 'Lives']\",\n",
       " \"['ISIS', 'Uses', 'Ramadan', 'as', 'Calling', 'for', 'New', 'Terrorist', 'Attacks', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['’Daily', 'Show’', 'Mocks', '’Mahatma', 'Blondie’', 'Megyn', 'Kelly’s', 'NBC', 'Debut']\",\n",
       " \"['Health', 'Care', 'Bill’s', 'Failure:', 'Just', 'Part', 'of', 'the', '’Art', 'of', 'the', 'Deal’', '-', 'Breitbart']\",\n",
       " \"['Lewandowski:', 'Comey', '’a', 'Liar’', '-', 'Looking', 'to', 'Sign', 'Major', 'Book', 'Deal', '-', 'Breitbart']\",\n",
       " \"['Kimberly', 'Guilfoyle', 'Discusses', 'Potential', 'White', 'House', 'Press', 'Secretary', 'Job', 'in', 'Interview']\",\n",
       " \"['Clinton’s', 'Campaign', 'Chairman', 'John', 'Podesta', 'Invited', 'to', 'an', 'Occult', '‘Spirit', 'Cooking’', 'Dinner', 'by', 'Marina', 'Abramović']\",\n",
       " \"['How', 'the', 'Obama', 'Coalition', 'Crumbled,', 'Leaving', 'an', 'Opening', 'for', 'Trump', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['BREAKING', ':', 'Soros', 'Hiring', 'Anti-Trump', '“Protesters”', 'Once', 'Again', 'For', 'Sunday', 'Palos', 'Verdes', 'Event', '–', 'TruthFeed']\",\n",
       " \"['Pregnancy', 'Changes', 'the', 'Brain', 'in', 'Ways', 'That', 'May', 'Help', 'Mothering', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Refugee', 'Manhunt', 'After', 'Family', 'Found', 'in', 'Freezer', 'in', 'Denmark']\",\n",
       " \"['Global', 'markets', 'turn', 'red', 'as', 'Toblerone', 'scandal', 'unfolds']\",\n",
       " \"['FEMA', 'Opens', 'Loan', 'Window', 'As', 'Red', 'Cross', 'Tries', 'To', 'Shut', 'Down', 'Shelters']\",\n",
       " \"['GaiaPortal:', 'Interwebs', 'of', 'Gaia', 'energetics', 'are', 'strengthened']\",\n",
       " \"['Scarborough:', 'Trump', 'Poops', 'His', 'Pants,', 'Calls', 'It', '’Modern', 'Art’', '-', 'Breitbart']\",\n",
       " \"['When', 'Cooking,', 'Invest', 'Time.', 'Or', 'Work.', 'Not', 'Both.', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Bad', 'News', 'For', 'Jackson', 'Family', 'As', 'Woman', 'Leaks', 'Star’s', 'Sick', '$900k', 'Sex', 'Secret']\",\n",
       " \"['Gorsuch,', 'London,', 'Republican', 'Party:', 'Your', 'Thursday', 'Evening', 'Briefing', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Emma', 'Morano,', 'Last', 'Person', 'Born', 'in', '1800s,', 'Dies', '-', 'Breitbart']\",\n",
       " \"['Loserpalooza:', '9', 'Craziest', 'Scenes', 'from', 'Anti-Trump', 'Protests', '-', 'Breitbart']\",\n",
       " \"['Racist', 'Conservatives', 'Cheer', 'After', 'Hannity', 'Tells', 'Obama', 'To', 'Go', 'Back', 'To', 'Africa', 'And', 'Stay', 'There']\",\n",
       " \"['What', 'Does', 'a', 'Trump', 'Victory', 'Mean', 'for', 'Africa?']\",\n",
       " \"['News:', 'Missed', 'Opportunity:', 'President', 'Obama', 'Just', 'Found', 'Out', 'That', 'He', 'Was', 'Allowed', 'To', 'Sleep', 'In', 'The', 'White', 'House', 'After', 'Years', 'Of', 'Living', 'In', 'A', 'Hotel', 'Near', 'The', 'Reagan', 'Airport']\",\n",
       " \"['Visions', 'of', 'Life', 'on', 'Mars', 'in', 'Earth’s', 'Depths', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['After', 'Obama', 'Cancels', 'Talk,', 'Rodrigo', 'Duterte', 'of', 'Philippines', 'Says', 'He', 'Regrets', 'Slur', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['New', 'Jersey', 'Will', 'Increase', 'Gas', 'Tax', '23¢,', 'Ending', 'Long', 'Political', 'Stalemate', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['ABC:', 'Manchester', 'Attack', '’Likely', 'to', 'Inflame', 'Anti-Islamic', 'Sentiment’', '-', 'Breitbart']\",\n",
       " \"['Hidden', 'in', 'plain', 'sight', '–', 'The', 'global', 'depopulation', 'agenda']\",\n",
       " \"['Voters', 'Can', 'Fight', 'Back', 'Against', 'Election', 'Fraud']\",\n",
       " \"['The', 'Jobs', 'Americans', 'Do', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Sanders', 'Asks', 'Obama', 'To', 'Intervene', 'In', 'Dakota', 'Access', 'Pipeline', 'Dispute']\",\n",
       " \"['Google', 'Adds', 'Jobs', 'Section', 'to', 'Search', 'Engine,', 'Including', 'Employer', 'Ratings', '-', 'Breitbart']\",\n",
       " \"['Crying', 'Jordan:', 'The', 'Meme', 'That', 'Just', 'Won’t', 'Die', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Dem', 'Rep', 'Sánchez:', 'Trump', 'Has', 'Used', '’Fear’', 'of', 'Muslims', 'And', 'Immigrants', 'To', 'Promote', 'Policies', 'That', '’Undermine’', 'Our', 'Values', '-', 'Breitbart']\",\n",
       " \"['Announcement', 'by', 'the', 'Saker', 'Community', 'about', 'the', 'German', 'Saker', 'blog', '|', 'The', 'Vineyard', 'of', 'the', 'Saker']\",\n",
       " \"['Shooting', 'Victims’', 'Families', 'Watch', 'as', 'Gun', 'Measures', 'Stall', 'Once', 'Again', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Images', 'reveal', 'crashed', 'Schiaparelli', 'Mars', 'lander']\",\n",
       " \"['Grassroots', 'Coalition', 'Shares', '‘Many', 'Questions', 'and', 'Concerns', 'About', 'Betsy', 'DeVos’', 'with', 'Senators']\",\n",
       " \"['October', '28:', 'Daily', 'Contrarian', 'Reads']\",\n",
       " \"['Trump', 'at', 'Inaugural', 'Balls:', '’Now', 'the', 'Work', 'Begins', '...', 'We', 'Are', 'Not', 'Playing', 'Games’', '-', 'Breitbart']\",\n",
       " \"['Trump’s', 'Choice', 'of', 'Stephen', 'Bannon', 'Is', 'Nod', 'to', 'Anti-Washington', 'Base', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Media', 'Outrage', 'over', 'White', 'House', '’Exclusion’', 'is', 'Fake', 'News', '-', 'Breitbart']\",\n",
       " \"['Benny', 'Morris’s', 'Untenable', 'Denial', 'of', 'the', 'Ethnic', 'Cleansing', 'of', 'Palestine']\",\n",
       " \"['Obama:', 'Hillary', 'Clinton', 'Pardon', 'Could', 'Heal', 'Divided', 'Nation']\",\n",
       " \"['Exceptional', 'Handling']\",\n",
       " \"['Nota', 'conceptual', 'para', 'la', 'presidencia', 'de', 'Rusia']\",\n",
       " \"['Response', 'of', 'Philippines', 'President', 'to', 'Fatal', 'Blast', 'Raises', 'Fears', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['David', 'Adjaye', 'on', 'Designing', 'a', 'Museum', 'That', 'Speaks', 'a', 'Different', 'Language', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['BULLETIN:', 'There', 'ARE', 'Righteous', 'Jews', 'For', 'Trump!;', 'Rule', 'Or', 'Ruin', 'For', 'GOP', 'Establishment;', 'etc.(60', 'ITEMS!)']\",\n",
       " '[\"Hallowe\\'en\", \\'-\\', \\'The\\', \\'Day\\', \\'of\\', \\'the\\', \\'Aos\\', \\'Si\\']',\n",
       " \"['Britain', 'Reduces', 'Terror', 'Level', 'One', 'Notch', 'to', '‘Severe’', 'After', 'Terror', 'Cell', 'Arrests']\",\n",
       " \"['Mute', 'and', 'Alone,', 'He', 'Was', 'Never', 'Short', 'of', 'Kind', 'Words', 'or', 'Friends', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Iranian', 'Military', 'Commander', 'Claims', 'Rogue', 'Nation', 'Sending', 'Elite', 'Fighter’s', 'to', 'Infiltrate', 'the', 'US', 'and', 'Europe']\",\n",
       " \"['All', 'Brexit', 'arguments', 'settled', 'by', '0.5', 'per', 'cent', 'third-quarter', 'growth']\",\n",
       " \"['No', 'Account', 'for', 'You']\",\n",
       " \"['Watch:', 'Muslim', 'Student', 'Claims', 'that', 'Non-Believers', 'Will', 'Be', 'Killed', 'in', 'Islamic', 'Countries', '-', 'Breitbart']\",\n",
       " \"['Yemen', 'and', 'YET', 'another', '“False', 'Flag”', 'to', 'Protect', 'Saudi', 'and', 'US', 'Interests', 'in', 'the', 'Middle', 'East', '|', 'New', 'Eastern', 'Outlook']\",\n",
       " \"['Over', '500', 'Russian', 'and', 'Egyptian', 'Troops', 'Train', 'to', 'Kill', 'Terrorists', '(Photos)', '-', 'Boris', 'Egorov']\",\n",
       " \"['Watch:', 'Muslim', '‘Palestinians’', 'Declare', '“We', 'follow', 'our', 'Prophet', 'Muhammad,', 'we', 'will', 'kill', 'all', 'Christians', 'and', 'Jews”']\",\n",
       " \"['Street', 'dogs', 'of', 'Kerala', 'call', 'upon', 'superdog', 'Krypto', 'to', 'rescue', 'them', 'from', 'humans']\",\n",
       " \"['Israel', 'Tracked', '‘Anti-Government’', 'Journalists', 'On', 'Facebook']\",\n",
       " \"['New', 'Alaska', 'Law', 'Taking', 'First', 'Step', 'Against', 'Common', 'Core']\",\n",
       " \"['Key', 'Baylor', 'Football', 'Executive', 'DeMarko', 'Butler', 'Fired', 'in', 'Texting', 'Scandal', '-', 'Breitbart']\",\n",
       " \"['Calgary', 'Airport', 'Arrivals', 'YYC']\",\n",
       " \"['George', 'Michael', 'Wrestled', 'With', 'Fame.', 'Frank', 'Sinatra', 'Had', 'Some', 'Advice.', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Sharon', 'Olds,', 'Laureate', 'of', 'Sexuality,', 'Scrutinizes', 'the', 'Body', 'in', '‘Odes’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Blue', 'Collar', 'Election', 'Shocks', 'Liberal', 'Media']\",\n",
       " \"['Markets', 'collapse', 'as', 'Donald', 'Trump', 'is', 'projected', 'to', 'win']\",\n",
       " 'nan',\n",
       " \"['‘They', 'Will', 'Have', 'to', 'Die', 'Now’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Казахстан', 'на', 'страже', 'ядерной', 'безопасности', '|', 'Новое', 'восточное', 'обозрение']\",\n",
       " \"['Police', 'fire', 'rubber', 'bullets', 'at', 'pipeline', 'protesters']\",\n",
       " \"['Ann', 'Coulter', 'Hits', 'Supposed', '’Gay', 'Icon’', 'Kathy', 'Griffin', 'on', 'ISIS-Cribbed', 'Antics']\",\n",
       " '[\\'Contaminated\\', \\'Food\\', \\'from\\', \\'China\\', \\'Now\\', \\'Entering\\', \\'the\\', \\'U.S.\\', \\'Under\\', \\'the\\', \"\\'Organic\\'\", \\'Label\\']',\n",
       " \"['Ten', 'Famous', 'People', 'on', 'What', 'to', 'Read', 'This', 'Summer', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['SCANDAL:', 'EPA', 'could', 'have', 'issued', 'an', 'emergency', 'order', '7', 'months', 'before...']\",\n",
       " \"['Hillary', 'Clinton', 'KNEW', '5', 'years', 'ago', 'Anthony', 'Weiner', 'was', 'a', 'Pedophile', '-', 'Wikileaks']\",\n",
       " \"['أوروبا', 'وخيار', 'القوة', 'في', 'مواجهة', 'اللاجئين', '-', 'RT', 'Arabic']\",\n",
       " \"['A', '$150', 'Million', 'Stairway', 'to', 'Nowhere', 'on', 'the', 'Far', 'West', 'Side', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Cyber', 'War', '\\\\x97', 'From', 'Trifle', 'to', 'Catastrophe', ':', 'Information']\",\n",
       " \"['President', 'Putin', 'Asks', 'US', 'To', 'Stop', 'Provoking', 'Russia']\",\n",
       " \"['Inquiries', 'Cloud', 'de', 'Blasio’s', 'Bid', 'to', 'Come', 'Off', 'as', 'Strong', 'Manager', 'Before', '’17', 'Race', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['US', 'Drone', 'Strike', 'In', 'Afghanistan', 'Kills,', 'Wounds', 'Several', 'Civilians']\",\n",
       " \"['Bobby', 'Hutcherson,', 'Vibraphonist', 'With', 'Coloristic', 'Range', 'of', 'Sound,', 'Dies', 'at', '75', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Tourist', 'helicopter', 'crashes', 'into', 'house', 'in', 'Sochi.', 'Video']\",\n",
       " \"['Amnesty', 'Advocates', 'Boycott', 'Agency', 'Meetings', 'When', 'Pro-American', 'Advocates', 'Are', 'Invited', '-', 'Breitbart']\",\n",
       " \"['Project', 'Veritas', '4:', 'Robert', 'Creamer’s', 'Illegal', '$20,000', 'Foreign', 'Wire', 'Transfer', 'Caught', 'On', 'Tape']\",\n",
       " \"['Ex-FLOTUS', 'Michelle', 'Obama:', 'Trump', 'Wants', 'to', 'Feed', '’Crap’', 'to', 'Your', 'Kids']\",\n",
       " \"['South', 'Sudan', 'Slides', 'Closer', 'to', 'War', 'as', 'Gunfire', 'Rumbles', 'in', 'Its', 'Capital', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Busted:', 'Bill', 'Recorded', 'Telling', 'Mistress', 'To', 'Deny', 'That', 'Clintons', 'Helped', 'Her', 'Get', 'A', 'State', 'Job']\",\n",
       " \"['Alabama', 'Declares', 'State', 'Of', 'Emergency', 'Over', 'Pipeline', 'Explosion']\",\n",
       " \"['“National', 'Mood”', 'Focus', 'Group', 'Reflects', 'Angry,', 'Divided', 'America']\",\n",
       " \"['ЕС', 'намерен', 'расширить', 'санкции', 'в', 'отношении', 'Сирии']\",\n",
       " \"['Bankrupt', 'Puerto', 'Rico', 'Voting', 'on', 'U.S.', 'Statehood', '-', 'Breitbart']\",\n",
       " \"['Overwhelmed', 'by', '‘Brexit’?', 'Here', 'Are', 'the', 'Basics', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Seattle’s', 'Gay', 'Mayor', 'Accused', 'of', 'Sexually', 'Molesting', 'Teens', 'in', 'the', '1980s', '-', 'Breitbart']\",\n",
       " \"['Gender', 'Fluidity', 'on', 'the', 'Runways', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['US', 'NATO', 'To', 'Attack', 'Putin', 'Military', 'Drills', 'in', 'Russia', '–', 'World', 'War', '3', 'RED', 'Alert', 'Kopya']\",\n",
       " \"['New', 'Clinton', 'Emails', 'Came', 'From', 'Underage', 'Sex', 'Pest', 'Anthony', 'Weiner']\",\n",
       " \"['In', 'a', 'Montana', 'Bear', 'Attack,', 'Lessons', 'on', 'Hope,', 'Survival', 'and', 'First', 'Aid', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['‘What', 'Is', 'Best', 'About', 'America’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'Pecan', 'Steps', 'Off', 'the', 'Pie', 'Plate', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['When', 'Barack', 'Obama', 'Plagiarized', '’Don’t', 'Tell', 'Me', 'Words', 'Don’t', 'Matter’', '-', 'Breitbart']\",\n",
       " \"['Lower', 'Back', 'Ache?', 'Be', 'Active', 'and', 'Wait', 'It', 'Out,', 'New', 'Guidelines', 'Say', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['‘If', 'I', 'Sleep', 'for', 'an', 'Hour,', '30', 'People', 'Will', 'Die’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['‘Veep’', 'Season', '5,', 'Episode', '7:', 'Do', 'a', 'Little', 'Dance', '...', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['On', '‘Dr.', 'Oz,’', 'Trump', 'Offers', 'Placebo', 'Transparency', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Comment', 'on', 'Gold', 'Medalist', 'Wrestler', 'Gets', 'Violent', 'with', 'Police', '—', 'All', '7', 'Cops', 'Choose', 'Not', 'to', 'Engage', 'in', 'Deadly', 'Force', 'by', 'Buck', 'Rogers']\",\n",
       " \"['One', 'Year', 'of', 'Water', 'in', 'Orange', 'County', 'in', 'Just', 'Four', 'Days', '-', 'Breitbart']\",\n",
       " \"['Doctors', 'With', 'Enemies:', 'Did', 'Afghan', 'Forces', 'Target', 'the', 'M.S.F.', 'Hospital?', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['2,700-year-old', 'Hebrew', 'mention', 'of', 'Jerusalem', 'found']\",\n",
       " \"['Spirit', 'Cooking:', 'The', 'Most', 'Disturbing', 'Podesta', 'Email', 'Yet?', '(Warning:', 'Graphic', 'Content)']\",\n",
       " \"['Donald', 'Trump’s', 'Missteps', 'Risk', 'Putting', 'a', 'Ceiling', 'Over', 'His', 'Support', 'in', 'Swing', 'States', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['Green', 'Party’s', 'Margaret', 'Flowers', 'Challenges', 'US', 'Senate', 'Debate', 'in', 'Maryland', 'as', 'Undemocratic']\",\n",
       " \"['Officer', 'Rescues', 'Drowning', 'Deer', 'From', 'Pool', 'With', 'Quick-Thinking']\",\n",
       " \"['Police:', 'Suspects', 'Punch', 'Army', 'Veteran,', 'Steal', 'His', 'Service', 'Dog', 'Outside', 'Home', 'in', 'the', 'Bronx', '-', 'Breitbart']\",\n",
       " \"['Insurance', 'Prices', 'for', 'Many', 'Obamacare', 'Customers', 'Will', 'Rise', 'By', 'Double', 'Digits', 'in', '2017']\",\n",
       " \"['YIKES!', 'Megyn', 'Kelly', 'Receives', 'RUDE', 'AWAKENING-', 'Reminded', 'She’s', 'REPLACEABLE!']\",\n",
       " 'nan',\n",
       " \"['Iran', 'Sending', 'Elite', 'IRGC', 'Warfighters', 'Into', 'Europe', 'And', 'United', 'States', 'In', 'Preparation', 'For', 'Battle']\",\n",
       " \"['Hillary', 'Endorsed', 'Donald', 'Trump', 'for', 'President', 'According', 'to', 'Wikileaks']\",\n",
       " \"['Woodward:', 'Trump', 'Dossier', 'Is', 'a', '’Garbage', 'Document’', '-', 'Intelligence', 'Chiefs', 'Should', '’Apologize’', 'to', 'Trump', '-', 'Breitbart']\",\n",
       " \"['Emmy', 'Nominations', '2016:', 'Tracy', 'Morgan', 'on', 'His', 'Emotional', 'Return', 'to', '‘Saturday', 'Night', 'Live’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump', 'Picks', 'Mick', 'Mulvaney,', 'South', 'Carolina', 'Congressman,', 'as', 'Budget', 'Director', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Donald', 'Trump,', 'el', 'primer', 'presidente', 'naranja', 'de', 'los', 'Estados', 'Unidos']\",\n",
       " \"['Re:', 'Why', 'Did', 'Attorney', 'General', 'Loretta', 'Lynch', 'Plead', 'The', 'Fifth?']\",\n",
       " \"['Russia:', 'U.S.', 'Missile', 'Defense', '’Poses', 'Deep', 'Risk’', 'to', 'Security', 'of', 'Asia']\",\n",
       " \"['What', 'Does', 'Washington', '‘Plan', 'B’', 'in', 'Syria', 'Really', 'Mean?']\",\n",
       " \"['Upset', 'by', 'Brexit,', 'Some', 'British', 'Jews', 'Look', 'to', 'Germany', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Girl', 'Asks', 'Her', '20', 'Boyfriends', 'To', 'Each', 'Give', 'Her', 'an', 'iPhone,', 'Uses', 'The', 'Money', 'To', 'Buy', 'a', 'House']\",\n",
       " \"['В', 'МВД', 'опровергли,', 'что', 'разыскивают', '13-летнюю', 'дочь', 'Сердюкова']\",\n",
       " \"['Amnesty', 'International', 'Slams', 'Obama', 'Gov', 'for', 'Killing', '300', 'Civilians', 'in', 'Syria']\",\n",
       " 'nan',\n",
       " \"['Finding', 'His', 'Flock:', 'A', 'Rural', 'Writer’s', 'Book', 'Club', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Venezuela', 'crisis', 'enters', 'dangerous', 'phase', 'as', 'Maduro', 'foes', 'go', 'militant']\",\n",
       " \"['Emigre', 'Super', 'Blocs', 'Part', 'VIII:The', 'Quasi-Legal', 'Coup-Hillary', 'Clinton', 'Information', 'Operations', 'In', 'Election', '2016']\",\n",
       " \"['How', 'Could', 'You?', '19', 'Questions', 'to', 'Ask', 'Loved', 'Ones', 'Who', 'Voted', 'the', 'Other', 'Way', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'BBC', 'Asks', '–', 'What', 'Really', 'Happened', 'With', 'the', 'Clintons', 'in', 'Haiti?']\",\n",
       " \"['Trump', 'Picks', 'Thomas', 'Bossert', 'as', 'Top', 'Counterterrorism', 'Adviser', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['Coming', 'Unglued']\",\n",
       " \"['Trump', 'Jr.', 'Suspiciously', 'Helps', 'Arizona', 'Woman', 'Push', 'Stalled', 'Car,', 'Uses', 'It', 'As', 'A', 'Photo', 'Op', '(VIDEO)']\",\n",
       " \"['Suicides', 'by', 'Chicago', 'Police', 'Officers', 'Skyrocket']\",\n",
       " \"['So', 'you', 'think', 'SUVs', 'are', 'safe?', 'Shocking', 'video']\",\n",
       " 'nan',\n",
       " \"['20', 'People', 'Tortured,', 'Killed', 'at', 'Sufi', 'Muslim', 'Shrine', 'in', 'Pakistan']\",\n",
       " \"['Comment', 'on', 'Architecture', 'is', 'More', 'Than', 'a', 'Clever', 'Arrangement', 'of', 'Bricks:', 'This', 'Man', 'Transforms', 'it', 'Into', 'a', 'Healing', 'Process', 'by', 'Architecture', 'is', 'More', 'Than', 'a', 'Clever', 'Arrangement', 'of', 'Bricks:', 'This', 'Man', 'Transforms', 'it', 'Into', 'a', 'Healing', 'Process', '-', 'New', 'Earth', 'Media']\",\n",
       " \"['Hybrid', 'Wars', '8.', 'Strategies', 'Against', 'Africa', '–', 'Introduction']\",\n",
       " \"['Julian', 'Assange', 'to', 'Speak', 'Prerecorded', 'RT', 'Interview', '(11/5/16)']\",\n",
       " \"['Watch:', 'Israel', 'Loving', 'Hollywood', 'Actor', 'Just', 'Issued', 'Major', 'Plea', 'To', 'America', 'In', '3', 'Minutes', 'That’ll', 'Rock', 'The', 'Election']\",\n",
       " \"['Le', 'top', 'des', 'recherches', 'Google', 'passe', 'en', 'top', 'des', 'recherches', 'Google', '>>', 'Le', 'Gorafi']\",\n",
       " \"['On', '‘S.N.L.,’', 'Donald', 'Trump', 'Botches', 'His', '‘Independence', 'Day’', 'Moment', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Man', 'who', '‘doesn’t', 'care’', 'about', 'election', 'acting', 'like', 'some', 'sort', 'of', 'Buddha']\",\n",
       " \"['Exclusive:', 'Congresswoman', 'Marsha', 'Blackburn', 'Says,', '‘Eliminate', 'Net', 'Neutrality', 'to', 'Preserve', 'an', 'Open', 'Internet’', '-', 'Breitbart']\",\n",
       " \"['How', 'To', 'Attach', 'A', 'Year-Round', 'Greenhouse', 'To', 'Your', 'Own', 'Home']\",\n",
       " \"['SHERIFF', 'CLARKE', 'On', 'Obama’s', 'Final', 'Days:', '“Obama’s', 'Like', 'A', 'Tenant', 'Who’s', 'Been', 'Evicted', 'From', 'A', 'Property,', 'And', 'He’s', 'Gonna', 'Trash', 'The', 'Place', 'On', 'The', 'Way', 'Out', 'The', 'Door”', '[VIDEO]']\",\n",
       " \"['45', 'Weird', 'Bans', 'on', 'Women', 'in', 'Iran']\",\n",
       " 'nan',\n",
       " \"['The', 'End', 'Game', 'Closes', 'In', 'On', 'The', 'Clintons', 'As', 'The', 'Deep', 'State', 'Turns']\",\n",
       " \"['Does', 'Trump', 'Have', 'a', 'Fighting', 'Chance', 'Against', 'the', 'Establishment', '|', 'New', 'Eastern', 'Outlook']\",\n",
       " \"['In', 'Trade', 'Stances', 'Toward', 'China,', 'Clinton', 'and', 'Trump', 'Both', 'Signal', 'a', 'Chill', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Review:', '‘The', 'Secret', 'Life', 'of', 'Pets’', 'Amuses,', 'but', 'Misses', 'Opportunities', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Компания', 'Alibaba', 'открыла', 'вакансию', 'кота']\",\n",
       " \"['First', 'case', 'of', 'demonetisation-related', 'HIV', 'after', 'man', 'has', 'unprotected', 'sex', 'with', 'an', 'ATM', 'machine']\",\n",
       " \"['5', 'Things', 'You', 'Need', 'to', 'Know', 'About', 'the', 'Dakota', 'Access', 'Pipeline', 'Protests']\",\n",
       " \"['Koch', 'Brothers', 'Battle', 'to', 'Prevent', 'Dark', 'Money', 'Disclosure', 'in', 'South', 'Dakota']\",\n",
       " \"['MILO', 'Berkeley', 'Event', 'Evacuated', 'as', 'Masked', 'Protesters', 'Light', 'Fires,', 'Storm', 'Venue', '-', 'Breitbart']\",\n",
       " '[\\'\"Authoritarianism\":\\', \\'How\\', \\'the\\', \\'West\\', \\'demonizes\\', \\'strong,\\', \\'popular\\', \\'leaders\\']',\n",
       " \"['Swedish', 'Journo:', 'Sweden', 'Will', 'Collapse', 'Without', 'Illegal', 'Migrants']\",\n",
       " \"['Is', 'your', 'promising', 'internet', 'career', 'over', 'now', 'Vine', 'is', 'dead?', 'Write', 'for', 'NewsBiscuit', 'to', 'cheer', 'yourself', 'up']\",\n",
       " \"['In', 'Iowa,', 'Trump', 'Voters', 'Are', 'Unfazed', 'by', 'Controversies', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Radical', 'American-Grown', 'Marxist', 'Terror', 'Group', 'Just', 'Made', 'Announcement', 'For', 'January', '20th', '|', 'Conservative', 'Daily', 'Post']\",\n",
       " \"['Guardian', 'Opinion', 'Writer:', 'Trump,', 'Bannon', 'Counting', 'On', 'Terrorist', '’Massacre’']\",\n",
       " \"['Putin’s', 'Pro-Trump', 'Online', 'Trolls', 'Just', 'Spilled', 'The', 'Beans', 'To', 'Samantha', 'Bee']\",\n",
       " \"['Along', 'Mosul’s', 'Front', 'Line,', 'Desperate', 'Civilians', 'and', 'Dug-In', 'Troops', 'and', 'Fighters', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['As', 'Second', 'Avenue', 'Subway', 'Opens,', 'a', 'Train', 'Delay', 'Ends', 'in', '(Happy)', 'Tears', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['71-Year-Old', 'Cher', 'Dons', 'See-Through', 'Top', 'and', 'Nipple', 'Pasties', 'at', 'Billboard', 'Awards']\",\n",
       " \"['Report:', 'Stopping', 'Only', 'Nine', 'Percent', 'of', 'Illegal', 'Alien', 'Border', 'Crossers', 'Would', 'Pay', 'for', 'Trump’s', 'Border', 'Wall', '-', 'Breitbart']\",\n",
       " \"['ICE', 'Union', 'Issues', 'Final', 'Warning', 'to', 'Voters']\",\n",
       " \"['How', 'an', 'Elite', 'New', 'York', 'Police', 'Unit', 'Rehearses', 'for', 'a', 'Terrorist', 'Attack', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\"Trump\\'s\", \\'Hollywood\\', \\'Star\\', \\'Vandal\\', \\'Outs\\', \\'Himself\\', \\'in\\', \\'Shameless\\', \\'New\\', \\'Video,\\', \\'Taunts\\', \\'Police\\', \\'Seeking\\', \\'Him\\', \\'Out\\']',\n",
       " \"['California', 'Today:', 'A', 'View', 'of', 'San', 'Francisco’s', 'Leaning', 'Tower', 'From', 'Space', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Matthews:', 'Trump’s', 'Speech', 'What', 'Putin', 'Has', 'Been', 'Saying,', '’America', 'First’', 'Has', '’Hitlerian', 'Background’', '-', 'Breitbart']\",\n",
       " \"['Vladimir', 'Putin', 'at', 'the', 'Valdai', 'International', 'Discussion', 'Club', ':', '«Shaping', 'the', 'World', 'of', 'Tomorrow»,', 'by', 'Vladimir', 'Putin']\",\n",
       " \"['Chinese', 'Social', 'Media', 'Rages', 'over', 'United', 'Airlines', 'Controversy', '-', 'Breitbart']\",\n",
       " \"['America', 'Is', 'Better', 'Without', 'Borders']\",\n",
       " \"['More', 'Than', '’Chaos’:', 'Terror', 'Ties', 'Make', 'Venezuela', 'Direct', 'Threat', 'to', 'USA,', 'Former', 'UN', 'Security', 'Council', 'President', 'Says', '-', 'Breitbart']\",\n",
       " \"['Rick', 'Ross', 'Documentary', 'Video', 'Doubles', 'as', 'Ad', 'for', 'Checkers', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Oil', 'has', 'been', 'spilling', 'into', 'the', 'Pacific', 'Ocean', 'since', 'last', 'month', 'and', 'its', 'being', 'totally', 'ignored']\",\n",
       " \"['Podesta', 'To', 'Mills:', '“We', 'Are', 'Going', 'To', 'Have', 'To', 'Dump', 'All', 'Those', 'Emails”']\",\n",
       " \"['Hillary', 'Collapses', 'On', 'Her', 'Way', 'To', 'The', 'Stage,', 'Sellout', 'Bruce', 'Springsteen', 'Covers', 'For', 'Her', '–', 'The', 'Resistance:', 'The', 'Last', 'Line', 'of', 'Defense']\",\n",
       " \"['Those', 'Who', 'Thoughtlessly', 'Disbelieve', '“Conspiracy', 'Theories”', 'Need', 'To', 'Read', 'This']\",\n",
       " \"['Dr.', 'David', 'Duke', 'and', 'Dr.', 'Slattery', 'Expose', 'Hillary’s', 'Treason', 'and', 'Why', 'Trump', '&', 'Duke', 'will', 'Win!']\",\n",
       " \"['Russian', 'frigate', 'off', 'Syrian', 'cost', 'blasts', 'terrorist', 'HQ', 'with', 'cruise', 'missiles', '(VIDEO)']\",\n",
       " \"['Hillary', 'Clinton', 'Liked', 'Covert', 'Action', 'if', 'It', 'Stayed', 'Covert,', 'Transcript', 'Shows', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Accusing', 'two', 'young', 'men', 'in', 'Al-Qatif']\",\n",
       " \"['US:', 'Kurdish', 'Troops', 'Will', 'Be', 'Involved', 'in', 'Invading', 'ISIS', 'Capital', 'of', 'Raqqa']\",\n",
       " \"['Awakened', 'Humanity', 'Awaits', 'Fully', 'Scripted', 'Ending', 'As', 'Controversial', 'Election', 'Day', 'Nears']\",\n",
       " \"['George', 'Soros-Backed', 'Climate', 'March', 'Brings', 'Celebs', 'to', 'National', 'Mall', 'on', 'Sweltering', 'Saturday', '-', 'Breitbart']\",\n",
       " \"['Edward', 'Snowden’s', 'Long,', 'Strange', 'Journey', 'to', 'Hollywood', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', '“Blacks', 'For', 'Trump”', 'Man', 'Once', 'Belonged', 'To', 'A', 'Murderous', 'Cult']\",\n",
       " \"['Bill', 'Clinton’s', 'Lover:', 'He', 'Called', 'Ruthless', 'Hillary', '‘The', 'Warden’']\",\n",
       " \"['Fox', 'News', 'Just', 'Exposed', 'Hillary’s', 'ILLEGAL', 'VOTING', 'Scheme', 'To', 'The', 'Entire', 'Country!']\",\n",
       " '[\\'Russia,\\', \\'Cina\\', \\'e\\', \\'Arabia\\', \\'Saudita\\', \\'domano\\', \"l\\'egemonia\", \\'del\\', \\'dollaro,\\', \\'di\\', \\'Ariel\\', \\'Noyola\\', \\'Rodríguez\\']',\n",
       " \"['In', 'Its', 'Third', 'Month,', 'India’s', 'Cash', 'Shortage', 'Begins', 'to', 'Bite', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['There’s', 'Only', 'One', 'Trump', 'Administration', 'Position', 'That’s', 'Gaining', 'Popularity', 'And', 'It’s', 'Going', 'To', 'Shock', 'You', '-', 'Breitbart']\",\n",
       " \"['Bill', 'Clinton', 'Said', 'White', 'Middle-class', 'Life', 'Expectancy', 'Declined', 'During', 'Obama', 'Years']\",\n",
       " \"['Furious', 'Eric', 'Holder', 'Just', 'Issued', 'A', 'Dire', 'Warning', 'About', 'Comey’s', 'Partisan', 'Smears']\",\n",
       " '[\\'Memo\\', \\'to\\', \\'Trump:\\', \"\\'Action\", \\'This\\', \"Day!\\'\"]',\n",
       " \"['Chaos', 'and', 'Desperation', 'as', 'Thousands', 'Flee', 'Aleppo', 'Amid', 'Government', 'Advance', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Wildfire', 'Empties', 'Fort', 'McMurray', 'in', 'Alberta’s', 'Oil', 'Sands', 'Region', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['PressTV-Russian', 'warships', 'flotilla', 'off', 'Syrian', 'coast']\",\n",
       " \"['Schumer:', 'Sessions', 'Should', 'Be', 'Investigated', '-', 'He', '’Seems', 'To', 'Be', 'Violating’', 'His', 'Recusal', '-', 'Breitbart']\",\n",
       " \"['Amid', 'Division,', 'a', 'March', 'in', 'Washington', 'Seeks', 'to', 'Bring', 'Women', 'Together', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Zoe', 'Saldana:', 'Trump', 'Won', 'Because', 'Hollywood', '’Got', 'Cocky,', 'Became', 'Arrogant', 'Bullies’']\",\n",
       " \"['‘Anti-Establishment’', 'Trump', 'Plans', 'to', 'Appoint', 'Goldman', 'Sachs', 'and', 'George', 'Soros', 'Insiders']\",\n",
       " \"['Do', 'Cholesterol', 'Drugs', 'Have', 'Men', 'By', 'Their', 'Gonads?']\",\n",
       " \"['Trump:', 'Flynn', 'Treated', '’Very', 'Unfair’', 'by', '’Fake', 'Media,’', 'Illegal', 'Leaks', '-', 'Breitbart']\",\n",
       " \"['California', 'Today:', 'In', 'Virtual', 'Reality,', 'Investigating', 'the', 'Trayvon', 'Martin', 'Case', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Nearly', '8', 'Decades', 'Later,', 'an', 'Apology', 'for', 'a', 'Lynching', 'in', 'Georgia', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Marcia', 'Clark', 'Finally', 'Has', 'a', 'Moment', 'to', 'Savor', 'at', 'the', 'Emmys', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['In', '‘Brexit’', 'Speech,', 'Theresa', 'May', 'Outlines', 'Clean', 'Break', 'for', 'U.K.', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['A', 'Man', 'Who', 'Hated', 'Black', 'Men', 'Found', 'a', 'Victim', 'Who', 'Cared', 'for', 'Others', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Michael', 'Flynn', 'Failed', 'to', 'Disclose', 'Income', 'From', 'Russia-Linked', 'Entities', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['US', 'Supreme', 'Court', 'justice', 'groped', 'female', 'lawyer', 'in', '1999:', 'Report']\",\n",
       " \"['Federal', 'Judge', 'Throws', 'Out', 'Convicted', 'D.C.', 'Sniper’s', 'Four', 'Life', 'Sentences']\",\n",
       " \"['Donald', 'Trump,', 'Obama,', 'Thanksgiving:', 'Your', 'Weekend', 'Briefing', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Очередная', 'автоколонна', 'МЧС', 'с', 'гуманитарной', 'помощью', 'направилась', 'в', 'Донбасс']\",\n",
       " \"['The', 'Crisis', 'of', 'the', 'European', 'Union', 'Is', 'Irreversible', '-', 'Giancarlo', 'Elia', 'Valori']\",\n",
       " \"['Meet', 'the', 'Neocons,', '9/11', 'Criminals', 'and', 'Goldman', 'Bankers', 'On', 'Team', 'Trump', ':', 'The', 'Corbett', 'Report']\",\n",
       " \"['Vine', '2013-2016:', 'celebrate', 'the', 'life', 'and', 'death', 'of', 'an', 'app', 'with', 'these', '12', 'clips']\",\n",
       " \"['Squatty', 'Potty', 'CEO:', 'Griffin', 'Image', '’So', 'Divisive', 'and', 'So', 'Disturbing,', 'You', 'Don’t', 'Have', 'a', 'Decision,', 'Really’']\",\n",
       " \"['11', 'Things', 'To', 'Let', 'Go', 'Of', 'Before', 'The', 'New', 'Year']\",\n",
       " \"['Comment', 'on', 'MA', 'police', 'union', 'posts', 'pics', 'of', 'Hillary', 'being', 'arrested', 'on', 'Facebook', 'by', 'Dan']\",\n",
       " \"['Koch', 'Brothers', 'Secretly', 'Allied', 'w.', 'George', 'Soros', 'for', 'Hillary', 'Clinton']\",\n",
       " \"['Trump', 'Expands', 'Search', 'for', 'His', 'Secretary', 'of', 'State', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\\'Teacher\\', \\'To\\', \\'11\\', \\'yr\\', \\'Old:\\', \"\\'I\", \"Can\\'t\", \\'Wait\\', \\'Until\\', \\'Trump\\', \\'Is\\', \"Elected,He\\'s\", \\'Going\\', \\'To\\', \\'Deport\\', \\'All\\', \\'You\\', \"Muslims\\'\"]',\n",
       " \"['Cost,', 'Not', 'Choice,', 'Is', 'Top', 'Concern', 'of', 'Health', 'Insurance', 'Customers', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Shaq', 'Announces', 'Plan', 'to', 'Run', 'for', 'Sheriff', 'in', '2020', '-', 'Breitbart']\",\n",
       " \"['Illegal', 'Immigrants', 'Crossing', 'The', 'Border', 'To', 'Vote']\",\n",
       " \"['Pirates', 'Fail', 'to', 'Take', 'the', 'Helm:', 'Iceland’s', 'Pirate', 'Party', 'Gains', 'Mileage', 'But', 'not', 'Enough', 'to', 'Steady', 'Ship', 'Alone']\",\n",
       " \"['The', 'Billionaire', 'Who’s', 'Building', 'a', 'Davos', 'of', 'His', 'Own', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Running', 'Into', 'Danger', 'on', 'an', 'Alaskan', 'Trail', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Ve', 'la', 'película', 'de', 'su', 'vida', 'y', 'descubre', 'que', 'ha', 'llevado', 'siempre', 'un', 'trozo', 'de', 'lechuga', 'entre', 'los', 'dientes']\",\n",
       " \"['Dems', 'Win', 'Congressional', 'Baseball', 'Game,', 'Give', 'Trophy', 'to', 'Republican', 'Steve', 'Scalise', '-', 'Breitbart']\",\n",
       " \"['France', '2015', 'to', 'Present:', 'Strict', 'Gun', 'Control', 'a', 'Paper', 'Tiger', '-', 'Breitbart']\",\n",
       " \"['China,', 'Russia,', 'The', 'Silk', 'Road,', 'Commodities,', 'Nixon', 'And', 'A', 'Massive', 'Bull', 'Market', 'In', 'Gold', '&', 'Silver']\",\n",
       " \"['Troubled', 'Quarterback', 'Johnny', 'Manziel', 'to', 'Appear', 'at', 'Shopping', 'Mall', 'to', 'Sign', 'Autographs', 'During', 'Next', 'Super', 'Bowl', '-', 'Breitbart']\",\n",
       " \"['Bob', 'Dylan', 'Accused', 'of', 'Lifting', 'Parts', 'of', 'Nobel', 'Prize', 'Speech', 'from', 'SparkNotes']\",\n",
       " \"['After', 'Blimp', 'Crash', 'and', 'E.', 'Coli', 'Contamination,', 'Snakebitten', 'U.S.', 'Open', 'Witnesses', 'a', 'Spectator', 'Death', '-', 'Breitbart']\",\n",
       " \"['Public', 'Employees', 'and', 'the', 'Shadow', 'World', 'of', 'American', 'Carnage']\",\n",
       " \"['The', 'Syria', 'conundrum', '-', 'Press', 'TV']\",\n",
       " \"['ESPN’s', 'LZ', 'Granderson:', '’Justified’', 'to', 'Think', 'Kaepernick', 'Is', 'Being', '’Blackballed’', 'if', 'Nobody', 'Signs', 'Him', '-', 'Breitbart']\",\n",
       " \"['Prescription', 'Painkiller', 'Deaths', 'Dropped', '25%', 'in', 'States', 'That', 'Legalized', 'Marijuana']\",\n",
       " \"['We’re', 'in', 'a', 'Low-Growth', 'World.', 'How', 'Did', 'We', 'Get', 'Here?', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Istanbul,', 'Donald', 'Trump,', 'Benjamin', 'Netanyahu:', 'Your', 'Morning', 'Briefing', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Hitler', 'or', 'Hillary?']\",\n",
       " \"['Dalian', 'Wanda’s', 'Hollywood', 'Event', 'Is', 'Itself', 'a', 'Production', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Comment', 'on', '2:00PM', 'Water', 'Cooler', '11/2/2016', 'by', 'Timmy']\",\n",
       " \"['Breakdown', 'of', 'the', 'Clinton', 'Money', 'Machine']\",\n",
       " 'nan',\n",
       " \"['California,', 'at', 'Forefront', 'of', 'Climate', 'Fight,', 'Won’t', 'Back', 'Down', 'to', 'Trump', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'New', 'York', 'Times', 'to', 'Offer', 'Open', 'Access', 'on', 'Web', 'and', 'Apps', 'for', 'the', 'Election', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Fifth', 'Varshavyanka', 'submarine', 'joins', 'Russia’s', 'Navy', '|', 'Russia', '&', 'India', 'Report']\",\n",
       " \"['G.E.,', 'the', '124-Year-Old', 'Software', 'Start-Up', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['$2,700', 'for', 'Hillary', 'Clinton', 'at', '‘Hamilton’?', 'That', 'Would', 'Be', 'Enough', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Brain', 'Concussions', 'in', 'Children', 'and', 'Adults:', 'What', 'to', 'Know', 'About', 'Vaccine', 'Damage']\",\n",
       " \"['Trump', 'the', 'Great', '—', 'Paul', 'Craig', 'Roberts']\",\n",
       " \"['Hillary', 'FRANTIC', 'As', 'Dirty', 'Secret', 'Implodes,', 'Gets', 'Worse', 'With', 'Prison', 'Bombshell']\",\n",
       " \"['Renting', 'a', 'Car?', 'Know', 'the', 'Rules', 'of', 'the', 'Road', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['All', 'Impeachments', 'Are', 'Political.', 'But', 'Was', 'Brazil’s', 'Something', 'More', 'Sinister?', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump,', 'in', 'Interview,', 'Moderates', 'Views', 'but', 'Defies', 'Conventions', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Review:', '‘Sweetbitter,’', 'a', '‘Bright', 'Lights,', 'Big', 'City’', 'for', 'the', 'Restaurant', 'Set', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Gonzaga', 'Beats', 'South', 'Carolina', 'in', 'Final', 'Four', 'for', 'Shot', 'at', 'Another', 'First', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Gorka:', 'Trump', '’Is', 'Not', 'an', 'Interventionist', 'Commander-in-Chief,’', '’Nothing', 'Has', 'Changed’', '-', 'Breitbart']\",\n",
       " \"['Re:', 'Thank', 'You', 'FBI:', 'The', 'Clinton', 'Email', 'Investigation', 'Has', 'Shifted', 'The', 'Poll', 'Numbers', 'Significantly', 'In', 'Trump’s', 'Favor']\",\n",
       " \"['Race,', 'Not', 'Class,', 'Dictates', 'Republican', 'Future']\",\n",
       " \"['Gorsuch:', 'Scalia', 'A', '’Lion', 'of', 'the', 'Law’', '-', 'Judges', 'Should', 'Look', 'to', 'What', 'Law', 'Demands,', 'Not', 'What', 'They', 'Prefer', '-', 'Breitbart']\",\n",
       " \"['NC', 'State', 'Provides', 'Students', 'with', 'Post-Election', 'Comfort', 'Food,', 'Therapy']\",\n",
       " \"['Iraqi', 'Troops', 'Push', 'Further', 'into', 'Mosul;', '202', 'Killed', 'Across', 'Iraq']\",\n",
       " \"['Pregnant', 'Women', 'Turn', 'to', 'Marijuana,', 'Perhaps', 'Harming', 'Infants', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['Biggest', 'Winners', 'and', 'Losers', 'of', 'the', '2016', 'U.S.', 'Presidential', 'Election']\",\n",
       " \"['Is', 'Trump', 'Psychic?', 'Listen', 'To', 'His', 'Words', 'One', 'Year', 'Ago,', 'Look', 'At', 'What', 'JUST', 'HAPPENED!']\",\n",
       " \"['McCain:', 'Trump’s', 'Attacks', 'on', 'Press', 'Are', '’How', 'Dictators', 'Get', 'Started’', '-', 'Breitbart']\",\n",
       " \"['JUDGMENT', 'DAY:', 'The', 'One', 'Reason', 'Why', 'Every', 'Christian', 'And', 'Jew', 'In', 'America', 'Should', 'Vote', 'For', 'Donald', 'Trump']\",\n",
       " \"['The', 'Only', 'Coup', 'Is', 'a', 'Stolen', 'Election']\",\n",
       " \"['СМИ:', 'в', 'России', 'создадут', 'систему', 'для', 'отслеживания', 'приближающихся', 'со', 'стороны', 'Солнца', 'тел']\",\n",
       " \"['5', 'Things', 'You', 'Need', 'to', 'Know', 'About', 'the', 'Blacked', 'Out', 'Dakota', 'Access', 'Pipeline', 'Protests']\",\n",
       " \"['Germany:', 'Iraqi', 'Asylum-Seeker', 'Convicted', 'of', 'Raping', 'Chinese', 'Students']\",\n",
       " \"['Teenage', 'Boy', 'KNOCKS', 'OUT', 'His', 'Classmate', 'For', 'Assaulting', 'Their', 'Female', 'Teacher', 'In', 'The', 'FACE-', 'And', 'It’s', 'EPIC!']\",\n",
       " \"['What', 'Does', 'it', 'Take', 'to', 'Bring', 'Hillary', 'Clinton', 'to', 'Justice?']\",\n",
       " \"['ICE', 'Rounds', 'Up', '44', 'Criminal', 'Aliens', 'in', 'Texas', 'Capital']\",\n",
       " \"['The', 'Election:', 'Of', 'Hate,', 'Grief,', 'and', 'a', 'New', 'Story']\",\n",
       " \"['Hillary', 'has', 'a', 'question', 'about', 'Michelle', 'Obama;', 'Can', 'you', 'help', 'her', 'out?']\",\n",
       " \"['Keith', 'Vaz,', 'British', 'Lawmaker,', 'Quits', 'Senior', 'Post', 'Amid', 'Sex', 'and', 'Drug', 'Scandal', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Comment', 'on', 'Sunday', 'Devotional:', 'The', 'whole', 'Universe', 'is', 'as', 'a', 'grain', 'by', 'Sunday', 'Devotional:', 'The', 'whole', 'Universe', 'is', 'as', 'a', 'grain', '—', 'Fellowship', 'of', 'the', 'Minds', '|', 'kommonsentsjane']\",\n",
       " \"['Donald', 'Trump,', 'Syria,', 'Emperor', 'Akihito:', 'Your', 'Morning', 'Briefing', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['HELL', 'HAS', 'FROZEN', 'OVER!', 'Michelle', 'Obama', 'Just', 'Made', 'A', 'Hillary-Destroying', 'Move', 'On', 'Twitter']\",\n",
       " \"['FLYNN:', 'Critics', 'Call', 'Nuclear', 'Scientist', 'Miss', 'USA', '’Dumb’', 'After', 'She', 'Disses', 'Feminism', 'and', 'Calls', 'Health', 'Care', 'a', '’Privilege’', '-', 'Breitbart']\",\n",
       " \"['Hampshire', 'College', 'Student', 'Accused', 'of', 'Assaulting', 'Basketball', 'Player', 'for', 'Wearing', 'Hair', 'Braids,', 'Claims', '’Cultural', 'Appropriation’', '-', 'Breitbart']\",\n",
       " \"['The', 'Obamamometer’s', 'Toxic', 'Legacy:', 'The', 'Rule', 'of', 'Lawlessness']\",\n",
       " 'nan',\n",
       " \"['For', 'Simone', 'Manuel,', 'Gold', 'Ripples', 'Beyond', 'the', 'Pool', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Rand', 'Paul:', 'Polls', 'Showing', 'Hillary', 'Ahead', 'Are', '‘Designed', 'To', 'Suppress', 'Turnout’']\",\n",
       " \"['How', 'Voting', 'Machines', 'Are', 'Programmed', 'In', 'Order', 'To', 'Steal', 'Elections']\",\n",
       " \"['Redrawing', 'the', 'tree', 'of', 'life:', 'Scientists', 'discover', 'new', 'bacteria', 'groups,', 'stunning', 'microbial', 'diversity', 'underground']\",\n",
       " '[\\'Trump\\', \\'And\\', \\'Putin:\\', \"\\'We\", \\'Will\\', \\'Destroy\\', \\'ISIS\\', \\'Once\\', \\'And\\', \\'For\\', \"All!\\'\"]',\n",
       " '[\\'Russia\\', \\'may\\', \\'run\\', \\'out\\', \\'of\\', \\'patience\\', \\'and\\', \\'respond\\', \\'to\\', \"USA\\'s\", \\'rudeness\\']',\n",
       " \"['PHOTO:', 'Game', 'Camera', 'Catches', 'Glimpse', 'Of', 'Possible', '3-Antlered', 'Buck']\",\n",
       " \"['Turkey’s', 'Relations', 'With', 'Europe', 'Sink', 'Amid', 'Quarrel', 'With', 'Netherlands', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['WikiLeaks', 'Documents', 'Reveal', 'United', 'Nations', 'Interest', 'In', 'UFOs', '[VIDEO]']\",\n",
       " \"['Blue', 'State', 'Blues:', 'The', 'Deliberate', 'Politicization', 'of', 'Intimacy']\",\n",
       " \"['Clare', 'Hollingworth,', 'Reporter', 'Who', 'Broke', 'News', 'of', 'World', 'War', 'II,', 'Dies', 'at', '105', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['North', 'Miami', 'Police', 'Officers', 'Shoot', 'Man', 'Aiding', 'Patient', 'With', 'Autism', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\\'The\\', \\'Path\\', \\'to\\', \\'Total\\', \\'Dictatorship:\\', \"America\\'s\", \\'Shadow\\', \\'Government\\', \\'and\\', \\'Its\\', \\'Silent\\', \\'Coup\\']',\n",
       " \"['Americans', 'given', 'historic', 'opportunity', 'to', 'tell', 'Donald', 'Trump', 'to', 'go', 'fuck', 'himself']\",\n",
       " \"['Efforts', 'to', 'Curb', 'Police', 'Abuses', 'Have', 'Mixed', 'Record,', 'and', 'Uncertain', 'Future', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Did', 'This', 'Just', 'Signal', 'A', 'Major', 'Bottom', 'In', 'Gold', '&', 'Silver?']\",\n",
       " \"['Hey', 'Ho,', 'It’s', 'Old:', 'England', 'Embraces', 'Punk', 'Rock', '40', 'Years', 'Later', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Canoes', 'Reek', 'of', 'Genocide,', 'Theft', 'and', 'White', 'Privilege,', 'Says', 'Canadian', 'Professor']\",\n",
       " \"['Illegal', 'Immigrant', 'Advocates', 'Pledge', 'to', 'Resist', 'Deportation', 'Under', 'Trump']\",\n",
       " \"['CDC', 'Scientist', 'Confirms', 'Donald', 'Trump', 'is', 'Right', 'About', 'Vaccines', 'and', 'Autism']\",\n",
       " \"['Breitbart', 'News', 'Daily:', 'Draining', 'the', 'Swamp', '-', 'Breitbart']\",\n",
       " \"['Trump:', 'The', 'Media’s', 'Frankenstein', 'Monster']\",\n",
       " \"['Watch:', 'Tony', 'Romo', 'Says', 'Goodbye', 'to', 'Dallas', 'Cowboys', 'in', 'Instagram', 'Video', '-', 'Breitbart']\",\n",
       " \"['Globalism:', 'A', 'Far-Right', 'Conspiracy', 'Theory', 'Buoyed', 'by', 'Trump', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Georgia', 'Father', 'Is', 'Convicted', 'of', 'Murder', 'in', 'Toddler’s', 'Death', 'in', 'Hot', 'Car', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Richard', 'Bolles', 'Dies', 'at', '90', 'Wrote', '‘What', 'Color', 'Is', 'Your', 'Parachute?’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Stranahan:', 'Steve', 'Bannon', 'Nailed', 'It', 'on', 'the', 'Media’s', 'Fight', 'Against', 'Trump', '-', 'Breitbart']\",\n",
       " \"['Texas', 'Elector', 'Expects', 'Massive', 'Corruption', 'Related', 'to', 'the', 'Electoral', 'College', 'Vote']\",\n",
       " \"['Not', 'Guilty:', 'The', 'Power', 'of', 'Nullification', 'to', 'Counteract', 'Government', 'Tyranny']\",\n",
       " \"['More', 'Than', '1/4', 'Trillion', 'In', 'New', 'Debt', 'In', '30', 'Days']\",\n",
       " \"['Tory', 'Councillor', 'Say', 'Homeless', 'People', 'Should', 'Be', '‘Eliminated’']\",\n",
       " \"['Review:', 'In', '‘Hillbilly', 'Elegy,’', 'a', 'Tough', 'Love', 'Analysis', 'of', 'the', 'Poor', 'Who', 'Back', 'Trump', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Criminal-in-chief?']\",\n",
       " \"['All', 'Governments', 'Lie,', 'The', 'Movie']\",\n",
       " \"['Trump', 'and', 'Brexit', 'Defeat', 'Globalism,', 'for', 'Now', 'Anyway']\",\n",
       " \"['Facing', 'Congress,', 'Some', 'Sports', 'Officials', '(Not', 'All)', 'Begin', 'to', 'Confront', 'Sexual', 'Abuse', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Airbnb', 'Ends', 'Fight', 'With', 'New', 'York', 'City', 'Over', 'Fines', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Big', 'Pharma’s', 'Martin', 'Shkreli', 'Suspended', 'From', 'Twitter', '-', 'Breitbart']\",\n",
       " \"['Next', 'Big', 'Tech', 'Corridor?', 'Between', 'Seattle', 'and', 'Vancouver,', 'Planners', 'Hope', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Evidence', 'That', 'Robots', 'Are', 'Winning', 'the', 'Race', 'for', 'American', 'Jobs', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Hillary', 'Clinton’s', '“Sudden', 'Move”', 'Of', '$1.8', 'Billion', 'To', 'Qatar', 'Central', 'Bank', 'Stuns', 'Financial', 'World']\",\n",
       " \"['Seeing', 'Africa', 'by', 'Road', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['What', 'happens', 'when', 'Hip-Hop', 'Hillary', 'goes', 'for', 'dead-broke?', 'Brace', 'yourselves.']\",\n",
       " \"['Bill', 'Maher', 'Isn’t', 'High', 'on', 'Trump:', 'The', 'State', 'of', 'Free', 'Speech', 'in', 'a', 'New', 'Era', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['The', 'L.P.G.A.', 'Tour', 'and', 'Donald', 'Trump:', 'It’s', 'Complicated', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Legend', 'Art', 'Cashin', 'On', 'A', 'Trump', 'Presidency,', 'The', 'New', 'World', 'Order,', 'Gold,', 'Brexit,', 'The', 'Great', 'Depression', 'And', 'Why', 'We', 'Will', 'See', 'Panic']\",\n",
       " \"['FL', 'Sheriff:', '’Not', 'a', 'Day', 'Goes', 'By', 'That', 'We', 'All', 'Don’t', 'Arrest', 'A', 'Lot', 'of', 'Illegal', 'Aliens’', 'Who', 'Are', '’Preying’', 'on', 'People', '-', 'Breitbart']\",\n",
       " \"['Donald', 'Trump', 'Threatens', 'to', 'Cancel', 'Berkeley', 'Federal', 'Funds', 'After', 'Riots', 'Shut', 'Down', 'Milo', 'Event']\",\n",
       " \"['Концепция', 'записка', 'российского', 'председательства']\",\n",
       " \"['Bill', 'Herz,', 'Last', 'of', '‘War', 'of', 'the', 'Worlds’', 'Broadcast', 'Crew,', 'Dies', 'at', '99', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Gary', 'Johnson', 'Equates', 'Syria', 'Deaths', 'Caused', 'by', 'Assad', 'and', 'West', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Republican', 'Senators', 'Bill', 'to', 'Defund', 'UN', 'Over', 'Anti-Israel', 'Resolution']\",\n",
       " 'nan',\n",
       " \"['Rose', 'Evansky,', 'a', 'Pioneer', 'in', 'Women’s', 'Hairstyling,', 'Dies', 'at', '94', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['Blackstone,', 'Saudi', 'Arabia', 'Announce', '$40', 'Billion', 'Investment', 'in', 'U.S.', 'Infrastructure', '-', 'Breitbart']\",\n",
       " \"['Saturated', 'Fat', 'and', 'Heart', 'Disease:', '“The', 'Greatest', 'Scam', 'in', 'the', 'History', 'of', 'Medicine”']\",\n",
       " \"['Dem', 'Sen', 'Merkley:', 'Gorsuch', 'Nomination', 'A', '’Court-Packing', 'Scheme’', '-', 'He', 'Should', 'Have', 'Turned', 'Down', 'Nomination', '-', 'Breitbart']\",\n",
       " \"['Four', 'Common', 'Mistakes', 'When', 'Burning', 'Wood']\",\n",
       " \"['Push', 'for', 'Internet', 'Privacy', 'Rules', 'Moves', 'to', 'Statehouses', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['Madonna', 'Gave', 'a', 'Surprise', 'Pop-Up', 'Concert', 'to', 'Support', 'Clinton.', 'We', 'Were', 'There.', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\\'Women\\', \"won\\'t\", \\'earn\\', \\'the\\', \\'same\\', \\'as\\', \\'men\\', \\'for\\', \\'another\\', \\'two\\', \\'centuries\\', \\'-\\', \\'report\\']',\n",
       " \"['Trump', 'Supporter', 'Plans', 'Anti-Trump', 'Rally', 'over', 'Lack', 'of', 'Tax', 'Reform']\",\n",
       " \"['British', 'Healthcare', 'Offers', 'a', 'Glimpse', 'into', 'the', 'Future', 'of', 'Obamacare']\",\n",
       " 'nan',\n",
       " \"['Trump', 'to', 'Iranian', 'President', 'Rouhani:', '’Better', 'Be', 'Careful’', '-', 'Breitbart']\",\n",
       " \"['BREAKING:', 'Sec', 'of', 'Defense', 'Carter', 'Attempting', 'to', 'Fool', 'the', 'American', 'Public', 'About', 'Veterans!', 'Caught', 'Red-Handed!']\",\n",
       " \"['Putin’s', 'Adviser', 'Takes', 'Credit', 'For', 'Trump', 'Victory:', '‘Maybe', 'We', 'Helped', 'A', 'Bit', 'With', 'Wikileaks’']\",\n",
       " \"['Warren', 'Buffett', 'Stake', 'Suggests', 'Apple', 'Is', 'All', 'Grown', 'Up', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\\'Re:\\', \\'The\\', \"FBI\\'s\", \\'Clinton\\', \\'Email\\', \\'Investigation\\', \\'Has\\', \\'Shifted\\', \\'Poll\\', \\'Numbers\\', \\'Significantly\\', \\'In\\', \\'Trump’s\\', \\'Favor\\']',\n",
       " \"['Hillary', 'Horrified', 'As', '2', 'Pics', 'Surfaced', 'Overnight', 'That', 'She', 'Didn’t', 'Want', 'Out']\",\n",
       " \"['Paul', 'LePage,', 'the', 'Governor', 'of', 'Maine,', 'Now', 'Says', 'He’s', 'Not', 'Quitting', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Colorado', 'Radio', 'Station’s', 'Paul', 'Martin', 'Interviews', 'Dave', 'Hodges', 'on', 'Election', 'Fraud', 'and', 'Standing', 'Rock']\",\n",
       " \"['Bill', 'Clinton', 'Wants', 'to', 'be', 'Called', 'Something', 'Completely', 'Ridiculous', 'if', 'Hillary', 'is', 'Elected']\",\n",
       " \"['After', 'Her', 'Son’s', 'Death,', 'Sally', 'Mann', 'Stages', 'a', 'Haunting', 'Show', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump', 'Team’s', 'Links', 'to', 'Russia', 'Crisscross', 'in', 'Washington', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['North', 'Korean', 'Is', 'Arrested', 'in', 'Killing', 'of', 'Kim', 'Jong-un’s', 'Half', 'Brother', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['SURGEONS', 'ADMIT', 'THAT', 'MAMMOGRAPHY', 'IS', 'OUTDATED', 'AND', 'HARMFUL', 'TO', 'WOMEN']\",\n",
       " \"['Le', 'terrorisme', 'régional', 'protège', 'Israël']\",\n",
       " \"['WAR', 'ON', 'THE', 'STREETS', 'OF', 'PARIS:', 'Armed', 'migrants', 'fight', 'running', 'battles', 'in', 'the', 'French', 'capital']\",\n",
       " \"['Oscar', 'Voter:', 'Meryl', 'Streep', 'Only', 'Nominated', 'for', 'Anti-Trump', 'Speech']\",\n",
       " \"['Texas', 'Enacts', '’Anti-Sharia’', 'Law']\",\n",
       " \"['Chicago', 'Police', 'Board', 'Chair:', 'Windy', 'City', 'Needs', 'Federal', 'Help', 'to', 'Turn', 'Tide', 'on', 'Crime', '-', 'Breitbart']\",\n",
       " \"['Looking', 'Beyond', 'November', '8th', '-', 'A', 'Song', 'of', 'Oligarchy', 'and', 'Doom']\",\n",
       " '[\\'Hillary\\', \\'cancels\\', \\'public\\', \\'appearance\\', \\'due\\', \\'to\\', \\'a\\', \\'large\\', \\'crowd\\', \\'of\\', \\'people\\', \\'chanting\\', \\'\"Lock\\', \\'Her\\', \\'Up!\"\\']',\n",
       " \"['Schools', 'All', 'Over', 'America', 'Are', 'Closing', 'On', 'Election', 'Day', 'Due', 'To', 'Fears', 'Of', 'Violence']\",\n",
       " \"['Neil', 'Young', 'Celebrates', 'His', '71st', 'Birthday', 'By', 'Performing', 'At', 'Standing', 'Rock']\",\n",
       " \"['Exclusive', '—', 'Amid', 'Paul', 'Ryan’s', 'Obamacare', '2.0', 'Push,', 'Mississippi’s', 'Chris', 'McDaniel', 'Preps', 'for', 'Potential', '2018', 'Senate', 'Run', '-', 'Breitbart']\",\n",
       " \"['BOOM:', 'Short', 'List', 'of', 'People', 'More', 'Inspiring', 'Than', 'Michelle', 'Obama']\",\n",
       " \"['‘More', 'Artists', 'Are', 'Going', 'to', 'Boycott’:', 'The', 'Grammys', 'Face', 'Fallout', 'After', 'Fraught', 'Awards', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['VIDEO:', 'Idiot', 'Destroys', 'Trump’s', 'Hollywood', 'Star,', 'Gets', 'BAD', 'NEWS', 'Seconds', 'Later']\",\n",
       " \"['“He', 'Won', 'Because', 'The', 'Elites', 'WANT', 'HIM', 'There,', 'The', 'Global', 'Economy', 'WILL', 'Collapse”']\",\n",
       " \"['4', 'Best', 'Health', 'Benefits', 'of', 'Sweating']\",\n",
       " \"['Senate', 'Confirms', 'Scott', 'Pruitt', 'as', 'E.P.A.', 'Head', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Crickets,', 'Snakes,', 'Crabs:', 'A', 'Mix', 'of', 'Fact', 'and', 'Fraud', 'in', 'New', 'York’s', 'Subway', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Sessions’s', 'Potential', 'Deputy', 'Faces', 'a', 'Stern', 'Test', 'on', 'Russia', 'Inquiries', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['‘Suicide', 'Squad’', 'Tops', 'Box', 'Office', 'for', 'Second', 'Weekend', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Health', 'Insurers', 'Plan', 'Rate', 'Hikes', 'for', 'Obamacare', 'Exchanges', '-', 'Breitbart']\",\n",
       " \"['Re:', 'Looks', 'Like', 'Someone', 'Thinks', 'the', 'Democrats', 'in', 'Ohio', 'Are', 'Full', 'of', 'Manure']\",\n",
       " \"['Syrian', 'War', 'Report', '–', 'November', '14,', '2016:', 'Govt', 'Forces', 'to', 'Relaunch', 'Offensive', 'Operations', 'inside', 'and', 'outside', 'Aleppo']\",\n",
       " \"['Mystery', 'solved!', 'So', 'THIS', 'is', 'what', 'gets', 'Hillary', 'Clinton', 'to', 'get', 'movin’', '–', 'twitchy.com']\",\n",
       " \"['Senate', 'Narrowly', 'Passes', 'Rollback', 'of', 'Obama-Era', '‘Auto-I.R.A.’', 'Rule', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['In', 'Cramped', 'and', 'Costly', 'Bay', 'Area,', 'Cries', 'to', 'Build,', 'Baby,', 'Build', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Championing', 'Optimism,', 'Obama', 'Hails', 'Clinton', 'as', 'His', 'Political', 'Heir', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Fiona', 'Apple', 'Releases', 'a', 'Trump', 'Protest', 'Chant', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Susan', 'Rice:', 'U.S.', 'Must', 'Integrate', 'LGBT', 'Rights', 'into', 'Gov’t', 'and', 'Foreign', 'Policy']\",\n",
       " \"['Trump', 'Organization', 'Moves', 'to', 'Avoid', 'Possible', 'Conflicts', 'of', 'Interest', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Sweden', 'on', 'the', 'Brink?', 'Police', 'Force', 'Pushed', 'to', 'Breaking', 'Point', 'by', 'Violence', 'amid', 'Migrant', 'Influx']\",\n",
       " \"['Comment', 'on', 'Hemp', 'vs', 'Cotton:', 'The', 'Ultimate', 'Showdown', 'by', 'Hemp:', 'Readdressing', 'Cannabis', '–', 'kuebiko.co']\",\n",
       " \"['Jared', 'Kushner,', 'Trump’s', 'Son-in-Law,', 'Is', 'Cleared', 'to', 'Serve', 'as', 'Adviser', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Fisherman', 'Faces', 'Life', 'in', 'Prison', 'for', 'Catching', '$500,000', 'Worth', 'of', 'Cocaine', 'and', 'Selling', 'It', '-', 'Breitbart']\",\n",
       " \"['Stepping', 'Out', 'of', 'the', 'Ring', 'While', 'Rolling', 'With', 'the', 'Punches', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Adnan', 'Syed,', 'of', '‘Serial’', 'Podcast,', 'Gets', 'a', 'Retrial', 'in', 'Murder', 'Case', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['NYT', 'Admits', 'Key', 'Al', 'Qaeda', 'Role', 'in', 'Aleppo']\",\n",
       " '[\\'Scientists\\', \\'say\\', \\'weird\\', \\'signals\\', \\'from\\', \\'space\\', \\'are\\', \"\\'probably\\'\", \\'aliens\\']',\n",
       " \"['U.S.', 'Swimmers’', 'Disputed', 'Robbery', 'Claim', 'Fuels', 'Tension', 'in', 'Brazil', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['5', 'Ways', 'to', 'Take', 'a', 'Self-Care', 'Vacation', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['370', 'Economists', 'Sign', 'Letter', 'Urging', 'America', 'Not', 'To', 'Vote', 'For', 'Donald', 'Trump']\",\n",
       " \"['Campaigns', 'Are', 'Long,', 'Expensive', 'and', 'Chaotic.', 'Maybe', 'That’s', 'a', 'Good', 'Thing.', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['‘Alien', 'Megastructure’', 'Star', 'Targeted', 'by', '$100', 'Million', 'SETI', 'Search']\",\n",
       " 'nan',\n",
       " \"['Brazen', 'Killing', 'of', 'Myanmar', 'Lawyer', 'Came', 'After', 'He', 'Sparred', 'With', 'Military', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['Jane', 'Pauley', 'Is', 'Back', '—', 'Again', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\\'People\\', \\'Power!\\', \\'Natives\\', \\'declare\\', \\'treaty\\', \\'rights,\\', \\'police\\', \\'admit\\', \\'defeat\\', \\'-\\', \\'cite\\', \\'lack\\', \\'of\\', \"\\'manpower\\'\", \\'to\\', \\'remove\\', \\'DAPL\\', \\'protesters\\']',\n",
       " \"['Deutsche', 'Bank', 'Considering', 'Alternatives', 'To', 'Paying', 'Cash', 'Bonus']\",\n",
       " \"['All', 'of', '“Danny', 'Dyer’s', 'Football', 'Foul', 'Ups”', 'DVD', 'in', '90', 'seconds']\",\n",
       " \"['Lack', 'of', 'Oxford', 'Comma', 'Could', 'Cost', 'Maine', 'Company', 'Millions', 'in', 'Overtime', 'Dispute', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['European', 'Parliament', 'Committee', 'Considering', 'Legal', 'Rights', 'for', 'Robots', '-', 'Breitbart']\",\n",
       " \"['Rex', 'Tillerson,', 'an', 'Aggressive', 'Dealmaker', 'Whose', 'Ties', 'With', 'Russia', 'May', 'Prompt', 'Scrutiny', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['NOT', 'OVER:', 'FBI', 'To', 'Conduct', 'New', 'Investigation', 'Of', 'Emails', 'From', 'Clinton’s', 'Private', 'Illegal', 'Server']\",\n",
       " \"['Hearts', '&', 'Mines:', 'The', 'US', 'Empire’s', 'Culture', 'Industry']\",\n",
       " \"['REPORT:', 'Megyn', 'Trashes', 'Trump,', 'Newt…', 'Then', 'Murdoch', 'Announces', 'Replacements', 'Are', 'Available']\",\n",
       " \"['Russian', 'scientists', 'will', 'track', 'sea', 'lions', 'from', 'space']\",\n",
       " \"['By', 'Seizing', 'the', 'Definition', 'of', '‘Populism,’', 'Reuters', 'Warns', 'Us', 'of', 'Chaos', 'to', 'Come']\",\n",
       " \"['Seaworthy', 'and', 'Ready', 'for', 'an', 'Early', 'Unveiling', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Donald', 'Trump', 'Adds', 'K.T.', 'McFarland', 'to', 'His', 'National', 'Security', 'Team', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Snowstorm', 'Brings', 'Wintry', 'Mix', 'of', 'Slush', 'and', 'Gripes', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Republicans,', 'Wilders,', 'Tillerson:', 'Your', 'Thursday', 'Evening', 'Briefing', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Review:', 'Bryan', 'Cranston', 'Shines', 'as', 'Lyndon', 'Johnson', 'in', '‘All', 'the', 'Way’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Samsung', 'Urges', 'Consumers', 'to', 'Stop', 'Using', 'Galaxy', 'Note', '7s', 'After', 'Battery', 'Fires', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Election', 'Result', 'Discussion', 'For', 'The', '2016', 'Presidential', 'Election', '(Open', 'Thread)']\",\n",
       " \"['AG', 'Lynch', 'told', 'FBI', 'Director', 'Comey', 'NOT', 'to', 'go', 'public', 'with', 'the', 'new', 'Clinton', 'email', 'investigation']\",\n",
       " \"['VIDEO', ':', 'Epic', 'Loser', 'Weiner', 'Says', 'He', 'Downloaded', 'ALL', 'OF', 'HUMA’S', 'EMAILS', 'By', '“ACCIDENT”', '–', 'TruthFeed']\",\n",
       " \"['“Donald', 'Trump', 'And', 'The', 'Rise', 'Of', 'White', 'Identity', 'In', 'Politics”']\",\n",
       " \"['GUEST', 'POST:', 'Why', 'I’m', 'Exceptional', '&', 'Indispensable', '–', 'by', 'Hillary', 'Clinton', '-', 'Rob', 'Slane']\",\n",
       " \"['Red,', 'Blue', 'and', 'Divided:', 'Six', 'Views', 'of', 'America', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['France', 'Identifies', '2nd', 'Man', 'Who', 'Attacked', 'Church', 'and', 'Killed', 'Priest', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['DREAMers', 'Arrested', 'in', 'Nationwide', 'Gang', 'Crackdown']\",\n",
       " \"['Maxine', 'Waters:', 'American', 'Public', '’Getting', 'Weary’', 'That', 'Trump', 'Not', 'Impeached', 'Yet', '-', 'Breitbart']\",\n",
       " \"['WWN’s', 'Horoscopes']\",\n",
       " \"['Deported', 'Italian', 'Mobster', 'Caught', 'Sneaking', 'Across', 'U.S.-Mexico', 'Border']\",\n",
       " \"['Leaked', 'Audio:', 'Hillary', 'Clinton', 'Pushed', '“Rigging”', 'Palestine', 'Elections', 'in', '2006']\",\n",
       " \"['How', 'the', 'Iranian-Saudi', 'Proxy', 'Struggle', 'Tore', 'Apart', 'the', 'Middle', 'East', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Lavrov', 'and', 'Kerry', 'discuss', 'Syrian', 'settlement']\",\n",
       " '[\"L\\'influence\", \\'des\\', \\'USA\\', \\'et\\', \\'de\\', \"l\\'Otan\", \\'dans\\', \\'les\\', \\'rapports\\', \\'de\\', \\'l’UE\\', \\'avec\\', \\'la\\', \\'Chine,\\', \\'Manlio\\', \\'Dinucci\\']',\n",
       " \"['Monica`s', 'Stained', 'Blue', 'Dress', 'Back', 'in', 'the', 'news']\",\n",
       " '[\\'The\\', \"MSM\\'s\", \\'twisted\\', \\'war\\', \\'language:\\', \\'Our\\', \"\\'sieges\\'\", \\'and\\', \\'theirs\\']',\n",
       " \"['Open', 'Borders', 'Groups', 'Gird', 'for', 'H-1B', 'Fights']\",\n",
       " '[\\'Report:\\', \\'Things\\', \\'Finally\\', \\'As\\', \\'Bad\\', \\'As\\', \\'Trump\\', \\'Claims\\', \\'-\\', \\'The\\', \\'Onion\\', \\'-\\', \"America\\'s\", \\'Finest\\', \\'News\\', \\'Source\\']',\n",
       " \"['‘Trump', 'Unveiled’', 'Reveals', 'the', 'Big', 'Con', 'of', 'Donald', 'Trump’s', 'Presidential', 'Run']\",\n",
       " \"['Will', 'WeinerGate', 'Expose', 'Darker', 'and', 'Dirtier', 'Secrets', 'Than', 'We', 'Imagined?']\",\n",
       " \"['Wallonia', 'caves', 'in?', 'Belgium', 'reaches', 'secret', 'EU-Canada', 'trade', 'deal', 'compromise']\",\n",
       " \"['Trump’s', 'election', 'breaks', 'chains', 'of', 'political', 'correctness']\",\n",
       " \"['WhiteHouse.gov', 'Takes', 'Down', 'Climate', 'Page,', 'Puts', 'Up', '’America', 'First’', 'Energy', 'Plan', '-', 'Breitbart']\",\n",
       " \"['Iran', 'Warns', 'President-Elect', 'Trump', 'Not', 'To', 'Mess', 'With', 'Their', 'Sweetheart', 'Nuclear', 'Deal', 'From', 'Obama']\",\n",
       " \"['Tony', 'Perkins:', 'Trump', 'EO', 'Re-Affirms', 'Jefferson’s', 'Doctrine', 'of', 'Separation', 'of', 'Church', 'and', 'State']\",\n",
       " \"['The', 'Moon', 'that', 'Fell', 'from', 'Heaven']\",\n",
       " \"['‘Mom’', 'Stars', 'Launch', 'Campaign', 'for', 'Planned', 'Parenthood']\",\n",
       " \"['Assad’s', 'Lesson', 'From', 'Aleppo:', 'Force', 'Works,', 'With', 'Few', 'Consequences', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Pence:', 'Bossert', 'and', 'Bannon', 'Weren’t', 'Demoted,', 'Both', 'Will', '’Continue', 'to', 'Play', 'Important', 'Policy', 'Roles’', '-', 'Breitbart']\",\n",
       " \"['Bundy', 'Ranch', 'occupiers', 'acquitted', 'on', 'all', 'counts', 'after', 'challenging', 'the', 'corrupt', 'Bureau', 'of', 'Land', 'Management']\",\n",
       " \"['Report:', 'Google', 'Faces', 'Fine', 'of', 'up', 'to', '$9', 'Billion', 'in', 'EU', 'Antitrust', 'Case', '-', 'Breitbart']\",\n",
       " \"['Don’t', 'ask', 'Thom', 'Yorke', 'to', 'write', 'a', 'cover', 'quote', 'for', 'your', 'book']\",\n",
       " \"['Internet', 'Flasher']\",\n",
       " \"['Gretchen', 'Carlson', 'Suit', 'Aims', 'at', 'Retaliation', 'Over', 'Discrimination', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Google', 'Launches', 'AI', 'Program', 'to', 'Detect', '’Hate', 'Speech’', '-', 'Breitbart']\",\n",
       " \"['Why', 'Polls', 'Showing', 'Hillary', 'in', 'The', 'Lead', 'Are', 'Useless', 'And', 'Misleading', '[CARTOON]']\",\n",
       " \"['Why', 'Last-Second', 'Lane', 'Mergers', 'Are', 'Good', 'for', 'Traffic', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['In', 'Macau,', 'Skipping', 'the', 'Casinos,', 'but', 'Embracing', 'the', 'Past', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['In', 'Montreal,', 'an', 'Ungainly', 'and', 'Unloved', 'Christmas', 'Tree', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Photos', 'of', 'Jupiter', 'From', 'NASA', 'Spacecraft,', 'Both', 'Near', 'and', 'Far', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Trump’s', 'Labor', 'Pick,', 'Andrew', 'Puzder,', 'Is', 'Critic', 'of', 'Minimum', 'Wage', 'Increases', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Nico', 'Rosberg', 'Takes', 'Formula', 'One', 'Drivers’', 'Title', 'Despite', 'Lewis', 'Hamilton’s', 'Win', 'in', 'Abu', 'Dhabi', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['What', 'a', 'Sensory', 'Isolation', 'Tank', 'Taught', 'Me', 'About', 'My', 'Brain', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Chelsea', 'Handler', 'Botches', 'Tweet', 'Attacking', 'Trump’s', 'Grandchild']\",\n",
       " \"['Report:', 'Trump', 'Moves', 'to', 'Own', 'Tax', 'Reform', 'Plan', 'Without', 'Speaker', 'Paul', 'Ryan', '-', 'Breitbart']\",\n",
       " 'nan',\n",
       " \"['AG', 'Jeff', 'Sessions', 'Unveils', 'Program', 'to', 'Accelerate', 'Deportation', 'of', 'Imprisoned', 'Illegals', '-', 'Breitbart']\",\n",
       " \"['Former', 'US', 'Attorney', 'for', 'DC:', 'New', 'Hillary', 'Email', 'Probe', 'Was', 'Result', 'of', '‘Revolt’', 'Inside', 'FBI']\",\n",
       " \"['How', 'Keeping', 'Up', 'Appearances', 'Ruined', 'a', 'Former', 'Dallas', 'Banker', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Sesame', 'Seeds', 'for', 'Knee', 'Osteoarthritis']\",\n",
       " \"['How', 'White', 'Cops', 'Interact', 'with', 'Blacks', 'in', 'Real', 'Life']\",\n",
       " \"['Pennsylvania', 'Republican', 'Pushing', 'Ban', 'on', 'Private', 'Gun', 'Sales', '-', 'Breitbart']\",\n",
       " \"['Review:', 'In', '‘Warcraft,’', 'Orcs', 'of', 'a', 'Different', 'Domain,', 'Fighting', 'With', 'Heart', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['When', 'Good', 'People', 'Share', 'Bad', 'Info', '—', 'Here’s', 'Why', 'You', 'Need', 'to', 'Fact', 'Check', 'Before', 'You', 'Click', 'Share']\",\n",
       " \"['For', 'Those', 'Who', 'Have', 'It', 'All,', 'Charitable', 'Wedding', 'Registries', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['THIS', 'IS', 'IT:', 'NYPD', 'Just', 'Raided', 'Hillary’s', 'Property!', 'What', 'They', 'Found', 'Will', 'RUIN', 'HER', 'LIFE', '•', 'USA', 'Newsflash']\",\n",
       " \"['Feinstein:', 'Gorsuch’s', '’Originalist’', 'Doctrine', '’Really', 'Troubling’', '-', 'Originalism', 'Would', 'Have', 'Allowed', 'Segregation', '-', 'Breitbart']\",\n",
       " \"['Confrontations', 'Flare', 'as', 'Obama’s', 'Traveling', 'Party', 'Reaches', 'China', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['3', 'Reasons', 'Why', 'You', 'Should', 'Apply', 'For', 'A', 'Job', 'In', 'The', 'Trump', 'Administration']\",\n",
       " \"['Police', 'truncheon', 'anyone', 'calling', 'for', 'Orgreave', 'inquiry']\",\n",
       " \"['And', 'You', 'Thought', 'the', 'Silver', 'Market', 'was', 'Rigged!']\",\n",
       " \"['UNSC', 'Members', 'Fail', 'to', 'Agree', 'on', 'New', 'Zealand', 'Draft', 'Resolution', 'on', 'Aleppo']\",\n",
       " \"['Can', 'You', 'Hear', 'Agnes', 'Martin’s', 'Serenity', 'in', 'John', 'Zorn’s', 'Frenzied', 'Music?', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['MUST', 'SEE!', '“Welcome', 'to', 'the', 'family.”', 'Why', 'so', 'many', 'Hispanic-Americans', 'are', 'voting', 'for', 'Donald', 'Trump']\",\n",
       " \"['Iraqi', 'Forces', 'Enter', 'Western', 'Mosul,', 'in', 'Fierce', 'Battle', 'Against', 'ISIS', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Europeans', 'have', 'no', 'future', '-', 'Putin', 'on', 'Migrant', 'Crisis', '[Video]']\",\n",
       " \"['In', 'Rolling', 'Stone', 'Defamation', 'Case,', 'Magazine', 'and', 'Reporter', 'Ordered', 'to', 'Pay', '$3', 'Million', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Hillary', 'is', 'Sick', '&', 'Tired', 'of', 'Suffering', 'from', 'Weiner', 'Backup']\",\n",
       " \"['Taiwan,', 'Italy,', 'Joe', 'McKnight:', 'Your', 'Friday', 'Evening', 'Briefing', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Shiite', 'militia', 'says', 'it', 'is', 'close', 'to', 'Tal', 'Afar,', 'which', 'Turkey', 'has', 'warned', 'is', 'off', 'limits']\",\n",
       " 'nan',\n",
       " \"['Brother', 'of', 'Clinton’s', 'Campaign', 'Chair', 'is', 'an', 'Active', 'Foreign', 'Agent', 'on', 'the', 'Saudi', 'Arabian', 'Payroll']\",\n",
       " \"['‘We', 'Must', 'Fight', 'Them’:', 'Trump', 'Goes', 'After', 'Conservatives', 'of', 'Freedom', 'Caucus', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['ISIS', 'Kidnaps,', 'Kills', 'at', 'Least', '30', 'Civilians', 'in', 'Afghanistan']\",\n",
       " \"['Holding', 'Hillary', 'Accountable']\",\n",
       " \"['BREAKING', ':', 'TED', 'CRUZ', 'CALLS', 'FOR', 'SPECIAL', 'PROSECUTOR', 'TO', 'INVESTIGATE', 'HILLARY', '–', 'TruthFeed']\",\n",
       " \"['Spared', 'by', 'Gunman', 'in', 'Charleston,', 'Churchgoer', 'Describes', 'Night', 'of', 'Terror', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['War', 'on', 'Saturated', 'Fats', 'Has', 'Harmed', 'People', 'in', 'Poor', 'Countries', 'Who', 'Shunned', 'Traditional', 'Fats', 'Like', 'Coconut', 'Oil']\",\n",
       " \"['Democrats,', 'With', 'Garland', 'on', 'Mind,', 'Mobilize', 'for', 'Supreme', 'Court', 'Fight', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Comment', 'on', '4', 'of', 'the', 'Best', 'Kinds', 'of', 'Milk', 'That', 'Aren’t', 'Dairy', 'by', '4', 'of', 'the', 'Best', 'Kinds', 'of', 'Milk', 'That', 'Aren’t', 'Dairy', '–', 'Collective', 'Evolution', '|', 'APG', 'Editorial']\",\n",
       " \"['Harry', 'Reid', 'BLASTS', 'Comey', 'For', 'Misconduct,', 'Drops', 'Bombshell:', 'FBI', 'Is', 'Sitting', 'On', 'Russian-Trump', 'Info']\",\n",
       " \"['NATIONAL', 'REVIEW,', 'Conservatism', 'Inc.,', 'Plan', 'To', 'Cave', 'EVEN', 'MORE', 'On', 'Immigration!']\",\n",
       " \"['[WATCH]', 'Thug', 'Calls', 'US', 'Marine', 'a', '“Pussy”', '–', 'Barely', 'Lives', 'to', 'Tell', 'the', 'Tale']\",\n",
       " \"['Will', 'the', 'next', 'US', 'president', 'be', 'a', 'psycho', 'lesbian?', '(plus', '2', 'breaking', 'news', 'videos)']\",\n",
       " \"['Critics', 'See', 'Efforts', 'by', 'Counties', 'and', 'Towns', 'to', 'Purge', 'Minority', 'Voters', 'From', 'Rolls', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Your', 'Evening', 'Briefing:', 'Hillary', 'Clinton,', 'Donald', 'Trump,', 'Cultural', 'Revolution', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['Indiana', 'Parents', 'Lose', 'Their', 'Baby', 'and', '2', 'Years', 'of', 'Their', 'Lives', 'in', 'Jail', 'for', '“Abuse”', 'They', 'Say', 'Never', 'Happened']\",\n",
       " \"['The', 'Rolling', 'Stones', 'Paint', 'It', 'Blue', 'on', 'Their', 'New', 'Album', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Uber', 'Extends', 'an', 'Olive', 'Branch', 'to', 'Local', 'Governments:', 'Its', 'Data', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Wilders:', 'Put', 'Dutch', 'First,', 'Not', '’Brussels,', 'Africa', 'and', 'Asylum', 'Seekers’']\",\n",
       " \"['Donald', 'Trump:', 'Good', 'Education', '‘Enriches', 'Both', 'the', 'Mind', 'and', 'the', 'Soul’']\",\n",
       " \"['Watch:', 'Brad', 'Pitt', 'Plays', 'Afghanistan', 'War', 'General', 'in', '’War', 'Machine’', 'Teaser', '-', 'Breitbart']\",\n",
       " \"['Two', 'Powerful', 'Earthquakes', 'Strike', 'Central', 'Italy']\",\n",
       " \"['Indoor', 'Gardening', 'Made', 'Easy:', 'The', 'Nutritower!']\",\n",
       " \"['Elizabeth', 'Warren', 'Defines', 'Sleazy', 'Hypocrisy']\",\n",
       " \"['Rush', 'Limbaugh:', 'O’Reilly', 'Departure', 'Was', 'Not', '’Natural’', '-', 'It', 'was', 'a', '’Campaign’', '-', 'Breitbart']\",\n",
       " \"['Budgies', 'demand', 'to', 'be', 'released', 'from', 'weird', 'people’s', 'homes']\",\n",
       " \"['Museum', 'Trustee,', 'a', 'Trump', 'Donor,', 'Supports', 'Groups', 'That', 'Deny', 'Climate', 'Change', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Cecile', 'Richards', 'Credits', 'Planned', 'Parenthood', 'Supporters', 'with', 'Stopping', 'AHCA', '-', 'Breitbart']\",\n",
       " \"['When', 'They', 'Switch', 'the', 'Chip', 'On', 'You', 'Won’t', 'Know', 'Who', 'You', 'Are', 'Anymore']\",\n",
       " \"['U.S.', 'Concedes', '$400', 'Million', 'Payment', 'to', 'Iran', 'Was', 'Delayed', 'as', 'Prisoner', '‘Leverage’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " 'nan',\n",
       " \"['Australia', 'Says', 'It', 'Foiled', 'a', 'Terrorist', 'Plot', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Oroville', 'Dam:', 'State', 'and', 'Federal', 'Government', 'Share', 'Blame']\",\n",
       " \"['The', 'Daily', 'Traditionalist:', 'Jeff', 'Schoep', 'and', 'the', 'NSM']\",\n",
       " \"['Michael', 'Moore:', 'Joe', 'Blow', 'Will', 'Vote', 'Trump', 'As', '“Ultimate', 'F––', 'You', 'to', 'the', 'Elite…', 'A', 'Human', 'Molotov', 'Cocktail”']\",\n",
       " \"['Jaguars', 'Owner', 'Shahid', 'Khan', 'Opposes', 'Trump’s', 'Immigration', 'Ban', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['This', 'anti-Trump', 'advert', 'on', 'the', 'side', 'of', 'a', 'bus', 'is', 'really', 'visually', 'clever', 'and', 'you', 'have', 'to', 'see', 'it', 'in', 'motion']\",\n",
       " \"['A', 'Piano', 'Man', 'of', 'Many', 'Faces,', 'and', 'Some', 'Stranger', 'Stories', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['250,000', 'More', 'Tickets', 'to', 'Be', 'Released', 'for', '‘Harry', 'Potter', 'and', 'the', 'Cursed', 'Child’', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['FBI', 'Visits', 'Man', 'at', 'His', 'Home', 'After', 'He', 'Films', 'US', 'Postal', 'Distribution', 'Center']\",\n",
       " \"['Hispanic', 'Crowd', 'Boos', 'Marco', 'Rubio', 'off', 'Stage']\",\n",
       " \"['Fired', 'TV', 'Reporter:', 'I’ve', 'Received', 'Thousands', 'of', '’Sexual', 'and', 'Violent’', 'Threats']\",\n",
       " \"['Detained', 'Illegal', 'Aliens', 'End', '3-Day', 'Hunger', 'Strike']\",\n",
       " \"['US,', 'Russia', 'to', 'Meet', 'to', 'Discuss', 'Intermediate', 'Nuclear', 'Forces', 'Treaty', '-', 'Alex', 'Gorka']\",\n",
       " \"['Breaking', 'Silence,', 'Officer', 'Testifies', 'About', 'Killing', 'of', 'Walter', 'Scott', '-', 'The', 'New', 'York', 'Times']\",\n",
       " '[\\'The\\', \\'Onion’s\\', \\'Special\\', \\'Coverage\\', \\'Of\\', \\'Election\\', \\'Day\\', \\'2016\\', \\'-\\', \\'The\\', \\'Onion\\', \\'-\\', \"America\\'s\", \\'Finest\\', \\'News\\', \\'Source\\']',\n",
       " \"['Illegal', 'Immigrant', 'Allegedly', 'Kills', 'for', 'Parking', 'Spot']\",\n",
       " \"['Writing', 'with', 'pen', 'and', 'paper', 'hailed', 'as', 'latest', 'twatty', 'show-off', 'thing', 'to', 'do']\",\n",
       " \"['President', 'Trump', 'Honors', 'Little', 'Sisters', 'of', 'the', 'Poor', 'on', 'First', 'White', 'House', 'Nat’l', 'Day', 'of', 'Prayer', 'in', 'Years', '-', 'Breitbart']\",\n",
       " \"['Hillary', 'Clinton', 'Supporters', 'Now', 'Calling', 'for', 'a', 'Recount', 'of', 'Votes', 'in', 'Battleground', 'States']\",\n",
       " \"['HBO', 'Scraps', 'Jon', 'Stewart', 'Animated', 'Comedy', 'Series']\",\n",
       " \"['Sentencing', 'for', 'Murderer', 'of', 'Rare', 'Book', 'Dealer']\",\n",
       " \"['The', 'Latest', 'Stock', 'Market', 'and', 'Investing', 'Books', '|', 'Financial', 'Markets']\",\n",
       " \"['Fashion', 'Industry', 'CEO:', 'Supporting', 'Planned', 'Parenthood', 'Is', 'a', '‘Civic', 'Responsibility’']\",\n",
       " \"['Anti-Trump', 'protests', 'are', 'paid', 'and', 'staged,', 'Craigslist', 'reveals']\",\n",
       " \"['Men,', 'Is', 'Exercise', 'Putting', 'a', 'Damper', 'on', 'Your', 'Sex', 'Life?', '-', 'The', 'New', 'York', 'Times']\",\n",
       " \"['Baking', 'Soda', '&', 'Coconut', 'Oil', 'Can', 'Kill', 'Cancer:', 'Eye-Opening', 'Evidence']\",\n",
       " \"['Saudi', 'Arabia', 'announces', 'date', 'certain', 'for', 'the', 'application', 'of', 'VAT']\",\n",
       " \"['Trump,', 'Citing', 'No', 'Evidence,', 'Suggests', 'Susan', 'Rice', 'Committed', 'Crime', '-', 'The', 'New', 'York', 'Times']\",\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 64)          64000     \n",
      "=================================================================\n",
      "Total params: 64,000\n",
      "Trainable params: 64,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_60 to have shape (512, 100) but got array with shape (512, 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-36b4da4b9ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mX_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m autoencoder.fit(X_embedded, X_embedded,epochs=10,\n\u001b[0;32m---> 31\u001b[0;31m             batch_size=256, validation_split=.1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_60 to have shape (512, 100) but got array with shape (512, 64)"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64))\n",
    "model.compile('rmsprop', 'mse')\n",
    "model.summary()\n",
    "\n",
    "input_i = Input(shape=(512, 64))\n",
    "encoded_h1 = Dense(64, activation='tanh')(input_i)\n",
    "encoded_h2 = Dense(32, activation='tanh')(encoded_h1)\n",
    "encoded_h3 = Dense(16, activation='tanh')(encoded_h2)\n",
    "encoded_h4 = Dense(8, activation='tanh')(encoded_h3)\n",
    "encoded_h5 = Dense(4, activation='tanh')(encoded_h4)\n",
    "latent = Dense(2, activation='tanh')(encoded_h5)\n",
    "decoder_h1 = Dense(4, activation='tanh')(latent)\n",
    "decoder_h2 = Dense(8, activation='tanh')(decoder_h1)\n",
    "decoder_h3 = Dense(16, activation='tanh')(decoder_h2)\n",
    "decoder_h4 = Dense(32, activation='tanh')(decoder_h3)\n",
    "decoder_h5 = Dense(64, activation='tanh')(decoder_h4)\n",
    "\n",
    "output = Dense(100, activation='tanh')(decoder_h5)\n",
    "\n",
    "autoencoder = Model(input_i, output)\n",
    "\n",
    "autoencoder.compile('adadelta','mse')\n",
    "\n",
    "X_embedded = model.predict(np.array(X_train))\n",
    "autoencoder.fit(X_embedded, X_embedded,epochs=10,\n",
    "            batch_size=256, validation_split=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Into Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextCleaner import TextCleaner\n",
    "help(TextCleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalSentenceEmbeddingTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def fetch_universal_sentence_embeddings(self, messages, verbose=0):\n",
    "        \"\"\"Fetches universal sentence embeddings from Google's\n",
    "        research paper https://arxiv.org/pdf/1803.11175.pdf.\n",
    "\n",
    "        INPUTS:\n",
    "        RETURNS:\n",
    "        \"\"\"\n",
    "        module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "\n",
    "        # Import the Universal Sentence Encoder's TF Hub module\n",
    "        embed = hub.Module(module_url)\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "            message_embeddings = session.run(embed(messages))\n",
    "            embeddings = list()\n",
    "            for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "                if verbose:\n",
    "                    print(\"Message: {}\".format(messages[i]))\n",
    "                    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "                    message_embedding_snippet = \", \".join(\n",
    "                        (str(x) for x in message_embedding[:3]))\n",
    "                    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n",
    "                embeddings.append(message_embedding)\n",
    "        return embeddings\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"interface conforming, and allows use of fit_transform\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.fetch_universal_sentence_embeddings(messages=X)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df['title'].astype(str).values, df.label.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7, random_state=7)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, train_size=.5, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6e39506a1871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    623\u001b[0m                                      n_candidates * n_splits))\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mbase_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mnew_object_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mnew_object_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mnew_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mparams_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# XXX: not handling dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# XXX: not handling dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# XXX: not handling dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# XXX: not handling dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mestimator_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mnew_object_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mnew_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mparams_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Final-Project-Fake-News/TextCleaner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, remove_stopwords, remove_urls, remove_puncts, lemmatize, extra_punct, custom_stopwords, custom_non_stopwords, verbose, parser)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Set up punctation tranlation table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremovals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigits\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_punct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremovals\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not NoneType"
     ]
    }
   ],
   "source": [
    "# test LinearSVC model as baseline\n",
    "params = {'clean__lemmatize': [True, False],\n",
    "          'clean__lower': [True, False],\n",
    "          'clean__remove_punct': [True, False],\n",
    "          'clean__extra_punct': [''],\n",
    "          'clf__C':[1, 2, 3], # 1 best here  [.001, .01, .1, 1, 10, 100, 1000]\n",
    "          'clf__random_state': [777]}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clean', TextCleaner()),\n",
    "    ('embed', UniversalSentenceEmbeddingTransformer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(pipeline, params, cv=5, verbose=1)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextCleaner import TextCleaner as tc\n",
    "help(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
