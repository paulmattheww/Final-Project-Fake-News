{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@inproceedings{wang2018eann,\n",
    "  title={EANN: Event Adversarial Neural Networks for Multi-Modal Fake News Detection},\n",
    "  author={Wang, Yaqing and Ma, Fenglong and Jin, Zhiwei and Yuan, Ye and Xun, Guangxu and Jha, Kishlay and Su, Lu and Gao, Jing},\n",
    "  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining},\n",
    "  pages={849--857},\n",
    "  year={2018},\n",
    "  organization={ACM}\n",
    "}\n",
    "\n",
    "https://arxiv.org/abs/1803.11175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20800.000000</td>\n",
       "      <td>20800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10399.500000</td>\n",
       "      <td>0.500625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6004.587135</td>\n",
       "      <td>0.500012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5199.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10399.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15599.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20799.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         label\n",
       "count  20800.000000  20800.000000\n",
       "mean   10399.500000      0.500625\n",
       "std     6004.587135      0.500012\n",
       "min        0.000000      0.000000\n",
       "25%     5199.750000      0.000000\n",
       "50%    10399.500000      1.000000\n",
       "75%    15599.250000      1.000000\n",
       "max    20799.000000      1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(code, n_code, phase_train):\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        with variable_scope(\"hidden_1\"):\n",
    "            hidden_1 = layer(code, [n_code, n_decoder_hidden_1], \n",
    "                            [n_decoder_hidden_1],  phase_train)\n",
    "            \n",
    "        with tf.variable_scope(\"hidden_2\"):\n",
    "            hidden_2 = layer(hidden_1, [n_decoder_hidden_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmw/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "class TextAutoencoder(object):\n",
    "    \"\"\"\n",
    "    Class that encapsulates the encoder-decoder architecture to\n",
    "    reconstruct pieces of text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lstm_units, embeddings, go, train=True,\n",
    "                 train_embeddings=False, bidirectional=True):\n",
    "        \"\"\"\n",
    "        Initialize the encoder/decoder and creates Tensor objects\n",
    "\n",
    "        :param lstm_units: number of LSTM units\n",
    "        :param embeddings: numpy array with initial embeddings\n",
    "        :param go: index of the GO symbol in the embedding matrix\n",
    "        :param train_embeddings: whether to adjust embeddings during training\n",
    "        :param bidirectional: whether to create a bidirectional autoencoder\n",
    "            (if False, a simple linear LSTM is used)\n",
    "        \"\"\"\n",
    "        # EOS and GO share the same symbol. Only GO needs to be embedded, and\n",
    "        # only EOS exists as a possible network output\n",
    "        self.go = go\n",
    "        self.eos = go\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.vocab_size = embeddings.shape[0]\n",
    "        self.embedding_size = embeddings.shape[1]\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        # the sentence is the object to be memorized\n",
    "        self.sentence = tf.placeholder(tf.int32, [None, None], 'sentence')\n",
    "        self.sentence_size = tf.placeholder(tf.int32, [None],\n",
    "                                            'sentence_size')\n",
    "        self.l2_constant = tf.placeholder(tf.float32, name='l2_constant')\n",
    "        self.clip_value = tf.placeholder(tf.float32, name='clip')\n",
    "        self.learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        self.dropout_keep = tf.placeholder(tf.float32, name='dropout_keep')\n",
    "\n",
    "        self.decoder_step_input = tf.placeholder(tf.int32,\n",
    "                                                 [None],\n",
    "                                                 'prediction_step')\n",
    "\n",
    "        name = 'decoder_fw_step_state_c'\n",
    "        self.decoder_fw_step_c = tf.placeholder(tf.float32,\n",
    "                                                [None, lstm_units], name)\n",
    "        name = 'decoder_fw_step_state_h'\n",
    "        self.decoder_fw_step_h = tf.placeholder(tf.float32,\n",
    "                                                [None, lstm_units], name)\n",
    "        self.decoder_bw_step_c = tf.placeholder(tf.float32,\n",
    "                                                [None, lstm_units],\n",
    "                                                'decoder_bw_step_state_c')\n",
    "        self.decoder_bw_step_h = tf.placeholder(tf.float32,\n",
    "                                                [None, lstm_units],\n",
    "                                                'decoder_bw_step_state_h')\n",
    "\n",
    "        with tf.variable_scope('autoencoder') as self.scope:\n",
    "            self.embeddings = tf.Variable(embeddings, name='embeddings',\n",
    "                                          trainable=train_embeddings)\n",
    "\n",
    "            initializer = tf.glorot_normal_initializer()\n",
    "            self.lstm_fw = tf.nn.rnn_cell.LSTMCell(lstm_units,\n",
    "                                                   initializer=initializer)\n",
    "            self.lstm_bw = tf.nn.rnn_cell.LSTMCell(lstm_units,\n",
    "                                                   initializer=initializer)\n",
    "\n",
    "            embedded = tf.nn.embedding_lookup(self.embeddings, self.sentence)\n",
    "            embedded = tf.nn.dropout(embedded, self.dropout_keep)\n",
    "\n",
    "            # encoding step\n",
    "            if bidirectional:\n",
    "                bdr = tf.nn.bidirectional_dynamic_rnn\n",
    "                ret = bdr(self.lstm_fw, self.lstm_bw,\n",
    "                          embedded, dtype=tf.float32,\n",
    "                          sequence_length=self.sentence_size,\n",
    "                          scope=self.scope)\n",
    "            else:\n",
    "                ret = tf.nn.dynamic_rnn(self.lstm_fw, embedded,\n",
    "                                        dtype=tf.float32,\n",
    "                                        sequence_length=self.sentence_size,\n",
    "                                        scope=self.scope)\n",
    "            _, self.encoded_state = ret\n",
    "            if bidirectional:\n",
    "                encoded_state_fw, encoded_state_bw = self.encoded_state\n",
    "\n",
    "                # set the scope name used inside the decoder.\n",
    "                # maybe there's a more elegant way to do it?\n",
    "                fw_scope_name = self.scope.name + '/fw'\n",
    "                bw_scope_name = self.scope.name + '/bw'\n",
    "            else:\n",
    "                encoded_state_fw = self.encoded_state\n",
    "                fw_scope_name = self.scope\n",
    "\n",
    "            self.scope.reuse_variables()\n",
    "\n",
    "            # generate a batch of embedded GO\n",
    "            # sentence_size has the batch dimension\n",
    "            go_batch = self._generate_batch_go(self.sentence_size)\n",
    "            embedded_eos = tf.nn.embedding_lookup(self.embeddings,\n",
    "                                                  go_batch)\n",
    "            embedded_eos = tf.reshape(embedded_eos,\n",
    "                                      [-1, 1, self.embedding_size])\n",
    "            decoder_input = tf.concat([embedded_eos, embedded], axis=1)\n",
    "\n",
    "            # decoding step\n",
    "\n",
    "            # We give the same inputs to the forward and backward LSTMs,\n",
    "            # but each one has its own hidden state\n",
    "            # their outputs are concatenated and fed to the softmax layer\n",
    "            if bidirectional:\n",
    "                outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    self.lstm_fw, self.lstm_bw, decoder_input,\n",
    "                    self.sentence_size, encoded_state_fw, encoded_state_bw)\n",
    "\n",
    "                # concat fw and bw outputs\n",
    "                outputs = tf.concat(outputs, -1)\n",
    "            else:\n",
    "                outputs, _ = tf.nn.dynamic_rnn(\n",
    "                    self.lstm_fw, decoder_input, self.sentence_size,\n",
    "                    encoded_state_fw)\n",
    "\n",
    "            self.decoder_outputs = outputs\n",
    "\n",
    "        # now project the outputs to the vocabulary\n",
    "        with tf.variable_scope('projection') as self.projection_scope:\n",
    "            # decoder_outputs has shape (batch, max_sentence_size, vocab_size)\n",
    "            self.logits = tf.layers.dense(outputs, self.vocab_size)\n",
    "\n",
    "        # tensors for running a model\n",
    "        embedded_step = tf.nn.embedding_lookup(self.embeddings,\n",
    "                                               self.decoder_step_input)\n",
    "        state_fw = tf.nn.rnn_cell.LSTMStateTuple(self.decoder_fw_step_c,\n",
    "                                                 self.decoder_fw_step_h)\n",
    "        state_bw = tf.nn.rnn_cell.LSTMStateTuple(self.decoder_bw_step_c,\n",
    "                                                 self.decoder_bw_step_h)\n",
    "        with tf.variable_scope(fw_scope_name, reuse=True):\n",
    "            ret_fw = self.lstm_fw(embedded_step, state_fw)\n",
    "        step_output_fw, self.decoder_fw_step_state = ret_fw\n",
    "\n",
    "        if bidirectional:\n",
    "            with tf.variable_scope(bw_scope_name, reuse=True):\n",
    "                ret_bw = self.lstm_bw(embedded_step, state_bw)\n",
    "                step_output_bw, self.decoder_bw_step_state = ret_bw\n",
    "                step_output = tf.concat(axis=1, values=[step_output_fw,\n",
    "                                                        step_output_bw])\n",
    "        else:\n",
    "            step_output = step_output_fw\n",
    "\n",
    "        with tf.variable_scope(self.projection_scope, reuse=True):\n",
    "            self.projected_step_output = tf.layers.dense(step_output,\n",
    "                                                         self.vocab_size)\n",
    "\n",
    "        if train:\n",
    "            self._create_training_tensors()\n",
    "\n",
    "    def _create_training_tensors(self):\n",
    "        \"\"\"\n",
    "        Create member variables related to training.\n",
    "        \"\"\"\n",
    "        eos_batch = self._generate_batch_go(self.sentence_size)\n",
    "        eos_batch = tf.reshape(eos_batch, [-1, 1])\n",
    "        decoder_labels = tf.concat([self.sentence, eos_batch], -1)\n",
    "\n",
    "        projection_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                            scope=self.projection_scope.name)\n",
    "        # a bit ugly, maybe we should improve this?\n",
    "        projection_w = [var for var in projection_vars\n",
    "                        if 'kernel' in var.name][0]\n",
    "        projection_b = [var for var in projection_vars\n",
    "                        if 'bias' in var.name][0]\n",
    "\n",
    "        # set the importance of each time step\n",
    "        # 1 if before sentence end or EOS itself; 0 otherwise\n",
    "        max_len = tf.shape(self.sentence)[1]\n",
    "        mask = tf.sequence_mask(self.sentence_size + 1, max_len + 1, tf.float32)\n",
    "        num_actual_labels = tf.reduce_sum(mask)\n",
    "        projection_w_t = tf.transpose(projection_w)\n",
    "\n",
    "        # reshape to have batch and time steps in the same dimension\n",
    "        decoder_outputs2d = tf.reshape(self.decoder_outputs,\n",
    "                                       [-1, tf.shape(self.decoder_outputs)[-1]])\n",
    "        labels = tf.reshape(decoder_labels, [-1, 1])\n",
    "        sampled_loss = tf.nn.sampled_softmax_loss(\n",
    "            projection_w_t, projection_b, labels, decoder_outputs2d, 100,\n",
    "            self.vocab_size)\n",
    "\n",
    "        masked_loss = tf.reshape(mask, [-1]) * sampled_loss\n",
    "        self.loss = tf.reduce_sum(masked_loss) / num_actual_labels\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(self.loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, self.clip_value)\n",
    "\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, v),\n",
    "                                                  global_step=self.global_step)\n",
    "\n",
    "    def get_trainable_variables(self):\n",
    "        \"\"\"\n",
    "        Return all trainable variables inside the model\n",
    "        \"\"\"\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "    def train(self, session, save_path, train_data, valid_data,\n",
    "              batch_size, epochs, learning_rate, dropout_keep,\n",
    "              clip_value, report_interval):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "\n",
    "        :param session: tensorflow session\n",
    "        :param train_data: Dataset object with training data\n",
    "        :param valid_data: Dataset object with validation data\n",
    "        :param batch_size: batch size\n",
    "        :param learning_rate: initial learning rate\n",
    "        :param dropout_keep: the probability that each LSTM input/output is kept\n",
    "        :param epochs: how many epochs to train for\n",
    "        :param clip_value: value to clip tensor norm during training\n",
    "        :param save_path: folder to save the model\n",
    "        :param report_interval: report after that many batches\n",
    "        \"\"\"\n",
    "        saver = tf.train.Saver(self.get_trainable_variables(),\n",
    "                               max_to_keep=1)\n",
    "\n",
    "        best_loss = 10000\n",
    "        accumulated_loss = 0\n",
    "        batch_counter = 0\n",
    "        num_sents = 0\n",
    "\n",
    "        # get all data at once. we need all matrices with the same size,\n",
    "        # or else they don't fit the placeholders\n",
    "        # train_sents, train_sizes = train_data.join_all(self.go,\n",
    "        #                                                self.num_time_steps,\n",
    "        #                                                shuffle=True)\n",
    "\n",
    "        # del train_data  # save memory...\n",
    "        valid_sents, valid_sizes = valid_data.join_all(self.go,\n",
    "                                                       shuffle=True)\n",
    "        train_data.reset_epoch_counter()\n",
    "        feeds = {self.clip_value: clip_value,\n",
    "                 self.dropout_keep: dropout_keep,\n",
    "                 self.learning_rate: learning_rate}\n",
    "\n",
    "        while train_data.epoch_counter < epochs:\n",
    "            batch_counter += 1\n",
    "            train_sents, train_sizes = train_data.next_batch(batch_size)\n",
    "            feeds[self.sentence] = train_sents\n",
    "            feeds[self.sentence_size] = train_sizes\n",
    "\n",
    "            _, loss = session.run([self.train_op, self.loss], feeds)\n",
    "\n",
    "            # multiply by len because some batches may be smaller\n",
    "            # (due to bucketing), then take the average\n",
    "            accumulated_loss += loss * len(train_sents)\n",
    "            num_sents += len(train_sents)\n",
    "\n",
    "            if batch_counter % report_interval == 0:\n",
    "                avg_loss = accumulated_loss / num_sents\n",
    "                accumulated_loss = 0\n",
    "                num_sents = 0\n",
    "\n",
    "                # we can't use all the validation at once, since it would\n",
    "                # take too much memory. running many small batches would\n",
    "                # instead take too much time. So let's just sample it.\n",
    "                sample_indices = np.random.randint(0, len(valid_data),\n",
    "                                                   5000)\n",
    "                validation_feeds = {\n",
    "                    self.sentence: valid_sents[sample_indices],\n",
    "                    self.sentence_size: valid_sizes[sample_indices],\n",
    "                    self.dropout_keep: 1}\n",
    "\n",
    "                loss = session.run(self.loss, validation_feeds)\n",
    "                msg = '%d epochs, %d batches\\t' % (train_data.epoch_counter,\n",
    "                                                   batch_counter)\n",
    "                msg += 'Avg batch loss: %f\\t' % avg_loss\n",
    "                msg += 'Validation loss: %f' % loss\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    self.save(saver, session, save_path)\n",
    "                    msg += '\\t(saved model)'\n",
    "\n",
    "                logging.info(msg)\n",
    "\n",
    "    def save(self, saver, session, directory):\n",
    "        \"\"\"\n",
    "        Save the autoencoder model and metadata to the specified\n",
    "        directory.\n",
    "        \"\"\"\n",
    "        model_path = os.path.join(directory, 'model')\n",
    "        saver.save(session, model_path)\n",
    "        metadata = {'vocab_size': self.vocab_size,\n",
    "                    'embedding_size': self.embedding_size,\n",
    "                    'num_units': self.lstm_fw.output_size,\n",
    "                    'go': self.go,\n",
    "                    'bidirectional': self.bidirectional\n",
    "                    }\n",
    "        metadata_path = os.path.join(directory, 'metadata.json')\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, directory, session, train=False):\n",
    "        \"\"\"\n",
    "        Load an instance of this class from a previously saved one.\n",
    "        :param directory: directory with the model files\n",
    "        :param session: tensorflow session\n",
    "        :param train: if True, also create training tensors\n",
    "        :return: a TextAutoencoder instance\n",
    "        \"\"\"\n",
    "        model_path = os.path.join(directory, 'model')\n",
    "        metadata_path = os.path.join(directory, 'metadata.json')\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        dummy_embeddings = np.empty((metadata['vocab_size'],\n",
    "                                     metadata['embedding_size'],),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "        ae = TextAutoencoder(metadata['num_units'], dummy_embeddings,\n",
    "                             metadata['go'], train=train,\n",
    "                             bidirectional=metadata['bidirectional'])\n",
    "        vars_to_load = ae.get_trainable_variables()\n",
    "        if not train:\n",
    "            # if not flagged for training, the embeddings won't be in\n",
    "            # the list\n",
    "            vars_to_load.append(ae.embeddings)\n",
    "\n",
    "        saver = tf.train.Saver(vars_to_load)\n",
    "        saver.restore(session, model_path)\n",
    "        return ae\n",
    "\n",
    "    def encode(self, session, inputs, sizes):\n",
    "        \"\"\"\n",
    "        Run the encoder to obtain the encoded hidden state\n",
    "\n",
    "        :param session: tensorflow session\n",
    "        :param inputs: 2-d array with the word indices\n",
    "        :param sizes: 1-d array with size of each sentence\n",
    "        :return: a 2-d numpy array with the hidden state\n",
    "        \"\"\"\n",
    "        feeds = {self.sentence: inputs,\n",
    "                 self.sentence_size: sizes,\n",
    "                 self.dropout_keep: 1}\n",
    "        state = session.run(self.encoded_state, feeds)\n",
    "        if self.bidirectional:\n",
    "            state_fw, state_bw = state\n",
    "            return np.hstack((state_fw.c, state_bw.c))\n",
    "        return state.c\n",
    "\n",
    "    def run(self, session, inputs, sizes):\n",
    "        \"\"\"\n",
    "        Run the autoencoder with the given data\n",
    "\n",
    "        :param session: tensorflow session\n",
    "        :param inputs: 2-d array with the word indices\n",
    "        :param sizes: 1-d array with size of each sentence\n",
    "        :return: a 2-d array (batch, output_length) with the answer\n",
    "            produced by the autoencoder. The output length is not\n",
    "            fixed; it stops after producing EOS for all items in the\n",
    "            batch or reaching two times the maximum number of time\n",
    "            steps in the inputs.\n",
    "        \"\"\"\n",
    "        feeds = {self.sentence: inputs,\n",
    "                 self.sentence_size: sizes,\n",
    "                 self.dropout_keep: 1}\n",
    "        state = session.run(self.encoded_state, feeds)\n",
    "        if self.bidirectional:\n",
    "            state_fw, state_bw = state\n",
    "        else:\n",
    "            state_fw = state\n",
    "\n",
    "        time_steps = 0\n",
    "        max_time_steps = 2 * len(inputs[0])\n",
    "        answer = []\n",
    "        input_symbol = self.go * np.ones_like(sizes, dtype=np.int32)\n",
    "\n",
    "        # this array control which sequences have already been finished by the\n",
    "        # decoder, i.e., for which ones it already produced the END symbol\n",
    "        sequences_done = np.zeros_like(sizes, dtype=np.bool)\n",
    "\n",
    "        while True:\n",
    "            # we could use tensorflow's rnn_decoder, but this gives us\n",
    "            # finer control\n",
    "\n",
    "            feeds = {self.decoder_fw_step_c: state_fw.c,\n",
    "                     self.decoder_fw_step_h: state_fw.h,\n",
    "                     self.decoder_step_input: input_symbol,\n",
    "                     self.dropout_keep: 1}\n",
    "            if self.bidirectional:\n",
    "                feeds[self.decoder_bw_step_c] = state_bw.c\n",
    "                feeds[self.decoder_bw_step_h] = state_bw.h\n",
    "\n",
    "                ops = [self.projected_step_output,\n",
    "                       self.decoder_fw_step_state,\n",
    "                       self.decoder_bw_step_state]\n",
    "                outputs, state_fw, state_bw = session.run(ops, feeds)\n",
    "            else:\n",
    "                ops = [self.projected_step_output,\n",
    "                       self.decoder_fw_step_state]\n",
    "                outputs, state_fw = session.run(ops, feeds)\n",
    "\n",
    "            input_symbol = outputs.argmax(1)\n",
    "            answer.append(input_symbol)\n",
    "\n",
    "            # use an \"additive\" or in order to avoid infinite loops\n",
    "            sequences_done |= (input_symbol == self.eos)\n",
    "\n",
    "            if sequences_done.all() or time_steps > max_time_steps:\n",
    "                break\n",
    "            else:\n",
    "                time_steps += 1\n",
    "\n",
    "        return np.hstack(answer)\n",
    "\n",
    "    def _generate_batch_go(self, like):\n",
    "        \"\"\"\n",
    "        Generate a 1-d tensor with copies of EOS as big as the batch size,\n",
    "\n",
    "        :param like: a tensor whose shape the returned embeddings should match\n",
    "        :return: a tensor with shape as `like`\n",
    "        \"\"\"\n",
    "        ones = tf.ones_like(like)\n",
    "        return ones * self.go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = TextAutoencoder(lstm_units, embeddings, go, train=True, train_embeddings=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(post, word_id_map, W):\n",
    "    \"\"\"Cited from https://github.com/yaqingwang/EANN-KDD18\n",
    "    Generates word2vec embeddings.\n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    RETURN:\n",
    "    \"\"\"\n",
    "    word_embedding = []\n",
    "    mask = []\n",
    "    for sentence in post:\n",
    "        sen_embedding = []\n",
    "        seq_len = len(sentence) - 1\n",
    "        mask_seq = np.zeros(args.sequence_len, dtype=np.float32)\n",
    "        mask_seq[:len(sentence)] = 1.0\n",
    "        for i, word in enumerate(sentence):\n",
    "            sen_embedding.append(word_id_map[word])\n",
    "\n",
    "        while len(sen_embedding) < args.sequence_len:\n",
    "            sen_embedding.append(0)\n",
    "        word_embedding.append(copy.deepcopy(sen_embedding))\n",
    "        mask.append(copy.deepcopy(mask_seq))\n",
    "    return word_embedding, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# read in data and preprocess\n",
    "train_df = pd.read_csv('c:/users/washburp/documents/kaggle/arthur/data/train_df.csv')\n",
    "\n",
    "# drop missing descriptions\n",
    "train_df.dropna(inplace=True, subset=['desc_of_operations'])\n",
    "\n",
    "# derive length of description & filter long/short ones\n",
    "train_df['len_description'] = train_df.desc_of_operations.apply(len)\n",
    "reasonable_sized_texts = (train_df.len_description.astype(int) >= 1) | (train_df.len_description.astype(int) <= 200)\n",
    "train_df = train_df.loc[reasonable_sized_texts]\n",
    "\n",
    "# clean up text\n",
    "from text_cleanup import TextCleaner\n",
    "\n",
    "train_df['clean_desc'] = TextCleaner().transform(train_df.desc_of_operations.values)\n",
    "train_df.head()\n",
    "\n",
    "sentences = []\n",
    "for description in train_df.clean_desc.tolist():\n",
    "    sentences.append(description.split())\n",
    "    \n",
    "sentences[:1]\n",
    "\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(iter=1, min_count=10, size=150, workers=4)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "topn = 20\n",
    "\n",
    "dat = model.most_similar(['salon'], topn=topn)\n",
    "df = pd.DataFrame(dat, columns=['word', 'prob']).set_index('word')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module(module_url)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(messages))\n",
    "\n",
    "    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "        print(\"Message: {}\".format(messages[i]))\n",
    "        print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "        message_embedding_snippet = \", \".join(\n",
    "            (str(x) for x in message_embedding[:3]))\n",
    "        print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
